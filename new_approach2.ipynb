{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipiFsA6YCtyi",
        "outputId": "1dbb196c-3ab8-4be7-afd4-f1a3b71f0afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         train_mean  test_mean  mean_diff  train_variance  test_variance  \\\n",
            "feature                                                                    \n",
            "0         15.560411  15.507078  -0.053333       18.730704      17.260227   \n",
            "1         12.650449  12.655507   0.005058       14.317654      13.704232   \n",
            "2         27.750084  27.736016  -0.014067       48.258792      44.019127   \n",
            "3         18.796808  18.825133   0.028325       24.323544      22.632589   \n",
            "4         19.071302  18.995343  -0.075959       27.471500      25.135808   \n",
            "5         11.820110  11.769083  -0.051027       13.312292      12.482966   \n",
            "6         19.365360  19.355964  -0.009395       28.260874      25.857634   \n",
            "7         15.602632  15.517396  -0.085235       19.669401      18.001512   \n",
            "8         14.163618  14.233636   0.070018       19.810112      19.051235   \n",
            "9         15.989661  16.041487   0.051826       22.978933      22.298445   \n",
            "\n",
            "         variance_ratio  corr_with_target  \n",
            "feature                                    \n",
            "0              0.921494          0.300607  \n",
            "1              0.957156          0.290802  \n",
            "2              0.912147          0.374769  \n",
            "3              0.930481          0.339719  \n",
            "4              0.914978          0.316651  \n",
            "5              0.937702          0.301198  \n",
            "6              0.914962          0.338679  \n",
            "7              0.915204          0.320353  \n",
            "8              0.961692          0.307160  \n",
            "9              0.970386          0.283087  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "x_train = pd.read_csv('/content/x_train.txt', sep=r'\\s+', header=None)\n",
        "y_train = pd.read_csv('/content/y_train.txt', sep=r'\\s+', header=None)[0]\n",
        "x_test  = pd.read_csv('/content/x_test.txt',  sep=r'\\s+', header=None)\n",
        "\n",
        "stats = []\n",
        "for i in x_train.columns:\n",
        "    t_mean = x_train[i].mean()\n",
        "    s_mean = x_test[i].mean()\n",
        "    t_var  = x_train[i].var()\n",
        "    s_var  = x_test[i].var()\n",
        "    stats.append({\n",
        "        'feature':             i,\n",
        "        'train_mean':          t_mean,\n",
        "        'test_mean':           s_mean,\n",
        "        'mean_diff':           s_mean - t_mean,\n",
        "        'train_variance':      t_var,\n",
        "        'test_variance':       s_var,\n",
        "        'variance_ratio':      (s_var / t_var) if t_var>0 else np.nan,\n",
        "        'corr_with_target':    x_train[i].corr(y_train)\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(stats).set_index('feature')\n",
        "\n",
        "print(summary_df.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "\n",
        "\n",
        "q_low  = x_train.quantile(0.01)\n",
        "q_high = x_train.quantile(0.99)\n",
        "\n",
        "stats = []\n",
        "for col in x_train.columns:\n",
        "    ks_stat, ks_p = ks_2samp(x_train[col], x_test[col])\n",
        "    train_outliers = ((x_train[col] < q_low[col]) | (x_train[col] > q_high[col])).sum()\n",
        "    test_outliers  = ((x_test[col]  < q_low[col]) | (x_test[col]  > q_high[col])).sum()\n",
        "    stats.append({\n",
        "        'feature':        col,\n",
        "        'ks_stat':        ks_stat,\n",
        "        'ks_pvalue':      ks_p,\n",
        "        'train_outliers': train_outliers,\n",
        "        'test_outliers':  test_outliers,\n",
        "        'train_q01':      q_low[col],\n",
        "        'train_q99':      q_high[col],\n",
        "        'train_min':      x_train[col].min(),\n",
        "        'train_max':      x_train[col].max(),\n",
        "        'test_min':       x_test[col].min(),\n",
        "        'test_max':       x_test[col].max(),\n",
        "    })\n",
        "\n",
        "stats_df = pd.DataFrame(stats).set_index('feature')\n",
        "\n",
        "print(\"KS statistics for top 10 ks_stat features:\")\n",
        "print(stats_df.sort_values('ks_stat', ascending=False).head(10))\n",
        "\n",
        "x_train_capped = x_train.clip(lower=q_low, upper=q_high, axis=1)\n",
        "x_test_capped  = x_test.clip( lower=q_low, upper=q_high, axis=1)\n",
        "\n",
        "ks_after = []\n",
        "for col in x_train.columns:\n",
        "    ks2, p2 = ks_2samp(x_train_capped[col], x_test_capped[col])\n",
        "    ks_after.append(ks2)\n",
        "stats_df['ks_after_capping'] = ks_after\n",
        "\n",
        "print(\"\\nKS statistics for top 10 ks_stat features: after capping:\")\n",
        "top10 = stats_df.sort_values('ks_stat', ascending=False).head(10)\n",
        "print(top10[['ks_stat','ks_after_capping']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueQOMfhPMI0h",
        "outputId": "53e08b84-0819-43c4-8337-33fcf53e0c34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KS statistics for top 10 ks_stat features:\n",
            "         ks_stat  ks_pvalue  train_outliers  test_outliers  train_q01  \\\n",
            "feature                                                                 \n",
            "428       0.0426   0.000229             100            112   2.665452   \n",
            "107       0.0368   0.002291             100             97  -2.315544   \n",
            "130       0.0368   0.002291             100            105  -2.249431   \n",
            "135       0.0354   0.003798             100            110  -2.354010   \n",
            "394       0.0340   0.006174             100            156   0.012440   \n",
            "303       0.0338   0.006607             100             76   0.008643   \n",
            "296       0.0336   0.007068             100             92   0.022230   \n",
            "252       0.0324   0.010504             100            108   0.025570   \n",
            "34        0.0314   0.014451             100             99  -2.257608   \n",
            "266       0.0308   0.017417             100            144   0.030595   \n",
            "\n",
            "         train_q99  train_min  train_max  test_min   test_max  \n",
            "feature                                                        \n",
            "428      23.415190   0.830056  34.928336  1.114300  32.063120  \n",
            "107       2.328245  -3.661105   3.565555 -3.407935   3.174591  \n",
            "130       2.302304  -3.641161   3.584047 -3.387836   3.381168  \n",
            "135       2.281959  -3.492702   3.617008 -3.427228   3.703639  \n",
            "394       0.988695   0.000535   0.999467  0.000059   0.999838  \n",
            "303       0.990880   0.000035   0.999969  0.000126   0.999650  \n",
            "296       9.410959   0.000598  18.144028  0.000620  17.965835  \n",
            "252       9.690338   0.000460  15.506521  0.001080  17.864720  \n",
            "34        2.389097  -3.176476   4.311408 -3.753966   3.770609  \n",
            "266       8.854601   0.000885  17.840303  0.000150  17.373207  \n",
            "\n",
            "KS statistics for top 10 ks_stat features: after capping:\n",
            "         ks_stat  ks_after_capping\n",
            "feature                           \n",
            "428       0.0426            0.0426\n",
            "107       0.0368            0.0368\n",
            "130       0.0368            0.0368\n",
            "135       0.0354            0.0354\n",
            "394       0.0340            0.0340\n",
            "303       0.0338            0.0338\n",
            "296       0.0336            0.0336\n",
            "252       0.0324            0.0324\n",
            "34        0.0314            0.0314\n",
            "266       0.0308            0.0308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "\n",
        "q_low, q_high = x_train.quantile(0.01), x_train.quantile(0.99)\n",
        "\n",
        "stats = []\n",
        "for col in x_train.columns:\n",
        "    ks_stat, _ = ks_2samp(x_train[col], x_test[col])\n",
        "    test_out = ((x_test[col] < q_low[col]) | (x_test[col] > q_high[col])).sum()\n",
        "    stats.append({'feature': col, 'ks_stat': ks_stat, 'test_outliers': test_out})\n",
        "stats_df = pd.DataFrame(stats).set_index('feature')\n",
        "\n",
        "ks_thresh = 0.035\n",
        "outlier_thresh = 120\n",
        "flagged = stats_df[\n",
        "    (stats_df['ks_stat'] >= ks_thresh) |\n",
        "    (stats_df['test_outliers'] > outlier_thresh)\n",
        "].index.tolist()\n",
        "print(f\"Flagged features (KS >= {ks_thresh} or test_outliers > {outlier_thresh}):\\n{flagged}\\n\")\n",
        "\n",
        "qt = QuantileTransformer(output_distribution='uniform', random_state=0)\n",
        "combined = pd.concat([x_train[flagged], x_test[flagged]], axis=0)\n",
        "qt.fit(combined)\n",
        "\n",
        "x_train_qt = x_train.copy()\n",
        "x_test_qt  = x_test.copy()\n",
        "x_train_qt[flagged] = qt.transform(x_train[flagged])\n",
        "x_test_qt[flagged]  = qt.transform(x_test[flagged])\n",
        "\n",
        "X_adv = pd.concat([x_train_qt[flagged], x_test_qt[flagged]], axis=0)\n",
        "y_adv = np.concatenate([np.zeros(len(x_train_qt)), np.ones(len(x_test_qt))])\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_adv, y_adv, test_size=0.3, random_state=0, stratify=y_adv\n",
        ")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf.fit(X_tr, y_tr)\n",
        "\n",
        "y_prob = rf.predict_proba(X_val)[:,1]\n",
        "print(f\"Adversarial AUC: {roc_auc_score(y_val, y_prob):.3f}, \"\n",
        "      f\"Accuracy: {accuracy_score(y_val, rf.predict(X_val)):.3f}\\n\")\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=flagged).sort_values(ascending=False)\n",
        "print(\"Top 10 adversarial-important features:\")\n",
        "print(importances.head(10), \"\\n\")\n",
        "\n",
        "new_stats = []\n",
        "for col in flagged:\n",
        "    ks2, _ = ks_2samp(x_train_qt[col], x_test_qt[col])\n",
        "    new_stats.append({'feature': col, 'ks_after': ks2})\n",
        "new_df = pd.DataFrame(new_stats).set_index('feature')\n",
        "print(\"KS after QuantileTransform:\")\n",
        "print(new_df, \"\\n\")\n",
        "\n",
        "to_drop = new_df[new_df['ks_after'] > ks_thresh].index.tolist()\n",
        "keep_after = [f for f in flagged if f not in to_drop]\n",
        "print(f\"Features to drop (ks_after > {ks_thresh}): {to_drop}\")\n",
        "print(f\"Features to keep: {keep_after}\\n\")\n",
        "\n",
        "x_train_processed = x_train_qt.drop(columns=to_drop)\n",
        "x_test_processed  = x_test_qt.drop(columns=to_drop)\n",
        "\n",
        "print(f\"Final shapes --> x_train: {x_train_processed.shape}, x_test: {x_test_processed.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXreImYIMLsP",
        "outputId": "47c26ca0-53a1-44f4-a434-950242d3c685"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flagged features (KS >= 0.035 or test_outliers > 120):\n",
            "[11, 20, 24, 44, 47, 50, 55, 107, 114, 117, 121, 122, 123, 127, 130, 133, 135, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 428, 430, 434, 439, 464]\n",
            "\n",
            "Adversarial AUC: 0.501, Accuracy: 0.490\n",
            "\n",
            "Top 10 adversarial-important features:\n",
            "130    0.022881\n",
            "107    0.022392\n",
            "428    0.022245\n",
            "388    0.022131\n",
            "135    0.021990\n",
            "184    0.021944\n",
            "117    0.021707\n",
            "394    0.021654\n",
            "204    0.021605\n",
            "232    0.021597\n",
            "dtype: float64 \n",
            "\n",
            "KS after QuantileTransform:\n",
            "         ks_after\n",
            "feature          \n",
            "11         0.0078\n",
            "20         0.0186\n",
            "24         0.0134\n",
            "44         0.0206\n",
            "47         0.0164\n",
            "50         0.0206\n",
            "55         0.0242\n",
            "107        0.0368\n",
            "114        0.0084\n",
            "117        0.0218\n",
            "121        0.0184\n",
            "122        0.0178\n",
            "123        0.0180\n",
            "127        0.0196\n",
            "130        0.0368\n",
            "133        0.0172\n",
            "135        0.0354\n",
            "163        0.0120\n",
            "175        0.0176\n",
            "177        0.0106\n",
            "184        0.0180\n",
            "188        0.0114\n",
            "193        0.0166\n",
            "198        0.0084\n",
            "204        0.0204\n",
            "214        0.0230\n",
            "225        0.0190\n",
            "232        0.0102\n",
            "244        0.0118\n",
            "253        0.0188\n",
            "264        0.0166\n",
            "266        0.0308\n",
            "279        0.0226\n",
            "342        0.0134\n",
            "343        0.0100\n",
            "354        0.0198\n",
            "388        0.0166\n",
            "390        0.0172\n",
            "394        0.0340\n",
            "399        0.0280\n",
            "404        0.0156\n",
            "413        0.0142\n",
            "426        0.0138\n",
            "428        0.0426\n",
            "430        0.0240\n",
            "434        0.0096\n",
            "439        0.0184\n",
            "464        0.0218 \n",
            "\n",
            "Features to drop (ks_after > 0.035): [107, 130, 135, 428]\n",
            "Features to keep: [11, 20, 24, 44, 47, 50, 55, 114, 117, 121, 122, 123, 127, 133, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 430, 434, 439, 464]\n",
            "\n",
            "Final shapes --> x_train: (5000, 496), x_test: (5000, 496)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "sel = VarianceThreshold(threshold=1e-5)\n",
        "sel.fit(x_train_processed)\n",
        "keep_var = x_train_processed.columns[sel.get_support()]\n",
        "drop_var = [c for c in x_train_processed.columns if c not in keep_var]\n",
        "print(\"Dropped for near-zero variance:\", drop_var)\n",
        "x_train_var = x_train_processed[keep_var].copy()\n",
        "x_test_var  = x_test_processed[keep_var].copy()\n",
        "\n",
        "corr = x_train_var.corr().abs()\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "drop_corr = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
        "print(\"Dropped for high correlation:\", drop_corr)\n",
        "x_train_corr = x_train_var.drop(columns=drop_corr).copy()\n",
        "x_test_corr  = x_test_var.drop(columns=drop_corr).copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "combined = pd.concat([x_train_corr, x_test_corr], axis=0)\n",
        "scaler.fit(combined)\n",
        "\n",
        "x_train_final = pd.DataFrame(\n",
        "    scaler.transform(x_train_corr),\n",
        "    columns=x_train_corr.columns,\n",
        "    index=x_train_corr.index\n",
        ")\n",
        "x_test_final = pd.DataFrame(\n",
        "    scaler.transform(x_test_corr),\n",
        "    columns=x_test_corr.columns,\n",
        "    index=x_test_corr.index\n",
        ")\n",
        "\n",
        "print(\"Final shapes -->\", x_train_final.shape, x_test_final.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI0DrgtVMQeh",
        "outputId": "2bfb8242-9f11-4bb8-9717-93b1fc2b29f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped for near-zero variance: []\n",
            "Dropped for high correlation: [7]\n",
            "Final shapes --> (5000, 495) (5000, 495)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpf68SLNvLio",
        "outputId": "a9a882bf-30f9-4d43-b4f8-d15f16f42e6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import shap\n",
        "\n",
        "\n",
        "def net_score(y_true, y_pred, n_features):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return 10 * acc * len(y_true) - 200 * n_features\n",
        "\n",
        "def objective(trial):\n",
        "    C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
        "    threshold = trial.suggest_float('threshold', 0.4, 0.6)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[train_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = LogisticRegression(\n",
        "            penalty='l1', solver='saga', C=C,\n",
        "            max_iter=10000, random_state=0\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = np.count_nonzero(model.coef_)\n",
        "\n",
        "        scores.append(net_score(y_val, preds, n_feats))\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "best_params = study.best_params\n",
        "best_C = best_params['C']\n",
        "best_threshold = best_params['threshold']\n",
        "print(f\"Best params C: {best_C:.5f}, threshold: {best_threshold:.2f}\")\n",
        "\n",
        "final_model = LogisticRegression(\n",
        "    penalty='l1', solver='saga', C=best_C,\n",
        "    max_iter=10000, random_state=0\n",
        ")\n",
        "final_model.fit(x_train_final, y_train)\n",
        "\n",
        "test_probs = final_model.predict_proba(x_test_final)[:, 1]\n",
        "y_test_pred = (test_probs >= best_threshold).astype(int)\n",
        "\n",
        "explainer = shap.LinearExplainer(\n",
        "    final_model,\n",
        "    x_train_final,\n",
        "    feature_perturbation=\"interventional\"\n",
        ")\n",
        "shap_values = explainer.shap_values(x_train_final)\n",
        "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "shap_imp = pd.Series(mean_abs_shap, index=x_train_final.columns)\n",
        "shap_imp = shap_imp.sort_values(ascending=False)\n",
        "print(\"\\nTop 10 features by mean |SHAP|:\")\n",
        "print(shap_imp.head(10))\n",
        "\n",
        "B = 100\n",
        "rng = np.random.RandomState(0)\n",
        "feat_count = pd.Series(0, index=x_train_final.columns)\n",
        "\n",
        "for i in range(B):\n",
        "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
        "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
        "    m = LogisticRegression(\n",
        "        penalty='l1', solver='saga', C=best_C,\n",
        "        max_iter=10000, random_state=i\n",
        "    )\n",
        "    m.fit(Xb, yb)\n",
        "    sel = x_train_final.columns[m.coef_[0] != 0]\n",
        "    feat_count[sel] += 1\n",
        "\n",
        "stability = (feat_count / B).sort_values(ascending=False)\n",
        "print(\"\\nBootstrap selection frequency (top 10):\")\n",
        "print(stability.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTlsGJD7MTv9",
        "outputId": "194cbf99-766c-4278-c739-37434402ea45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 13:54:28,141] A new study created in memory with name: no-name-5db40622-c652-436f-a0fb-38d9d38ae953\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:30,923] Trial 0 finished with value: 5792.0 and parameters: {'C': 0.002438094350364712, 'threshold': 0.4152312827398985}. Best is trial 0 with value: 5792.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:36,644] Trial 1 finished with value: 6548.0 and parameters: {'C': 0.008466416294971805, 'threshold': 0.4523414734403631}. Best is trial 1 with value: 6548.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:40,211] Trial 2 finished with value: 4686.0 and parameters: {'C': 0.001352001747920444, 'threshold': 0.4329609410259692}. Best is trial 1 with value: 6548.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:45,622] Trial 3 finished with value: 6324.0 and parameters: {'C': 0.008237084210830125, 'threshold': 0.4143799559785441}. Best is trial 1 with value: 6548.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:49,775] Trial 4 finished with value: 6176.0 and parameters: {'C': 0.0057948863168633844, 'threshold': 0.40580293276829504}. Best is trial 1 with value: 6548.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:51,716] Trial 5 finished with value: 5114.0 and parameters: {'C': 0.0012776349900466971, 'threshold': 0.5801415611466907}. Best is trial 1 with value: 6548.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:54,473] Trial 6 finished with value: 6596.0 and parameters: {'C': 0.0034689824042374155, 'threshold': 0.523438335836739}. Best is trial 6 with value: 6596.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:55,898] Trial 7 finished with value: 5114.0 and parameters: {'C': 0.0010629939690590838, 'threshold': 0.517196288625469}. Best is trial 6 with value: 6596.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:54:58,988] Trial 8 finished with value: 6354.0 and parameters: {'C': 0.0020980105260704146, 'threshold': 0.4534652219683908}. Best is trial 6 with value: 6596.0.\n",
            "<ipython-input-13-41d9fe053d56>:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
            "[I 2025-06-01 13:55:02,631] Trial 9 finished with value: 6356.0 and parameters: {'C': 0.00480000555722477, 'threshold': 0.42152952647162856}. Best is trial 6 with value: 6596.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params C: 0.00347, threshold: 0.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/shap/explainers/_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
            "  warnings.warn(wmsg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 features by mean |SHAP|:\n",
            "2      0.445865\n",
            "342    0.000000\n",
            "341    0.000000\n",
            "340    0.000000\n",
            "339    0.000000\n",
            "338    0.000000\n",
            "337    0.000000\n",
            "336    0.000000\n",
            "335    0.000000\n",
            "334    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Bootstrap selection frequency (top 10):\n",
            "2      1.00\n",
            "6      0.35\n",
            "414    0.01\n",
            "341    0.00\n",
            "340    0.00\n",
            "339    0.00\n",
            "338    0.00\n",
            "337    0.00\n",
            "336    0.00\n",
            "335    0.00\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "import shap\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def net_score(y_true, y_pred, n_features):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return 10 * acc * len(y_true) - 200 * n_features\n",
        "\n",
        "def count_tree_features(model):\n",
        "    imp = model.feature_importances_\n",
        "    return np.count_nonzero(imp)\n",
        "\n",
        "def objective_xgb(trial):\n",
        "    lambda_l1 = trial.suggest_float('lambda_l1', 50.0, 200.0, log=True)\n",
        "    max_depth = trial.suggest_int('max_depth', 2, 4)\n",
        "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 0.3)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "    estimators = trial.suggest_int('n_estimators', 500, 2000)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=estimators,\n",
        "            eval_metric='logloss',\n",
        "            reg_lambda=lambda_l1,\n",
        "            reg_alpha=0.0,\n",
        "            max_depth=max_depth,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            random_state=0, n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = count_tree_features(model)\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=10)\n",
        "best_xgb = study_xgb.best_params\n",
        "best_l1_xgb = best_xgb['lambda_l1']\n",
        "best_depth_xgb = best_xgb['max_depth']\n",
        "best_col_xgb = best_xgb['colsample_bytree']\n",
        "best_thr_xgb = best_xgb['threshold']\n",
        "best_estimators_xgb = best_xgb['n_estimators']\n",
        "\n",
        "print(f\"\\nBest XGBoost params: lambda_l1 = {best_l1_xgb:.2f}, max_depth = {best_depth_xgb}, colsample_bytree = {best_col_xgb:.2f}, threshold = {best_thr_xgb:.2f}, n_estimators = {best_estimators_xgb}\")\n",
        "\n",
        "model_xgb = XGBClassifier(\n",
        "    n_estimators=best_estimators_xgb,\n",
        "    eval_metric='logloss',\n",
        "    reg_lambda=best_l1_xgb,\n",
        "    max_depth=best_depth_xgb,\n",
        "    colsample_bytree=best_col_xgb,\n",
        "    random_state=0, n_jobs=-1\n",
        ")\n",
        "model_xgb.fit(x_train_final, y_train)\n",
        "\n",
        "print(\"\\nFinal model tuning results:\")\n",
        "print(f\"XGBoost: lambda_l1 = {best_l1_xgb:.2f}, max_depth = {best_depth_xgb}, colsample_bytree = {best_col_xgb:.2f}, thresh = {best_thr_xgb:.2f}, #feat = {count_tree_features(model_xgb)}\")\n",
        "\n",
        "# SHAP analysis\n",
        "print(\"\\nXGBoost analysis\")\n",
        "test_probs = model_xgb.predict_proba(x_test_final)[:, 1]\n",
        "y_test_pred = (test_probs >= best_thr_xgb).astype(int)\n",
        "frac_pos = np.mean(y_test_pred)\n",
        "print(f\"Threshold = {best_thr_xgb:.2f}, # positives = {int(frac_pos * len(test_probs))} / {len(test_probs)}\")\n",
        "\n",
        "explainer = shap.TreeExplainer(model_xgb, x_train_final)\n",
        "shap_vals = explainer.shap_values(x_train_final)\n",
        "if isinstance(shap_vals, list):\n",
        "    shap_vals = shap_vals[1]\n",
        "mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
        "shap_imp = pd.Series(mean_abs_shap, index=x_train_final.columns).sort_values(ascending=False)\n",
        "print(\"\\nTop 10 features by mean |SHAP|:\")\n",
        "print(shap_imp.head(10))\n",
        "\n",
        "# Bootstrap feature stability\n",
        "B = 100\n",
        "rng = np.random.RandomState(0)\n",
        "feat_count = pd.Series(0, index=x_train_final.columns)\n",
        "for i in range(B):\n",
        "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
        "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
        "    m = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        eval_metric='logloss',\n",
        "        reg_lambda=best_l1_xgb, max_depth=best_depth_xgb,\n",
        "        colsample_bytree=best_col_xgb,\n",
        "        random_state=i, n_jobs=-1\n",
        "    )\n",
        "    m.fit(Xb, yb)\n",
        "    sel = x_train_final.columns[m.feature_importances_ > 0]\n",
        "    feat_count[sel] += 1\n",
        "\n",
        "stability = (feat_count / B).sort_values(ascending=False)\n",
        "print(\"\\nBootstrap selection frequency (top 10):\")\n",
        "print(stability.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPDgWwUJMkR1",
        "outputId": "c581b054-4684-48f2-b481-9c3212e2ea90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-01 14:45:28,513] A new study created in memory with name: no-name-46b033a7-070e-437d-bbe2-d5be07a868d8\n",
            "[I 2025-06-01 14:46:09,055] Trial 0 finished with value: -92398.0 and parameters: {'lambda_l1': 75.11144515942499, 'max_depth': 2, 'colsample_bytree': 0.2675973065638475, 'threshold': 0.6865588228768184, 'n_estimators': 1774}. Best is trial 0 with value: -92398.0.\n",
            "[I 2025-06-01 14:46:35,848] Trial 1 finished with value: -91082.0 and parameters: {'lambda_l1': 89.63160656170535, 'max_depth': 2, 'colsample_bytree': 0.2501350223394448, 'threshold': 0.4826623980995811, 'n_estimators': 1108}. Best is trial 1 with value: -91082.0.\n",
            "[I 2025-06-01 14:47:23,075] Trial 2 finished with value: -92342.0 and parameters: {'lambda_l1': 160.4212700574204, 'max_depth': 3, 'colsample_bytree': 0.245366678377152, 'threshold': 0.3530274552643515, 'n_estimators': 1338}. Best is trial 1 with value: -91082.0.\n",
            "[I 2025-06-01 14:48:07,691] Trial 3 finished with value: -92206.0 and parameters: {'lambda_l1': 144.6498432176954, 'max_depth': 2, 'colsample_bytree': 0.2435493641534208, 'threshold': 0.671383586258757, 'n_estimators': 1749}. Best is trial 1 with value: -91082.0.\n",
            "[I 2025-06-01 14:48:38,605] Trial 4 finished with value: -91424.0 and parameters: {'lambda_l1': 156.58155304219508, 'max_depth': 2, 'colsample_bytree': 0.2278688009702493, 'threshold': 0.4987948440676574, 'n_estimators': 1231}. Best is trial 1 with value: -91082.0.\n",
            "[I 2025-06-01 14:48:52,736] Trial 5 finished with value: -83188.0 and parameters: {'lambda_l1': 193.37482095280157, 'max_depth': 2, 'colsample_bytree': 0.16142370899203348, 'threshold': 0.5161243661209094, 'n_estimators': 606}. Best is trial 5 with value: -83188.0.\n",
            "[I 2025-06-01 14:49:11,746] Trial 6 finished with value: -89224.0 and parameters: {'lambda_l1': 151.94623179385331, 'max_depth': 2, 'colsample_bytree': 0.12794992084107534, 'threshold': 0.6592250754384439, 'n_estimators': 851}. Best is trial 5 with value: -83188.0.\n",
            "[I 2025-06-01 14:49:44,825] Trial 7 finished with value: -91786.0 and parameters: {'lambda_l1': 50.65360460330541, 'max_depth': 2, 'colsample_bytree': 0.24967390095708156, 'threshold': 0.3514658411249527, 'n_estimators': 1313}. Best is trial 5 with value: -83188.0.\n",
            "[I 2025-06-01 14:50:47,434] Trial 8 finished with value: -92240.0 and parameters: {'lambda_l1': 151.85993673977043, 'max_depth': 3, 'colsample_bytree': 0.1829133894810087, 'threshold': 0.5305293289917726, 'n_estimators': 1922}. Best is trial 5 with value: -83188.0.\n",
            "[I 2025-06-01 14:51:40,964] Trial 9 finished with value: -92200.0 and parameters: {'lambda_l1': 191.87246187496558, 'max_depth': 3, 'colsample_bytree': 0.27712823401024345, 'threshold': 0.5189077168071498, 'n_estimators': 1488}. Best is trial 5 with value: -83188.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best XGBoost params: lambda_l1 = 193.37, max_depth = 2, colsample_bytree = 0.16, threshold = 0.52, n_estimators = 606\n",
            "\n",
            "Final model tuning results:\n",
            "XGBoost: lambda_l1 = 193.37, max_depth = 2, colsample_bytree = 0.16, thresh = 0.52, #feat = 459\n",
            "\n",
            "XGBoost analysis\n",
            "Threshold = 0.52, # positives = 2371 / 5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|===================| 4922/5000 [00:28<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 features by mean |SHAP|:\n",
            "2      0.511079\n",
            "462    0.219063\n",
            "6      0.178071\n",
            "8      0.174194\n",
            "3      0.134786\n",
            "298    0.091659\n",
            "127    0.083215\n",
            "348    0.077653\n",
            "180    0.077067\n",
            "332    0.076496\n",
            "dtype: float64\n",
            "\n",
            "Bootstrap selection frequency (top 10):\n",
            "2      1.00\n",
            "6      1.00\n",
            "462    1.00\n",
            "228    0.99\n",
            "414    0.98\n",
            "298    0.97\n",
            "8      0.93\n",
            "425    0.92\n",
            "4      0.91\n",
            "127    0.88\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_subset(feats):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    all_true, all_prob = [], []\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr = x_train_final.iloc[tr_idx][feats]\n",
        "        y_tr = y_train.iloc[tr_idx]\n",
        "        X_val = x_train_final.iloc[val_idx][feats]\n",
        "        y_val = y_train.iloc[val_idx]\n",
        "\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=best_estimators_xgb,\n",
        "            eval_metric='logloss',\n",
        "            reg_lambda=best_l1_xgb,\n",
        "            max_depth=best_depth_xgb,\n",
        "            colsample_bytree=best_col_xgb,\n",
        "            random_state=0, n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        all_true.append(y_val.values)\n",
        "        all_prob.append(model.predict_proba(X_val)[:,1])\n",
        "\n",
        "    all_true = np.concatenate(all_true)\n",
        "    all_prob = np.concatenate(all_prob)\n",
        "\n",
        "    best_net, best_thr = -np.inf, 0.5\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        preds = (all_prob >= thr).astype(int)\n",
        "        net = net_score(all_true, preds, len(feats))\n",
        "        if net > best_net:\n",
        "            best_net, best_thr = net, thr\n",
        "    return best_net, best_thr\n",
        "\n",
        "candidates = [[2], [2, 6]]\n",
        "\n",
        "best_net = -np.inf\n",
        "winning_subset = None\n",
        "winning_thr = None\n",
        "\n",
        "for subset in candidates:\n",
        "    net, thr = evaluate_subset(subset)\n",
        "    print(f\"Subset {subset}: CV net-score = {net:.1f} and threshold {thr:.2f}\")\n",
        "    if net > best_net:\n",
        "        best_net, winning_subset, winning_thr = net, subset, thr\n",
        "\n",
        "print(f\"\\nWinning subset: {winning_subset} with net-score {best_net:.1f} and threshold {winning_thr:.2f}\")\n",
        "\n",
        "final = XGBClassifier(\n",
        "    n_estimators=best_estimators_xgb,\n",
        "    eval_metric='logloss',\n",
        "    reg_lambda=best_l1_xgb,\n",
        "    max_depth=best_depth_xgb,\n",
        "    colsample_bytree=best_col_xgb,\n",
        "    random_state=0, n_jobs=-1\n",
        ")\n",
        "final.fit(x_train_final[winning_subset], y_train)\n",
        "\n",
        "y_test_pred = (final.predict_proba(x_test_final[winning_subset])[:,1] >= winning_thr).astype(int)\n",
        "\n",
        "print(f\"Final XGBoost model trained with features {winning_subset}; test predictions ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLfqH5Q69i9y",
        "outputId": "ce7d7e6f-210c-4679-fa0c-4805ac8286a8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset [2]: CV net-score = 35130.0 and threshold 0.55\n",
            "Subset [2, 6]: CV net-score = 34850.0 and threshold 0.53\n",
            "\n",
            "Winning subset: [2] with net-score 35130.0 and threshold 0.55\n",
            "Final XGBoost model trained with features [2]; test predictions ready.\n"
          ]
        }
      ]
    }
  ]
}