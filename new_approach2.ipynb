{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipiFsA6YCtyi",
    "outputId": "1dbb196c-3ab8-4be7-afd4-f1a3b71f0afc",
    "ExecuteTime": {
     "end_time": "2025-06-02T08:20:30.497273Z",
     "start_time": "2025-06-02T08:20:28.963169Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_train = pd.read_csv('data/x_train.txt', sep=r'\\s+', header=None)\n",
    "y_train = pd.read_csv('data/y_train.txt', sep=r'\\s+', header=None)[0]\n",
    "x_test  = pd.read_csv('data/x_test.txt',  sep=r'\\s+', header=None)\n",
    "\n",
    "stats = []\n",
    "for i in x_train.columns:\n",
    "    t_mean = x_train[i].mean()\n",
    "    s_mean = x_test[i].mean()\n",
    "    t_var  = x_train[i].var()\n",
    "    s_var  = x_test[i].var()\n",
    "    stats.append({\n",
    "        'feature':             i,\n",
    "        'train_mean':          t_mean,\n",
    "        'test_mean':           s_mean,\n",
    "        'mean_diff':           s_mean - t_mean,\n",
    "        'train_variance':      t_var,\n",
    "        'test_variance':       s_var,\n",
    "        'variance_ratio':      (s_var / t_var) if t_var>0 else np.nan,\n",
    "        'corr_with_target':    x_train[i].corr(y_train)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(stats).set_index('feature')\n",
    "\n",
    "print(summary_df.head(10))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         train_mean  test_mean  mean_diff  train_variance  test_variance  \\\n",
      "feature                                                                    \n",
      "0         15.560411  15.507078  -0.053333       18.730704      17.260227   \n",
      "1         12.650449  12.655507   0.005058       14.317654      13.704232   \n",
      "2         27.750084  27.736016  -0.014067       48.258792      44.019127   \n",
      "3         18.796808  18.825133   0.028325       24.323544      22.632589   \n",
      "4         19.071302  18.995343  -0.075959       27.471500      25.135808   \n",
      "5         11.820110  11.769083  -0.051027       13.312292      12.482966   \n",
      "6         19.365360  19.355964  -0.009395       28.260874      25.857634   \n",
      "7         15.602632  15.517396  -0.085235       19.669401      18.001512   \n",
      "8         14.163618  14.233636   0.070018       19.810112      19.051235   \n",
      "9         15.989661  16.041487   0.051826       22.978933      22.298445   \n",
      "\n",
      "         variance_ratio  corr_with_target  \n",
      "feature                                    \n",
      "0              0.921494          0.300607  \n",
      "1              0.957156          0.290802  \n",
      "2              0.912147          0.374769  \n",
      "3              0.930481          0.339719  \n",
      "4              0.914978          0.316651  \n",
      "5              0.937702          0.301198  \n",
      "6              0.914962          0.338679  \n",
      "7              0.915204          0.320353  \n",
      "8              0.961692          0.307160  \n",
      "9              0.970386          0.283087  \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "\n",
    "q_low  = x_train.quantile(0.01)\n",
    "q_high = x_train.quantile(0.99)\n",
    "\n",
    "stats = []\n",
    "for col in x_train.columns:\n",
    "    ks_stat, ks_p = ks_2samp(x_train[col], x_test[col])\n",
    "    train_outliers = ((x_train[col] < q_low[col]) | (x_train[col] > q_high[col])).sum()\n",
    "    test_outliers  = ((x_test[col]  < q_low[col]) | (x_test[col]  > q_high[col])).sum()\n",
    "    stats.append({\n",
    "        'feature':        col,\n",
    "        'ks_stat':        ks_stat,\n",
    "        'ks_pvalue':      ks_p,\n",
    "        'train_outliers': train_outliers,\n",
    "        'test_outliers':  test_outliers,\n",
    "        'train_q01':      q_low[col],\n",
    "        'train_q99':      q_high[col],\n",
    "        'train_min':      x_train[col].min(),\n",
    "        'train_max':      x_train[col].max(),\n",
    "        'test_min':       x_test[col].min(),\n",
    "        'test_max':       x_test[col].max(),\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats).set_index('feature')\n",
    "\n",
    "print(\"KS statistics for top 10 ks_stat features:\")\n",
    "print(stats_df.sort_values('ks_stat', ascending=False).head(10))\n",
    "\n",
    "x_train_capped = x_train.clip(lower=q_low, upper=q_high, axis=1)\n",
    "x_test_capped  = x_test.clip( lower=q_low, upper=q_high, axis=1)\n",
    "\n",
    "ks_after = []\n",
    "for col in x_train.columns:\n",
    "    ks2, p2 = ks_2samp(x_train_capped[col], x_test_capped[col])\n",
    "    ks_after.append(ks2)\n",
    "stats_df['ks_after_capping'] = ks_after\n",
    "\n",
    "print(\"\\nKS statistics for top 10 ks_stat features: after capping:\")\n",
    "top10 = stats_df.sort_values('ks_stat', ascending=False).head(10)\n",
    "print(top10[['ks_stat','ks_after_capping']])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueQOMfhPMI0h",
    "outputId": "53e08b84-0819-43c4-8337-33fcf53e0c34",
    "ExecuteTime": {
     "end_time": "2025-06-02T08:20:37.653168Z",
     "start_time": "2025-06-02T08:20:35.178442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistics for top 10 ks_stat features:\n",
      "         ks_stat  ks_pvalue  train_outliers  test_outliers  train_q01  \\\n",
      "feature                                                                 \n",
      "428       0.0426   0.000229             100            112   2.665452   \n",
      "130       0.0368   0.002291             100            105  -2.249431   \n",
      "107       0.0368   0.002291             100             97  -2.315544   \n",
      "135       0.0354   0.003798             100            110  -2.354010   \n",
      "394       0.0340   0.006174             100            156   0.012440   \n",
      "303       0.0338   0.006607             100             76   0.008643   \n",
      "296       0.0336   0.007068             100             92   0.022230   \n",
      "252       0.0324   0.010504             100            108   0.025570   \n",
      "34        0.0314   0.014451             100             99  -2.257608   \n",
      "266       0.0308   0.017417             100            144   0.030595   \n",
      "\n",
      "         train_q99  train_min  train_max  test_min   test_max  \n",
      "feature                                                        \n",
      "428      23.415190   0.830056  34.928336  1.114300  32.063120  \n",
      "130       2.302304  -3.641161   3.584047 -3.387836   3.381168  \n",
      "107       2.328245  -3.661105   3.565555 -3.407935   3.174591  \n",
      "135       2.281959  -3.492702   3.617008 -3.427228   3.703639  \n",
      "394       0.988695   0.000535   0.999467  0.000059   0.999838  \n",
      "303       0.990880   0.000035   0.999969  0.000126   0.999650  \n",
      "296       9.410959   0.000598  18.144028  0.000620  17.965835  \n",
      "252       9.690338   0.000460  15.506521  0.001080  17.864720  \n",
      "34        2.389097  -3.176476   4.311408 -3.753966   3.770609  \n",
      "266       8.854601   0.000885  17.840303  0.000150  17.373207  \n",
      "\n",
      "KS statistics for top 10 ks_stat features: after capping:\n",
      "         ks_stat  ks_after_capping\n",
      "feature                           \n",
      "428       0.0426            0.0426\n",
      "130       0.0368            0.0368\n",
      "107       0.0368            0.0368\n",
      "135       0.0354            0.0354\n",
      "394       0.0340            0.0340\n",
      "303       0.0338            0.0338\n",
      "296       0.0336            0.0336\n",
      "252       0.0324            0.0324\n",
      "34        0.0314            0.0314\n",
      "266       0.0308            0.0308\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "q_low, q_high = x_train.quantile(0.01), x_train.quantile(0.99)\n",
    "\n",
    "stats = []\n",
    "for col in x_train.columns:\n",
    "    ks_stat, _ = ks_2samp(x_train[col], x_test[col])\n",
    "    test_out = ((x_test[col] < q_low[col]) | (x_test[col] > q_high[col])).sum()\n",
    "    stats.append({'feature': col, 'ks_stat': ks_stat, 'test_outliers': test_out})\n",
    "stats_df = pd.DataFrame(stats).set_index('feature')\n",
    "\n",
    "ks_thresh = 0.035\n",
    "outlier_thresh = 120\n",
    "flagged = stats_df[\n",
    "    (stats_df['ks_stat'] >= ks_thresh) |\n",
    "    (stats_df['test_outliers'] > outlier_thresh)\n",
    "].index.tolist()\n",
    "print(f\"Flagged features (KS >= {ks_thresh} or test_outliers > {outlier_thresh}):\\n{flagged}\\n\")\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='uniform', random_state=0)\n",
    "combined = pd.concat([x_train[flagged], x_test[flagged]], axis=0)\n",
    "qt.fit(combined)\n",
    "\n",
    "x_train_qt = x_train.copy()\n",
    "x_test_qt  = x_test.copy()\n",
    "x_train_qt[flagged] = qt.transform(x_train[flagged])\n",
    "x_test_qt[flagged]  = qt.transform(x_test[flagged])\n",
    "\n",
    "X_adv = pd.concat([x_train_qt[flagged], x_test_qt[flagged]], axis=0)\n",
    "y_adv = np.concatenate([np.zeros(len(x_train_qt)), np.ones(len(x_test_qt))])\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_adv, y_adv, test_size=0.3, random_state=0, stratify=y_adv\n",
    ")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_tr, y_tr)\n",
    "\n",
    "y_prob = rf.predict_proba(X_val)[:,1]\n",
    "print(f\"Adversarial AUC: {roc_auc_score(y_val, y_prob):.3f}, \"\n",
    "      f\"Accuracy: {accuracy_score(y_val, rf.predict(X_val)):.3f}\\n\")\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=flagged).sort_values(ascending=False)\n",
    "print(\"Top 10 adversarial-important features:\")\n",
    "print(importances.head(10), \"\\n\")\n",
    "\n",
    "new_stats = []\n",
    "for col in flagged:\n",
    "    ks2, _ = ks_2samp(x_train_qt[col], x_test_qt[col])\n",
    "    new_stats.append({'feature': col, 'ks_after': ks2})\n",
    "new_df = pd.DataFrame(new_stats).set_index('feature')\n",
    "print(\"KS after QuantileTransform:\")\n",
    "print(new_df, \"\\n\")\n",
    "\n",
    "to_drop = new_df[new_df['ks_after'] > ks_thresh].index.tolist()\n",
    "keep_after = [f for f in flagged if f not in to_drop]\n",
    "print(f\"Features to drop (ks_after > {ks_thresh}): {to_drop}\")\n",
    "print(f\"Features to keep: {keep_after}\\n\")\n",
    "\n",
    "x_train_processed = x_train_qt.drop(columns=to_drop)\n",
    "x_test_processed  = x_test_qt.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Final shapes --> x_train: {x_train_processed.shape}, x_test: {x_test_processed.shape}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXreImYIMLsP",
    "outputId": "47c26ca0-53a1-44f4-a434-950242d3c685",
    "ExecuteTime": {
     "end_time": "2025-06-02T08:20:47.307390Z",
     "start_time": "2025-06-02T08:20:40.988190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged features (KS >= 0.035 or test_outliers > 120):\n",
      "[11, 20, 24, 44, 47, 50, 55, 107, 114, 117, 121, 122, 123, 127, 130, 133, 135, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 428, 430, 434, 439, 464]\n",
      "\n",
      "Adversarial AUC: 0.501, Accuracy: 0.490\n",
      "\n",
      "Top 10 adversarial-important features:\n",
      "130    0.022881\n",
      "107    0.022392\n",
      "428    0.022245\n",
      "388    0.022131\n",
      "135    0.021990\n",
      "184    0.021944\n",
      "117    0.021707\n",
      "394    0.021654\n",
      "204    0.021605\n",
      "232    0.021597\n",
      "dtype: float64 \n",
      "\n",
      "KS after QuantileTransform:\n",
      "         ks_after\n",
      "feature          \n",
      "11         0.0078\n",
      "20         0.0186\n",
      "24         0.0134\n",
      "44         0.0206\n",
      "47         0.0164\n",
      "50         0.0206\n",
      "55         0.0242\n",
      "107        0.0368\n",
      "114        0.0084\n",
      "117        0.0218\n",
      "121        0.0184\n",
      "122        0.0178\n",
      "123        0.0180\n",
      "127        0.0196\n",
      "130        0.0368\n",
      "133        0.0172\n",
      "135        0.0354\n",
      "163        0.0120\n",
      "175        0.0176\n",
      "177        0.0106\n",
      "184        0.0180\n",
      "188        0.0114\n",
      "193        0.0166\n",
      "198        0.0084\n",
      "204        0.0204\n",
      "214        0.0230\n",
      "225        0.0190\n",
      "232        0.0102\n",
      "244        0.0118\n",
      "253        0.0188\n",
      "264        0.0166\n",
      "266        0.0308\n",
      "279        0.0226\n",
      "342        0.0134\n",
      "343        0.0100\n",
      "354        0.0198\n",
      "388        0.0166\n",
      "390        0.0172\n",
      "394        0.0340\n",
      "399        0.0280\n",
      "404        0.0156\n",
      "413        0.0142\n",
      "426        0.0138\n",
      "428        0.0426\n",
      "430        0.0240\n",
      "434        0.0096\n",
      "439        0.0184\n",
      "464        0.0218 \n",
      "\n",
      "Features to drop (ks_after > 0.035): [107, 130, 135, 428]\n",
      "Features to keep: [11, 20, 24, 44, 47, 50, 55, 114, 117, 121, 122, 123, 127, 133, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 430, 434, 439, 464]\n",
      "\n",
      "Final shapes --> x_train: (5000, 496), x_test: (5000, 496)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "sel = VarianceThreshold(threshold=1e-5)\n",
    "sel.fit(x_train_processed)\n",
    "keep_var = x_train_processed.columns[sel.get_support()]\n",
    "drop_var = [c for c in x_train_processed.columns if c not in keep_var]\n",
    "print(\"Dropped for near-zero variance:\", drop_var)\n",
    "x_train_var = x_train_processed[keep_var].copy()\n",
    "x_test_var  = x_test_processed[keep_var].copy()\n",
    "\n",
    "corr = x_train_var.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "drop_corr = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "print(\"Dropped for high correlation:\", drop_corr)\n",
    "x_train_corr = x_train_var.drop(columns=drop_corr).copy()\n",
    "x_test_corr  = x_test_var.drop(columns=drop_corr).copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "combined = pd.concat([x_train_corr, x_test_corr], axis=0)\n",
    "scaler.fit(combined)\n",
    "\n",
    "x_train_final = pd.DataFrame(\n",
    "    scaler.transform(x_train_corr),\n",
    "    columns=x_train_corr.columns,\n",
    "    index=x_train_corr.index\n",
    ")\n",
    "x_test_final = pd.DataFrame(\n",
    "    scaler.transform(x_test_corr),\n",
    "    columns=x_test_corr.columns,\n",
    "    index=x_test_corr.index\n",
    ")\n",
    "\n",
    "print(\"Final shapes -->\", x_train_final.shape, x_test_final.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lI0DrgtVMQeh",
    "outputId": "2bfb8242-9f11-4bb8-9717-93b1fc2b29f7",
    "ExecuteTime": {
     "end_time": "2025-06-02T08:20:55.785881Z",
     "start_time": "2025-06-02T08:20:53.752822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped for near-zero variance: []\n",
      "Dropped for high correlation: [7]\n",
      "Final shapes --> (5000, 495) (5000, 495)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install optuna"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpf68SLNvLio",
    "outputId": "a9a882bf-30f9-4d43-b4f8-d15f16f42e6c"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "\n",
    "\n",
    "def net_score(y_true, y_pred, n_features):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return 10 * acc * len(y_true) - 200 * n_features\n",
    "\n",
    "def objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, X_val = x_train_final.iloc[train_idx], x_train_final.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = LogisticRegression(\n",
    "            penalty='l1', solver='saga', C=C,\n",
    "            max_iter=10000, random_state=0\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        probs = model.predict_proba(X_val)[:, 1]\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        n_feats = np.count_nonzero(model.coef_)\n",
    "\n",
    "        scores.append(net_score(y_val, preds, n_feats))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_C = best_params['C']\n",
    "best_threshold = best_params['threshold']\n",
    "print(f\"Best params C: {best_C:.5f}, threshold: {best_threshold:.2f}\")\n",
    "\n",
    "final_model = LogisticRegression(\n",
    "    penalty='l1', solver='saga', C=best_C,\n",
    "    max_iter=10000, random_state=0\n",
    ")\n",
    "final_model.fit(x_train_final, y_train)\n",
    "\n",
    "test_probs = final_model.predict_proba(x_test_final)[:, 1]\n",
    "y_test_pred = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "explainer = shap.LinearExplainer(\n",
    "    final_model,\n",
    "    x_train_final,\n",
    "    feature_perturbation=\"interventional\"\n",
    ")\n",
    "shap_values = explainer.shap_values(x_train_final)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "shap_imp = pd.Series(mean_abs_shap, index=x_train_final.columns)\n",
    "shap_imp = shap_imp.sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by mean |SHAP|:\")\n",
    "print(shap_imp.head(10))\n",
    "\n",
    "B = 100\n",
    "rng = np.random.RandomState(0)\n",
    "feat_count = pd.Series(0, index=x_train_final.columns)\n",
    "\n",
    "for i in range(B):\n",
    "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
    "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
    "    m = LogisticRegression(\n",
    "        penalty='l1', solver='saga', C=best_C,\n",
    "        max_iter=10000, random_state=i\n",
    "    )\n",
    "    m.fit(Xb, yb)\n",
    "    sel = x_train_final.columns[m.coef_[0] != 0]\n",
    "    feat_count[sel] += 1\n",
    "\n",
    "stability = (feat_count / B).sort_values(ascending=False)\n",
    "print(\"\\nBootstrap selection frequency (top 10):\")\n",
    "print(stability.head(10))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTlsGJD7MTv9",
    "outputId": "194cbf99-766c-4278-c739-37434402ea45",
    "ExecuteTime": {
     "end_time": "2025-06-02T08:22:12.097611Z",
     "start_time": "2025-06-02T08:21:12.595354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-02 10:21:13,069] A new study created in memory with name: no-name-967f0f26-0559-4bc7-9e0e-807f00b2c498\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:14,688] Trial 0 finished with value: 6676.0 and parameters: {'C': 0.004244948564139532, 'threshold': 0.513010733783312}. Best is trial 0 with value: 6676.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:16,241] Trial 1 finished with value: 6002.0 and parameters: {'C': 0.0028034506997998213, 'threshold': 0.5595350989019569}. Best is trial 0 with value: 6676.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:17,049] Trial 2 finished with value: 6330.0 and parameters: {'C': 0.003863402046440928, 'threshold': 0.5452278377537986}. Best is trial 0 with value: 6676.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:18,573] Trial 3 finished with value: 6180.0 and parameters: {'C': 0.0020524145576423752, 'threshold': 0.5251974628795961}. Best is trial 0 with value: 6676.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:21,303] Trial 4 finished with value: 6084.0 and parameters: {'C': 0.00839922992162961, 'threshold': 0.5792607198947478}. Best is trial 0 with value: 6676.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:22,762] Trial 5 finished with value: 6782.0 and parameters: {'C': 0.0036417236939617213, 'threshold': 0.5039644213388912}. Best is trial 5 with value: 6782.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:24,187] Trial 6 finished with value: 6702.0 and parameters: {'C': 0.0035645621838852697, 'threshold': 0.513518959703152}. Best is trial 5 with value: 6782.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:25,786] Trial 7 finished with value: 6814.0 and parameters: {'C': 0.004616603301729729, 'threshold': 0.4962606517726526}. Best is trial 7 with value: 6814.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:27,383] Trial 8 finished with value: 6052.0 and parameters: {'C': 0.004073325831495002, 'threshold': 0.5723792311892569}. Best is trial 7 with value: 6814.0.\n",
      "/tmp/ipykernel_68291/3427876904.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-06-02 10:21:28,732] Trial 9 finished with value: 6210.0 and parameters: {'C': 0.0033762783737376115, 'threshold': 0.42000384292428106}. Best is trial 7 with value: 6814.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params C: 0.00462, threshold: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by mean |SHAP|:\n",
      "2      0.502722\n",
      "343    0.000000\n",
      "341    0.000000\n",
      "340    0.000000\n",
      "339    0.000000\n",
      "338    0.000000\n",
      "337    0.000000\n",
      "336    0.000000\n",
      "335    0.000000\n",
      "334    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Bootstrap selection frequency (top 10):\n",
      "2      1.00\n",
      "6      0.54\n",
      "414    0.04\n",
      "5      0.03\n",
      "8      0.01\n",
      "331    0.00\n",
      "332    0.00\n",
      "333    0.00\n",
      "334    0.00\n",
      "335    0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def net_score(y_true, y_pred, n_features):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return 10 * acc * len(y_true) - 200 * n_features\n",
    "\n",
    "def count_tree_features(model):\n",
    "    imp = model.feature_importances_\n",
    "    return np.count_nonzero(imp)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    lambda_l1 = trial.suggest_float('lambda_l1', 50.0, 200.0, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 4)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 0.3)\n",
    "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
    "    estimators = trial.suggest_int('n_estimators', 500, 2000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    fold_scores = []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=estimators,\n",
    "            eval_metric='logloss',\n",
    "            reg_lambda=lambda_l1,\n",
    "            reg_alpha=0.0,\n",
    "            max_depth=max_depth,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            random_state=0, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        probs = model.predict_proba(X_val)[:, 1]\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        n_feats = count_tree_features(model)\n",
    "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=10)\n",
    "best_xgb = study_xgb.best_params\n",
    "best_l1_xgb = best_xgb['lambda_l1']\n",
    "best_depth_xgb = best_xgb['max_depth']\n",
    "best_col_xgb = best_xgb['colsample_bytree']\n",
    "best_thr_xgb = best_xgb['threshold']\n",
    "best_estimators_xgb = best_xgb['n_estimators']\n",
    "\n",
    "print(f\"\\nBest XGBoost params: lambda_l1 = {best_l1_xgb:.2f}, max_depth = {best_depth_xgb}, colsample_bytree = {best_col_xgb:.2f}, threshold = {best_thr_xgb:.2f}, n_estimators = {best_estimators_xgb}\")\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators=best_estimators_xgb,\n",
    "    eval_metric='logloss',\n",
    "    reg_lambda=best_l1_xgb,\n",
    "    max_depth=best_depth_xgb,\n",
    "    colsample_bytree=best_col_xgb,\n",
    "    random_state=0, n_jobs=-1\n",
    ")\n",
    "model_xgb.fit(x_train_final, y_train)\n",
    "\n",
    "print(\"\\nFinal model tuning results:\")\n",
    "print(f\"XGBoost: lambda_l1 = {best_l1_xgb:.2f}, max_depth = {best_depth_xgb}, colsample_bytree = {best_col_xgb:.2f}, thresh = {best_thr_xgb:.2f}, #feat = {count_tree_features(model_xgb)}\")\n",
    "\n",
    "# SHAP analysis\n",
    "print(\"\\nXGBoost analysis\")\n",
    "test_probs = model_xgb.predict_proba(x_test_final)[:, 1]\n",
    "y_test_pred = (test_probs >= best_thr_xgb).astype(int)\n",
    "frac_pos = np.mean(y_test_pred)\n",
    "print(f\"Threshold = {best_thr_xgb:.2f}, # positives = {int(frac_pos * len(test_probs))} / {len(test_probs)}\")\n",
    "\n",
    "explainer = shap.TreeExplainer(model_xgb, x_train_final)\n",
    "shap_vals = explainer.shap_values(x_train_final)\n",
    "if isinstance(shap_vals, list):\n",
    "    shap_vals = shap_vals[1]\n",
    "mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "shap_imp = pd.Series(mean_abs_shap, index=x_train_final.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by mean |SHAP|:\")\n",
    "print(shap_imp.head(10))\n",
    "\n",
    "# Bootstrap feature stability\n",
    "B = 100\n",
    "rng = np.random.RandomState(0)\n",
    "feat_count = pd.Series(0, index=x_train_final.columns)\n",
    "for i in range(B):\n",
    "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
    "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
    "    m = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        eval_metric='logloss',\n",
    "        reg_lambda=best_l1_xgb, max_depth=best_depth_xgb,\n",
    "        colsample_bytree=best_col_xgb,\n",
    "        random_state=i, n_jobs=-1\n",
    "    )\n",
    "    m.fit(Xb, yb)\n",
    "    sel = x_train_final.columns[m.feature_importances_ > 0]\n",
    "    feat_count[sel] += 1\n",
    "\n",
    "stability = (feat_count / B).sort_values(ascending=False)\n",
    "print(\"\\nBootstrap selection frequency (top 10):\")\n",
    "print(stability.head(10))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPDgWwUJMkR1",
    "outputId": "c581b054-4684-48f2-b481-9c3212e2ea90",
    "ExecuteTime": {
     "end_time": "2025-06-01T17:11:29.578130Z",
     "start_time": "2025-06-01T17:05:16.507735Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-01 19:05:16,554] A new study created in memory with name: no-name-aad542cf-6870-4532-ac47-1f35cdc6ea07\n",
      "[I 2025-06-01 19:05:53,458] Trial 0 finished with value: -92176.0 and parameters: {'lambda_l1': 64.79187284350219, 'max_depth': 4, 'colsample_bytree': 0.13997947586681198, 'threshold': 0.5674105803405487, 'n_estimators': 834}. Best is trial 0 with value: -92176.0.\n",
      "[I 2025-06-01 19:06:31,605] Trial 1 finished with value: -92362.0 and parameters: {'lambda_l1': 86.48215786499351, 'max_depth': 2, 'colsample_bytree': 0.12827430274691726, 'threshold': 0.36389368299450253, 'n_estimators': 1964}. Best is trial 0 with value: -92176.0.\n",
      "[I 2025-06-01 19:07:36,788] Trial 2 finished with value: -92286.0 and parameters: {'lambda_l1': 86.68266131541282, 'max_depth': 4, 'colsample_bytree': 0.10918418277237694, 'threshold': 0.693389824574521, 'n_estimators': 1874}. Best is trial 0 with value: -92176.0.\n",
      "[I 2025-06-01 19:07:56,774] Trial 3 finished with value: -89764.0 and parameters: {'lambda_l1': 63.1426397313178, 'max_depth': 2, 'colsample_bytree': 0.17979354734089195, 'threshold': 0.443475192293636, 'n_estimators': 883}. Best is trial 3 with value: -89764.0.\n",
      "[I 2025-06-01 19:08:20,214] Trial 4 finished with value: -91164.0 and parameters: {'lambda_l1': 160.06996456396317, 'max_depth': 2, 'colsample_bytree': 0.1689937626212386, 'threshold': 0.6105680597048233, 'n_estimators': 1184}. Best is trial 3 with value: -89764.0.\n",
      "[I 2025-06-01 19:08:29,098] Trial 5 finished with value: -81716.0 and parameters: {'lambda_l1': 135.27649641729286, 'max_depth': 2, 'colsample_bytree': 0.1565557861185104, 'threshold': 0.6461168929965427, 'n_estimators': 544}. Best is trial 5 with value: -81716.0.\n",
      "[I 2025-06-01 19:09:06,799] Trial 6 finished with value: -92224.0 and parameters: {'lambda_l1': 193.4786431653512, 'max_depth': 2, 'colsample_bytree': 0.174619175455536, 'threshold': 0.6433688564339932, 'n_estimators': 1979}. Best is trial 5 with value: -81716.0.\n",
      "[I 2025-06-01 19:09:45,713] Trial 7 finished with value: -92260.0 and parameters: {'lambda_l1': 84.7787086234361, 'max_depth': 4, 'colsample_bytree': 0.21346067470655689, 'threshold': 0.3376688270787248, 'n_estimators': 999}. Best is trial 5 with value: -81716.0.\n",
      "[I 2025-06-01 19:10:15,859] Trial 8 finished with value: -91912.0 and parameters: {'lambda_l1': 99.42229756927152, 'max_depth': 2, 'colsample_bytree': 0.2924560225629693, 'threshold': 0.5714810414899123, 'n_estimators': 1588}. Best is trial 5 with value: -81716.0.\n",
      "[I 2025-06-01 19:10:44,745] Trial 9 finished with value: -92356.0 and parameters: {'lambda_l1': 137.97349923063433, 'max_depth': 3, 'colsample_bytree': 0.14195036534753966, 'threshold': 0.6675891894848565, 'n_estimators': 1120}. Best is trial 5 with value: -81716.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best XGBoost params: lambda_l1 = 135.28, max_depth = 2, colsample_bytree = 0.16, threshold = 0.65, n_estimators = 544\n",
      "\n",
      "Final model tuning results:\n",
      "XGBoost: lambda_l1 = 135.28, max_depth = 2, colsample_bytree = 0.16, thresh = 0.65, #feat = 447\n",
      "\n",
      "XGBoost analysis\n",
      "Threshold = 0.65, # positives = 1723 / 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|=================== | 4667/5000 [00:14<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by mean |SHAP|:\n",
      "2      0.515774\n",
      "462    0.216470\n",
      "8      0.183946\n",
      "6      0.179436\n",
      "3      0.166162\n",
      "298    0.101901\n",
      "348    0.086394\n",
      "127    0.084769\n",
      "146    0.081619\n",
      "458    0.077183\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 99\u001B[39m\n\u001B[32m     91\u001B[39m Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n\u001B[32m     92\u001B[39m m = XGBClassifier(\n\u001B[32m     93\u001B[39m     n_estimators=\u001B[32m100\u001B[39m,\n\u001B[32m     94\u001B[39m     eval_metric=\u001B[33m'\u001B[39m\u001B[33mlogloss\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     97\u001B[39m     random_state=i, n_jobs=-\u001B[32m1\u001B[39m\n\u001B[32m     98\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m \u001B[43mm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    100\u001B[39m sel = x_train_final.columns[m.feature_importances_ > \u001B[32m0\u001B[39m]\n\u001B[32m    101\u001B[39m feat_count[sel] += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.12/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.12/site-packages/xgboost/sklearn.py:1682\u001B[39m, in \u001B[36mXGBClassifier.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[39m\n\u001B[32m   1660\u001B[39m model, metric, params, feature_weights = \u001B[38;5;28mself\u001B[39m._configure_fit(\n\u001B[32m   1661\u001B[39m     xgb_model, params, feature_weights\n\u001B[32m   1662\u001B[39m )\n\u001B[32m   1663\u001B[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001B[32m   1664\u001B[39m     missing=\u001B[38;5;28mself\u001B[39m.missing,\n\u001B[32m   1665\u001B[39m     X=X,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1679\u001B[39m     feature_types=\u001B[38;5;28mself\u001B[39m.feature_types,\n\u001B[32m   1680\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m \u001B[38;5;28mself\u001B[39m._Booster = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1683\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1684\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1685\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1686\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1687\u001B[39m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1688\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1689\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1690\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1691\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1692\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1693\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1694\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1696\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.objective):\n\u001B[32m   1697\u001B[39m     \u001B[38;5;28mself\u001B[39m.objective = params[\u001B[33m\"\u001B[39m\u001B[33mobjective\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.12/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.12/site-packages/xgboost/training.py:183\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[39m\n\u001B[32m    181\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001B[32m    182\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m183\u001B[39m \u001B[43mbst\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001B[32m    185\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.12/site-packages/xgboost/core.py:2247\u001B[39m, in \u001B[36mBooster.update\u001B[39m\u001B[34m(self, dtrain, iteration, fobj)\u001B[39m\n\u001B[32m   2243\u001B[39m \u001B[38;5;28mself\u001B[39m._assign_dmatrix_features(dtrain)\n\u001B[32m   2245\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2246\u001B[39m     _check_call(\n\u001B[32m-> \u001B[39m\u001B[32m2247\u001B[39m         \u001B[43m_LIB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2248\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\n\u001B[32m   2249\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2250\u001B[39m     )\n\u001B[32m   2251\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2252\u001B[39m     pred = \u001B[38;5;28mself\u001B[39m.predict(dtrain, output_margin=\u001B[38;5;28;01mTrue\u001B[39;00m, training=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_subset(feats):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr = x_train_final.iloc[tr_idx][feats]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        X_val = x_train_final.iloc[val_idx][feats]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=best_estimators_xgb,\n",
    "            eval_metric='logloss',\n",
    "            reg_lambda=best_l1_xgb,\n",
    "            max_depth=best_depth_xgb,\n",
    "            colsample_bytree=best_col_xgb,\n",
    "            random_state=0, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:,1])\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "\n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "candidates = [[2], [2, 462, 6]]\n",
    "\n",
    "best_net = -np.inf\n",
    "winning_subset = None\n",
    "winning_thr = None\n",
    "\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset(subset)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f} and threshold {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\nWinning subset: {winning_subset} with net-score {best_net:.1f} and threshold {winning_thr:.2f}\")\n",
    "\n",
    "final = XGBClassifier(\n",
    "    n_estimators=best_estimators_xgb,\n",
    "    eval_metric='logloss',\n",
    "    reg_lambda=best_l1_xgb,\n",
    "    max_depth=best_depth_xgb,\n",
    "    colsample_bytree=best_col_xgb,\n",
    "    random_state=0, n_jobs=-1\n",
    ")\n",
    "final.fit(x_train_final[winning_subset], y_train)\n",
    "\n",
    "y_test_pred = (final.predict_proba(x_test_final[winning_subset])[:,1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"Final XGBoost model trained with features {winning_subset}; test predictions ready.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLfqH5Q69i9y",
    "outputId": "021c3b0d-e0d5-42bc-e328-7512a8b4b1fd",
    "ExecuteTime": {
     "end_time": "2025-06-02T09:08:52.809409Z",
     "start_time": "2025-06-02T09:08:52.677289Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_estimators_xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 45\u001B[39m\n\u001B[32m     42\u001B[39m winning_thr = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m subset \u001B[38;5;129;01min\u001B[39;00m candidates:\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m     net, thr = \u001B[43mevaluate_subset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSubset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubset\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: CV net-score = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnet\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m and threshold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mthr\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     47\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m net > best_net:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mevaluate_subset\u001B[39m\u001B[34m(feats)\u001B[39m\n\u001B[32m     12\u001B[39m X_val = x_train_final.iloc[val_idx][feats]\n\u001B[32m     13\u001B[39m y_val = y_train.iloc[val_idx]\n\u001B[32m     15\u001B[39m model = XGBClassifier(\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     n_estimators=\u001B[43mbest_estimators_xgb\u001B[49m,\n\u001B[32m     17\u001B[39m     eval_metric=\u001B[33m'\u001B[39m\u001B[33mlogloss\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     18\u001B[39m     reg_lambda=best_l1_xgb,\n\u001B[32m     19\u001B[39m     max_depth=best_depth_xgb,\n\u001B[32m     20\u001B[39m     colsample_bytree=best_col_xgb,\n\u001B[32m     21\u001B[39m     random_state=\u001B[32m0\u001B[39m, n_jobs=-\u001B[32m1\u001B[39m\n\u001B[32m     22\u001B[39m )\n\u001B[32m     23\u001B[39m model.fit(X_tr, y_tr)\n\u001B[32m     24\u001B[39m all_true.append(y_val.values)\n",
      "\u001B[31mNameError\u001B[39m: name 'best_estimators_xgb' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_subset(feats):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr = x_train_final.iloc[tr_idx][feats]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        X_val = x_train_final.iloc[val_idx][feats]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=best_estimators_xgb,\n",
    "            eval_metric='logloss',\n",
    "            reg_lambda=best_l1_xgb,\n",
    "            max_depth=best_depth_xgb,\n",
    "            colsample_bytree=best_col_xgb,\n",
    "            random_state=0, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:,1])\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "\n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "candidates = [[2], [2, 462]]\n",
    "\n",
    "best_net = -np.inf\n",
    "winning_subset = None\n",
    "winning_thr = None\n",
    "\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset(subset)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f} and threshold {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\nWinning subset: {winning_subset} with net-score {best_net:.1f} and threshold {winning_thr:.2f}\")\n",
    "\n",
    "final = XGBClassifier(\n",
    "    n_estimators=best_estimators_xgb,\n",
    "    eval_metric='logloss',\n",
    "    reg_lambda=best_l1_xgb,\n",
    "    max_depth=best_depth_xgb,\n",
    "    colsample_bytree=best_col_xgb,\n",
    "    random_state=0, n_jobs=-1\n",
    ")\n",
    "final.fit(x_train_final[winning_subset], y_train)\n",
    "\n",
    "y_test_pred = (final.predict_proba(x_test_final[winning_subset])[:,1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"Final XGBoost model trained with features {winning_subset}; test predictions ready.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXkJD1vLAqKc",
    "outputId": "d4db9154-f4eb-47ae-fc88-3fed99035e92"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset [2]: CV net-score = 35130.0 and threshold 0.55\n",
      "Subset [2, 462]: CV net-score = 35020.0 and threshold 0.52\n",
      "\n",
      "Winning subset: [2] with net-score 35130.0 and threshold 0.55\n",
      "Final XGBoost model trained with features [2]; test predictions ready.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:27:59.072282Z",
     "start_time": "2025-06-02T10:22:30.596021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import shap\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def extract_phi(shap_out, n_features, target_class=1):\n",
    "    if isinstance(shap_out, list):\n",
    "        phi = shap_out[target_class]\n",
    "        return phi[:, :n_features] if phi.shape[1] == n_features + 1 else phi\n",
    "    if hasattr(shap_out, \"values\"):\n",
    "        arr = shap_out.values\n",
    "    else:\n",
    "        arr = np.asarray(shap_out)\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Unexpected SHAP shape: {arr.shape}\")\n",
    "    N, A, B = arr.shape\n",
    "    if A in {n_features, n_features + 1}:\n",
    "        phi = arr[:, :n_features, target_class]\n",
    "        return phi\n",
    "    if B in {n_features, n_features + 1}:\n",
    "        phi = arr[:, target_class, :n_features]\n",
    "        return phi\n",
    "    raise ValueError(f\"Unknown axis configuration: {arr.shape}\")\n",
    "\n",
    "def net_score(y_true, y_pred, n_feats):\n",
    "    return 10 * accuracy_score(y_true, y_pred) * len(y_true) - 200 * n_feats\n",
    "\n",
    "def count_nb_features(model):\n",
    "    return model.n_features_in_\n",
    "\n",
    "def objective_nb(trial):\n",
    "    vs  = trial.suggest_float(\"var_smoothing\", 1e-12, 1e-6, log=True)\n",
    "    thr = trial.suggest_float(\"threshold\", 0.3, 0.7)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        m = GaussianNB(var_smoothing=vs).fit(X_tr, y_tr)\n",
    "        preds = (m.predict_proba(X_val)[:, 1] >= thr).astype(int)\n",
    "        scores.append(net_score(y_val, preds, count_nb_features(m)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_nb, n_trials=25, show_progress_bar=True)\n",
    "\n",
    "best_vs, best_thr = study.best_params[\"var_smoothing\"], study.best_params[\"threshold\"]\n",
    "print(f\"\\nBest parameters → var_smoothing={best_vs:.2e}, threshold={best_thr:.2f}\")\n",
    "\n",
    "model_nb = GaussianNB(var_smoothing=best_vs).fit(x_train_final, y_train)\n",
    "print(f\"\\nFinal model: #features = {count_nb_features(model_nb)}\")\n",
    "\n",
    "print(\"\\nNaive Bayes SHAP analysis\")\n",
    "background = shap.sample(x_train_final, 100, random_state=0)\n",
    "explainer  = shap.KernelExplainer(model_nb.predict_proba, background)\n",
    "\n",
    "sample = shap.sample(x_train_final, 300, random_state=1)\n",
    "shap_out = explainer.shap_values(sample, nsamples=100)\n",
    "phi      = extract_phi(shap_out, n_features=x_train_final.shape[1])\n",
    "mean_abs = np.abs(phi).mean(axis=0)\n",
    "\n",
    "shap_imp = pd.Series(mean_abs, index=x_train_final.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by mean |SHAP|:\")\n",
    "print(shap_imp.head(10))\n",
    "\n",
    "B, rng = 20, np.random.RandomState(0)\n",
    "feat_count = pd.Series(0, index=x_train_final.columns)\n",
    "\n",
    "for b in range(B):\n",
    "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
    "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
    "    m = GaussianNB(var_smoothing=best_vs).fit(Xb, yb)\n",
    "    expl = shap.KernelExplainer(m.predict_proba, shap.sample(Xb, 100, random_state=b))\n",
    "    shap_b = expl.shap_values(Xb.iloc[:200], nsamples=100)\n",
    "    phi_b  = extract_phi(shap_b, n_features=Xb.shape[1])\n",
    "    mean_abs_b = np.abs(phi_b).mean(axis=0)\n",
    "    sel = Xb.columns[mean_abs_b > np.median(mean_abs_b)]\n",
    "    feat_count[sel] += 1\n",
    "\n",
    "stability = (feat_count / B).sort_values(ascending=False)\n",
    "print(\"\\nBootstrap selection frequency – Top 10:\")\n",
    "print(stability.head(10))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:30,603] A new study created in memory with name: no-name-04ec09a1-44ee-4bf4-ae76-6bf722c0b810\n",
      "Best trial: 0. Best value: -92324:   8%|▊         | 2/25 [00:00<00:02, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:30,696] Trial 0 finished with value: -92324.0 and parameters: {'var_smoothing': 7.806640133564179e-08, 'threshold': 0.5670128482223724}. Best is trial 0 with value: -92324.0.\n",
      "[I 2025-06-02 12:22:30,779] Trial 1 finished with value: -92350.0 and parameters: {'var_smoothing': 3.6946144305347125e-10, 'threshold': 0.6777501925100201}. Best is trial 0 with value: -92324.0.\n",
      "[I 2025-06-02 12:22:30,861] Trial 2 finished with value: -92346.0 and parameters: {'var_smoothing': 4.4280850319495966e-10, 'threshold': 0.6812700262924618}. Best is trial 0 with value: -92324.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -92312:  24%|██▍       | 6/25 [00:00<00:01, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:30,951] Trial 3 finished with value: -92324.0 and parameters: {'var_smoothing': 1.1443956295431147e-10, 'threshold': 0.5464489499733285}. Best is trial 0 with value: -92324.0.\n",
      "[I 2025-06-02 12:22:31,040] Trial 4 finished with value: -92312.0 and parameters: {'var_smoothing': 1.5615004361575018e-12, 'threshold': 0.5233114758446538}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,125] Trial 5 finished with value: -92344.0 and parameters: {'var_smoothing': 2.531897285295792e-07, 'threshold': 0.6527861950596133}. Best is trial 4 with value: -92312.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -92312:  32%|███▏      | 8/25 [00:00<00:01, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:31,208] Trial 6 finished with value: -92324.0 and parameters: {'var_smoothing': 5.612068124478917e-07, 'threshold': 0.3013164320661367}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,292] Trial 7 finished with value: -92324.0 and parameters: {'var_smoothing': 8.421392174639971e-07, 'threshold': 0.5590293399001562}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,375] Trial 8 finished with value: -92336.0 and parameters: {'var_smoothing': 3.5016820775183155e-07, 'threshold': 0.5954485701083021}. Best is trial 4 with value: -92312.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -92312:  48%|████▊     | 12/25 [00:01<00:01, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:31,458] Trial 9 finished with value: -92350.0 and parameters: {'var_smoothing': 6.920563501356304e-10, 'threshold': 0.6261521830697945}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,550] Trial 10 finished with value: -92336.0 and parameters: {'var_smoothing': 1.0476661175980006e-12, 'threshold': 0.43008344723578934}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,637] Trial 11 finished with value: -92314.0 and parameters: {'var_smoothing': 1.5626611176841904e-08, 'threshold': 0.48093766862532544}. Best is trial 4 with value: -92312.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -92312:  56%|█████▌    | 14/25 [00:01<00:00, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:31,727] Trial 12 finished with value: -92334.0 and parameters: {'var_smoothing': 8.015580824366263e-09, 'threshold': 0.45213367279501626}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,815] Trial 13 finished with value: -92316.0 and parameters: {'var_smoothing': 8.455925772623108e-12, 'threshold': 0.48053210349413944}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:31,906] Trial 14 finished with value: -92318.0 and parameters: {'var_smoothing': 1.4185352496255862e-08, 'threshold': 0.37243471921676996}. Best is trial 4 with value: -92312.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: -92304:  72%|███████▏  | 18/25 [00:01<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:31,998] Trial 15 finished with value: -92320.0 and parameters: {'var_smoothing': 6.619924525010713e-09, 'threshold': 0.5268003873675288}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:32,089] Trial 16 finished with value: -92332.0 and parameters: {'var_smoothing': 5.3649093003823575e-11, 'threshold': 0.4001667815640613}. Best is trial 4 with value: -92312.0.\n",
      "[I 2025-06-02 12:22:32,173] Trial 17 finished with value: -92304.0 and parameters: {'var_smoothing': 2.0677188559998505e-12, 'threshold': 0.4989169883295357}. Best is trial 17 with value: -92304.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: -92304:  80%|████████  | 20/25 [00:01<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:32,261] Trial 18 finished with value: -92310.0 and parameters: {'var_smoothing': 1.2452497875444768e-12, 'threshold': 0.5070617695926103}. Best is trial 17 with value: -92304.0.\n",
      "[I 2025-06-02 12:22:32,349] Trial 19 finished with value: -92322.0 and parameters: {'var_smoothing': 8.75395343094027e-12, 'threshold': 0.3587311442487369}. Best is trial 17 with value: -92304.0.\n",
      "[I 2025-06-02 12:22:32,436] Trial 20 finished with value: -92334.0 and parameters: {'var_smoothing': 5.891522297720488e-12, 'threshold': 0.44319204628776143}. Best is trial 17 with value: -92304.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: -92304:  96%|█████████▌| 24/25 [00:02<00:00, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:32,524] Trial 21 finished with value: -92308.0 and parameters: {'var_smoothing': 1.079936788179908e-12, 'threshold': 0.5100691654390016}. Best is trial 17 with value: -92304.0.\n",
      "[I 2025-06-02 12:22:32,612] Trial 22 finished with value: -92318.0 and parameters: {'var_smoothing': 3.055820240868782e-11, 'threshold': 0.4765185459414491}. Best is trial 17 with value: -92304.0.\n",
      "[I 2025-06-02 12:22:32,705] Trial 23 finished with value: -92308.0 and parameters: {'var_smoothing': 3.3873676775185963e-12, 'threshold': 0.5118132539980456}. Best is trial 17 with value: -92304.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 17. Best value: -92304: 100%|██████████| 25/25 [00:02<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:22:32,792] Trial 24 finished with value: -92334.0 and parameters: {'var_smoothing': 4.414652830651082e-12, 'threshold': 0.5890243410136639}. Best is trial 17 with value: -92304.0.\n",
      "\n",
      "Best parameters → var_smoothing=2.07e-12, threshold=0.50\n",
      "\n",
      "Final model: #features = 495\n",
      "\n",
      "Naive Bayes SHAP analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 19/300 [00:01<00:20, 14.04it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 23/300 [00:01<00:19, 13.93it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.180e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 67/300 [00:04<00:16, 13.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.603e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.060e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 75/300 [00:05<00:17, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.397e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 81/300 [00:05<00:17, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.840e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 83/300 [00:06<00:17, 12.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 40%|███▉      | 119/300 [00:08<00:13, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.111e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 41%|████      | 123/300 [00:09<00:13, 13.59it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.953e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.761e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 43%|████▎     | 129/300 [00:09<00:12, 13.61it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.926e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 141/300 [00:10<00:11, 13.71it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.262e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 50%|████▉     | 149/300 [00:11<00:10, 13.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 163/300 [00:12<00:09, 13.86it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 195/300 [00:14<00:08, 12.35it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.696e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 201/300 [00:15<00:07, 13.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 207/300 [00:15<00:07, 12.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.450e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 211/300 [00:15<00:07, 11.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 215/300 [00:16<00:07, 11.93it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.544e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 219/300 [00:16<00:07, 11.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 74%|███████▎  | 221/300 [00:16<00:06, 11.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 225/300 [00:17<00:06, 12.00it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.644e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.568e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 80%|███████▉  | 239/300 [00:18<00:04, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.066e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 241/300 [00:18<00:04, 12.96it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 86%|████████▌ | 257/300 [00:19<00:03, 13.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.902e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 261/300 [00:19<00:02, 13.60it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 265/300 [00:20<00:02, 13.46it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.525e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 92%|█████████▏| 277/300 [00:20<00:01, 13.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▎| 281/300 [00:21<00:01, 13.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 283/300 [00:21<00:01, 13.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.856e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 293/300 [00:22<00:00, 13.45it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|█████████▉| 299/300 [00:22<00:00, 12.16it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.764e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 300/300 [00:22<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by mean |SHAP|:\n",
      "274    1.635562e+13\n",
      "214    1.635562e+13\n",
      "3      1.457642e-02\n",
      "2      1.361911e-02\n",
      "6      1.255340e-02\n",
      "5      1.220861e-02\n",
      "414    1.026418e-02\n",
      "4      1.019199e-02\n",
      "9      9.913078e-03\n",
      "462    9.320831e-03\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:00<00:14, 13.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:13, 13.87it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:01<00:13, 13.18it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.145e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.240e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 21%|██        | 42/200 [00:02<00:03, 40.68it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 57/200 [00:03<00:07, 18.05it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.109e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 64/200 [00:03<00:08, 15.45it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.128e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 35%|███▌      | 70/200 [00:04<00:09, 13.62it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 76/200 [00:04<00:09, 13.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.609e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 78/200 [00:04<00:08, 13.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.664e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:05<00:08, 13.57it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.769e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 106/200 [00:06<00:06, 14.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.771e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 112/200 [00:07<00:06, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.153e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 114/200 [00:07<00:06, 12.94it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.808e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 116/200 [00:07<00:06, 12.96it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:08<00:04, 13.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.039e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [00:09<00:04, 13.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.229e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:10<00:03, 13.18it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 152/200 [00:10<00:03, 12.81it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.569e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.497e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 164/200 [00:11<00:02, 12.30it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 176/200 [00:12<00:01, 12.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.424e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:12<00:01, 13.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.698e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.657e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 14.28it/s]\n",
      "  8%|▊         | 16/200 [00:01<00:16, 11.30it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.468e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:15, 11.65it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.378e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 10%|█         | 20/200 [00:01<00:14, 12.18it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 34/200 [00:02<00:12, 13.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 38/200 [00:02<00:12, 13.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.011e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.588e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:11, 14.00it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 44/200 [00:03<00:13, 11.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.001e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [00:03<00:12, 12.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.174e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 50/200 [00:03<00:12, 12.38it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.473e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 58/200 [00:04<00:11, 12.71it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.096e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 64/200 [00:05<00:10, 12.96it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.243e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:09, 13.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.866e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 78/200 [00:06<00:09, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 41%|████      | 82/200 [00:06<00:09, 12.73it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.322e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.601e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 90/200 [00:07<00:08, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.622e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 100/200 [00:07<00:07, 13.06it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 51%|█████     | 102/200 [00:08<00:07, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.436e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 112/200 [00:08<00:06, 13.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.216e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 116/200 [00:09<00:06, 13.25it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.621e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 59%|█████▉    | 118/200 [00:09<00:06, 13.37it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:09<00:05, 13.44it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.524e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 126/200 [00:09<00:05, 13.10it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 66%|██████▌   | 132/200 [00:10<00:05, 12.60it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.068e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:10<00:05, 12.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 71%|███████   | 142/200 [00:11<00:04, 13.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.789e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.906e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 152/200 [00:11<00:03, 13.24it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.696e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 162/200 [00:12<00:02, 13.23it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [00:13<00:02, 12.79it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:13<00:02, 11.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.004e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 89%|████████▉ | 178/200 [00:13<00:01, 11.87it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:14<00:01, 13.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.474e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 190/200 [00:14<00:00, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.483e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.94it/s]\n",
      "  2%|▏         | 3/200 [00:00<00:17, 11.43it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  6%|▌         | 11/200 [00:00<00:15, 12.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.064e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  6%|▋         | 13/200 [00:01<00:15, 12.08it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.994e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 15/200 [00:01<00:14, 12.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.634e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 10%|█         | 21/200 [00:01<00:14, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.637e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 37/200 [00:03<00:13, 11.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.864e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 24%|██▎       | 47/200 [00:03<00:12, 12.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.971e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.092e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 30%|███       | 60/200 [00:03<00:04, 33.80it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.738e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 103/200 [00:07<00:07, 12.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 60%|█████▉    | 119/200 [00:08<00:06, 12.80it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 62%|██████▎   | 125/200 [00:09<00:05, 12.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.021e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 129/200 [00:09<00:05, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.165e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.474e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.908e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.885e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 66%|██████▋   | 133/200 [00:09<00:05, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.175e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 72%|███████▎  | 145/200 [00:10<00:04, 12.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.343e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 76%|███████▋  | 153/200 [00:11<00:03, 12.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 80%|███████▉  | 159/200 [00:11<00:03, 12.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.933e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 163/200 [00:12<00:02, 12.81it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 92%|█████████▎| 185/200 [00:13<00:01, 13.14it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.175e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 96%|█████████▋| 193/200 [00:14<00:00, 12.73it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.32it/s]\n",
      "  1%|          | 2/200 [00:00<00:16, 12.25it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 6/200 [00:00<00:15, 12.15it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  6%|▌         | 12/200 [00:00<00:15, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:02<00:14, 11.32it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.280e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:13, 11.92it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.545e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [00:03<00:12, 12.34it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:12, 12.64it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 26%|██▌       | 52/200 [00:04<00:11, 13.21it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.135e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 64/200 [00:05<00:10, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [00:05<00:10, 13.08it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.294e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 68/200 [00:05<00:09, 13.24it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 43%|████▎     | 86/200 [00:06<00:09, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:06<00:08, 12.64it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.606e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 96/200 [00:07<00:08, 12.87it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.382e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 51%|█████     | 102/200 [00:07<00:07, 13.40it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.768e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.790e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 108/200 [00:08<00:07, 12.41it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.197e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [00:08<00:06, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.869e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 120/200 [00:09<00:05, 13.43it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.280e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:09<00:05, 13.66it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.737e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 126/200 [00:09<00:05, 13.14it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 128/200 [00:09<00:05, 13.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.474e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 138/200 [00:10<00:05, 12.01it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 144/200 [00:11<00:04, 12.94it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.361e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.361e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 160/200 [00:12<00:03, 12.59it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.911e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 162/200 [00:12<00:03, 11.85it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.112e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 164/200 [00:12<00:02, 12.31it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 176/200 [00:13<00:02, 11.87it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 188/200 [00:14<00:01, 11.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.059e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 194/200 [00:15<00:00, 12.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.825e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.816e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.903e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 196/200 [00:15<00:00, 12.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.278e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.62it/s]\n",
      "  4%|▍         | 8/200 [00:00<00:15, 12.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.458e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:14, 12.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.896e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 15%|█▌        | 30/200 [00:02<00:12, 13.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.037e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 32/200 [00:02<00:12, 13.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.797e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.825e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 34/200 [00:02<00:13, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.412e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:11, 13.41it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.307e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 94/200 [00:06<00:07, 13.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 96/200 [00:06<00:07, 13.04it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.912e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 59%|█████▉    | 118/200 [00:08<00:06, 12.24it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 130/200 [00:09<00:05, 11.70it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 136/200 [00:09<00:05, 12.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [00:09<00:04, 12.62it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.420e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:10<00:03, 12.68it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 164/200 [00:11<00:02, 13.64it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:12<00:02, 12.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.707e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 182/200 [00:13<00:01, 13.10it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 188/200 [00:13<00:00, 12.70it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.009e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.68it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.085e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 6/200 [00:00<00:15, 12.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.096e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.611e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 8/200 [00:00<00:15, 12.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.297e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 10/200 [00:00<00:15, 12.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 24/200 [00:02<00:15, 11.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 34/200 [00:02<00:13, 12.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.395e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:03<00:13, 12.15it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:12, 12.38it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:12, 12.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 54/200 [00:04<00:11, 12.84it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 30%|███       | 60/200 [00:04<00:10, 13.09it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [00:05<00:10, 13.28it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.080e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:09, 13.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.904e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.900e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 84/200 [00:06<00:08, 13.40it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.308e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 43%|████▎     | 86/200 [00:06<00:08, 13.85it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.541e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:06<00:08, 13.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.395e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.420e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.420e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 92/200 [00:07<00:07, 13.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.123e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 104/200 [00:08<00:07, 12.85it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 108/200 [00:08<00:07, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 114/200 [00:08<00:06, 13.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 116/200 [00:09<00:06, 13.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.103e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 124/200 [00:09<00:05, 13.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.344e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.468e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.468e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:10<00:05, 12.25it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 138/200 [00:10<00:05, 11.85it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 156/200 [00:12<00:03, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 79%|███████▉  | 158/200 [00:12<00:03, 12.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 160/200 [00:12<00:03, 12.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.898e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.983e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 162/200 [00:12<00:03, 12.42it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 168/200 [00:13<00:02, 12.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [00:13<00:02, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.933e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:13<00:01, 13.08it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:14<00:01, 12.69it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 188/200 [00:14<00:00, 12.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.598e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [00:15<00:00, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.492e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.72it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  1%|          | 2/200 [00:00<00:15, 12.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.923e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 10/200 [00:00<00:14, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.342e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:01<00:13, 13.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.388e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 32/200 [00:02<00:13, 12.69it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:02<00:13, 12.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.519e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:12, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.069e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 58/200 [00:04<00:10, 13.73it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.470e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 31%|███       | 62/200 [00:04<00:10, 12.65it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [00:05<00:10, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.782e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 42%|████▎     | 85/200 [00:05<00:04, 26.65it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.480e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:05<00:05, 21.32it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 91/200 [00:06<00:05, 18.93it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.107e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 98/200 [00:06<00:06, 15.18it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 66%|██████▌   | 132/200 [00:09<00:04, 13.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 138/200 [00:09<00:04, 13.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.998e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [00:09<00:04, 13.44it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.830e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 71%|███████   | 142/200 [00:09<00:04, 13.91it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 148/200 [00:10<00:03, 13.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:10<00:03, 13.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 152/200 [00:10<00:03, 13.04it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.170e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [00:12<00:02, 11.71it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.388e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:12<00:02, 11.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 98%|█████████▊| 196/200 [00:14<00:00, 12.63it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.80it/s]\n",
      "  8%|▊         | 16/200 [00:01<00:14, 12.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.333e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 14%|█▍        | 28/200 [00:02<00:12, 13.30it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.794e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 15%|█▌        | 30/200 [00:02<00:13, 13.06it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.177e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 32/200 [00:02<00:12, 13.00it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.628e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.381e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 34/200 [00:02<00:12, 13.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:02<00:12, 13.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.522e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.179e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:11, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.501e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.199e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 58/200 [00:04<00:11, 12.28it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 30%|███       | 60/200 [00:04<00:11, 12.37it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [00:05<00:11, 12.16it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.032e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 35%|███▌      | 70/200 [00:05<00:10, 12.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:11, 11.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.175e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.720e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.076e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.076e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.799e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.420e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 78/200 [00:06<00:10, 11.74it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 84/200 [00:06<00:09, 12.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.207e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 90/200 [00:07<00:09, 12.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 104/200 [00:08<00:07, 12.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 106/200 [00:08<00:07, 13.05it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.203e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 59%|█████▉    | 118/200 [00:09<00:06, 13.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.531e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 130/200 [00:10<00:05, 12.42it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=8.149e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:10<00:05, 12.40it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 136/200 [00:10<00:05, 12.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.345e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 138/200 [00:10<00:04, 13.16it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 144/200 [00:11<00:04, 12.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.551e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:11<00:04, 12.34it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.018e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.302e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 156/200 [00:12<00:03, 12.94it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 79%|███████▉  | 158/200 [00:12<00:03, 12.99it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [00:13<00:02, 13.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:13<00:01, 13.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.787e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 176/200 [00:13<00:01, 12.16it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 182/200 [00:14<00:01, 11.85it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.127e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 92%|█████████▏| 184/200 [00:14<00:01, 11.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:14<00:01, 12.05it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 188/200 [00:14<00:00, 12.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.740e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [00:15<00:00, 11.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.312e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 194/200 [00:15<00:00, 12.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.53it/s]\n",
      "  2%|▏         | 4/200 [00:00<00:15, 12.43it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 8/200 [00:00<00:15, 12.46it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.000e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.852e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:14, 13.04it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:13, 13.37it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:14, 12.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 24/200 [00:01<00:14, 12.52it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:12, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [00:03<00:12, 12.01it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.682e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.733e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:12, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.723e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 30%|███       | 60/200 [00:04<00:10, 13.02it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [00:05<00:10, 12.28it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [00:05<00:10, 12.45it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.777e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 103/200 [00:07<00:06, 14.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.985e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 52%|█████▎    | 105/200 [00:07<00:06, 14.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.640e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.979e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 57%|█████▊    | 115/200 [00:08<00:06, 13.62it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 117/200 [00:08<00:06, 13.44it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.843e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.484e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 60%|█████▉    | 119/200 [00:08<00:06, 13.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.464e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 62%|██████▎   | 125/200 [00:08<00:05, 13.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.717e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 135/200 [00:09<00:05, 12.92it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 137/200 [00:09<00:04, 12.62it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 74%|███████▎  | 147/200 [00:10<00:04, 12.37it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 82%|████████▏ | 163/200 [00:12<00:02, 12.43it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.212e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.216e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 82%|████████▎ | 165/200 [00:12<00:02, 12.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.148e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.700e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 175/200 [00:12<00:01, 13.02it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 181/200 [00:13<00:01, 12.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 191/200 [00:14<00:00, 12.38it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.157e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|█████████▉| 199/200 [00:14<00:00, 12.65it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.37it/s]\n",
      "  5%|▌         | 10/200 [00:00<00:15, 12.01it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.552e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 16/200 [00:01<00:15, 11.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 12%|█▏        | 24/200 [00:01<00:13, 12.73it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.131e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:02<00:13, 12.91it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.010e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.320e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 38/200 [00:03<00:12, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:12, 12.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 50/200 [00:04<00:11, 12.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.539e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 31%|███       | 62/200 [00:04<00:11, 12.25it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [00:05<00:09, 12.71it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:07<00:08, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 94/200 [00:07<00:08, 12.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 96/200 [00:07<00:08, 12.38it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.983e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 100/200 [00:07<00:07, 12.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 106/200 [00:08<00:07, 12.39it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 108/200 [00:08<00:07, 12.84it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.631e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [00:08<00:07, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.266e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 112/200 [00:08<00:06, 12.60it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 130/200 [00:10<00:05, 12.62it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 168/200 [00:13<00:02, 13.01it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.299e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 180/200 [00:14<00:01, 12.36it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 188/200 [00:14<00:00, 12.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.549e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 190/200 [00:15<00:00, 12.89it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.513e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [00:15<00:00, 12.98it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 198/200 [00:15<00:00, 13.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.119e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.119e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.57it/s]\n",
      " 10%|█         | 20/200 [00:01<00:14, 12.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:02<00:13, 11.79it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:12, 12.35it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.009e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 68/200 [00:05<00:10, 12.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [00:05<00:09, 12.98it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 78/200 [00:06<00:09, 13.01it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.992e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 95/200 [00:06<00:04, 25.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.861e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 98/200 [00:06<00:04, 21.23it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.884e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 108/200 [00:07<00:06, 15.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 112/200 [00:08<00:06, 13.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.258e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:08<00:06, 12.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.045e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 146/200 [00:10<00:04, 12.46it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.747e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.241e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 180/200 [00:13<00:01, 13.36it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.134e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 194/200 [00:14<00:00, 12.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.193e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.36it/s]\n",
      "  1%|          | 2/200 [00:00<00:15, 13.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.992e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 4/200 [00:00<00:14, 13.11it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.115e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:13, 14.10it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 16/200 [00:01<00:13, 13.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.625e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.722e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.312e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.312e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:13, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.386e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.677e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:13, 13.05it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:01<00:12, 13.39it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 26%|██▌       | 52/200 [00:03<00:11, 13.21it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.749e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 56/200 [00:04<00:10, 13.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.204e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [00:05<00:10, 12.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 78/200 [00:06<00:09, 12.28it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 40%|████      | 80/200 [00:06<00:09, 12.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 90/200 [00:06<00:08, 12.68it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.570e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [00:08<00:06, 13.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 116/200 [00:09<00:06, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.227e-01, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 120/200 [00:09<00:06, 13.06it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 128/200 [00:09<00:05, 12.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [00:10<00:04, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.935e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.299e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.445e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 71%|███████   | 142/200 [00:11<00:04, 12.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 144/200 [00:11<00:04, 13.44it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:11<00:03, 12.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 154/200 [00:11<00:03, 12.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 83%|████████▎ | 166/200 [00:12<00:02, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.373e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.325e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 168/200 [00:13<00:02, 12.93it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.636e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 176/200 [00:13<00:01, 12.60it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 89%|████████▉ | 178/200 [00:13<00:01, 12.74it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.077e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:14<00:01, 12.79it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [00:14<00:00, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.84it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 4/200 [00:00<00:15, 12.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.401e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 10/200 [00:00<00:14, 12.92it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.638e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:13, 13.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 14%|█▍        | 28/200 [00:02<00:13, 12.74it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.407e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 38/200 [00:02<00:12, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.767e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:11, 13.63it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.466e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 31%|███       | 62/200 [00:04<00:10, 13.22it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [00:05<00:09, 13.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.282e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.275e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 41%|████      | 82/200 [00:06<00:09, 12.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.881e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 92/200 [00:07<00:08, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.114e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 116/200 [00:07<00:04, 19.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.381e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 60%|█████▉    | 119/200 [00:08<00:04, 18.21it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 66%|██████▌   | 131/200 [00:09<00:04, 14.10it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 70%|██████▉   | 139/200 [00:09<00:04, 13.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 72%|███████▎  | 145/200 [00:10<00:04, 13.30it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 149/200 [00:10<00:03, 13.09it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 76%|███████▋  | 153/200 [00:10<00:03, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 80%|███████▉  | 159/200 [00:11<00:03, 13.23it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 84%|████████▎ | 167/200 [00:11<00:02, 12.92it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.382e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.072e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 90%|████████▉ | 179/200 [00:12<00:01, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.572e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.86it/s]\n",
      "  4%|▍         | 8/200 [00:00<00:13, 13.84it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.083e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.205e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:14, 12.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:13, 13.08it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:01<00:13, 12.74it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 15%|█▌        | 30/200 [00:02<00:13, 12.57it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.596e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 21%|██        | 42/200 [00:03<00:12, 12.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 50/200 [00:03<00:11, 13.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.983e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 56/200 [00:04<00:10, 13.42it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 31%|███       | 62/200 [00:04<00:10, 13.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.508e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:09, 13.24it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.131e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 76/200 [00:05<00:09, 12.94it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 40%|████      | 80/200 [00:06<00:09, 12.86it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.165e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 94/200 [00:07<00:08, 12.43it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 98/200 [00:07<00:08, 12.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 100/200 [00:07<00:07, 12.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.539e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [00:08<00:07, 12.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:09<00:06, 12.15it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.063e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 128/200 [00:09<00:05, 12.39it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:10<00:05, 12.56it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:11<00:03, 12.73it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 79%|███████▉  | 158/200 [00:12<00:03, 13.32it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 83%|████████▎ | 166/200 [00:12<00:02, 13.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 180/200 [00:13<00:01, 12.43it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [00:14<00:00, 13.14it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 194/200 [00:15<00:00, 13.07it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 198/200 [00:15<00:00, 13.28it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.90it/s]\n",
      "  1%|          | 2/200 [00:00<00:14, 13.91it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 8/200 [00:00<00:13, 14.06it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.184e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:14, 12.99it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 10%|█         | 20/200 [00:01<00:13, 12.98it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.700e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 24%|██▍       | 48/200 [00:03<00:11, 12.81it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 50/200 [00:03<00:11, 12.84it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.274e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 54/200 [00:04<00:11, 12.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.089e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.847e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 56/200 [00:04<00:12, 11.99it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.490e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.003e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.307e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 31%|███       | 62/200 [00:04<00:10, 12.69it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:09, 12.99it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 84/200 [00:06<00:08, 13.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.097e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 43%|████▎     | 86/200 [00:06<00:08, 13.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:06<00:08, 13.22it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 90/200 [00:06<00:08, 13.04it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.196e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 94/200 [00:07<00:08, 12.98it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 96/200 [00:07<00:08, 12.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 104/200 [00:08<00:07, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 108/200 [00:08<00:06, 13.25it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.966e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.477e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.049e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:08<00:02, 34.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 129/200 [00:09<00:03, 19.60it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.404e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 148/200 [00:10<00:03, 13.59it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.595e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 152/200 [00:10<00:03, 13.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 79%|███████▉  | 158/200 [00:11<00:03, 12.68it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.696e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:12<00:02, 12.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.477e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:13<00:01, 13.23it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.642e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 99%|█████████▉| 198/200 [00:14<00:00, 13.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.052e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.79it/s]\n",
      "  2%|▏         | 4/200 [00:00<00:13, 14.09it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.478e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 6/200 [00:00<00:14, 13.31it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 8/200 [00:00<00:14, 13.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:13, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.109e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.976e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.009e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:02<00:13, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.789e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 21%|██        | 42/200 [00:03<00:12, 12.74it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.036e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 54/200 [00:04<00:11, 13.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 33%|███▎      | 66/200 [00:05<00:10, 12.98it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.648e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.648e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:09, 13.23it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.505e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 74/200 [00:05<00:09, 13.60it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.990e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 84/200 [00:06<00:08, 13.34it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 94/200 [00:07<00:08, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.599e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.436e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 100/200 [00:07<00:07, 12.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.702e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [00:08<00:07, 12.74it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.923e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 124/200 [00:09<00:06, 12.48it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 126/200 [00:09<00:05, 12.52it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.359e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 130/200 [00:10<00:05, 12.61it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.313e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 66%|██████▌   | 132/200 [00:10<00:05, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.006e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 68%|██████▊   | 136/200 [00:10<00:05, 12.59it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.271e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.592e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.079e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 75%|███████▌  | 150/200 [00:11<00:03, 12.78it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 156/200 [00:12<00:03, 13.16it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.324e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.708e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 162/200 [00:12<00:02, 12.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.177e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:13<00:02, 12.52it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.374e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:14<00:01, 11.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [00:15<00:00, 13.15it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.225e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.74it/s]\n",
      "  3%|▎         | 6/200 [00:00<00:14, 13.00it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:14, 13.20it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.505e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:13, 12.86it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 20%|██        | 40/200 [00:03<00:12, 12.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.580e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.672e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [00:03<00:12, 12.45it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:10, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 76/200 [00:05<00:09, 13.06it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.161e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 40%|████      | 80/200 [00:06<00:09, 13.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.119e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 84/200 [00:06<00:08, 13.01it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 104/200 [00:08<00:07, 12.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.972e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 106/200 [00:08<00:07, 12.35it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 54%|█████▍    | 108/200 [00:08<00:07, 13.02it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.234e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 114/200 [00:08<00:06, 12.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 59%|█████▉    | 118/200 [00:09<00:06, 12.92it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.570e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:09<00:01, 34.83it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 149/200 [00:10<00:02, 17.40it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.553e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 152/200 [00:10<00:03, 15.79it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 154/200 [00:10<00:02, 15.54it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.402e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 168/200 [00:12<00:02, 13.63it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 180/200 [00:12<00:01, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 91%|█████████ | 182/200 [00:13<00:01, 12.57it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.655e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 93%|█████████▎| 186/200 [00:13<00:01, 12.78it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 190/200 [00:13<00:00, 12.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.672e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.75it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.232e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  1%|          | 2/200 [00:00<00:15, 12.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 8/200 [00:00<00:15, 12.27it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 10/200 [00:00<00:14, 12.79it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.968e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:14, 12.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  8%|▊         | 16/200 [00:01<00:14, 12.77it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  9%|▉         | 18/200 [00:01<00:14, 12.75it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.572e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.732e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 10%|█         | 20/200 [00:01<00:14, 12.67it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.279e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:14, 12.64it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.232e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 16%|█▌        | 32/200 [00:02<00:13, 12.11it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.264e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.264e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.383e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 34/200 [00:02<00:13, 12.68it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.112e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:02<00:13, 12.61it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.238e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 44/200 [00:03<00:11, 13.64it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.491e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.947e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [00:03<00:11, 13.21it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 50/200 [00:03<00:10, 13.72it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.546e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 54/200 [00:04<00:10, 13.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.840e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 68/200 [00:05<00:10, 13.03it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 72/200 [00:05<00:10, 12.51it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 43%|████▎     | 86/200 [00:06<00:09, 12.57it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.563e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 49%|████▉     | 98/200 [00:07<00:07, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.620e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 114/200 [00:08<00:06, 12.87it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 120/200 [00:09<00:06, 12.94it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.093e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:09<00:06, 12.90it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 124/200 [00:09<00:05, 12.84it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 126/200 [00:09<00:05, 12.70it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.595e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 128/200 [00:09<00:05, 12.69it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [00:10<00:04, 13.19it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.386e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 146/200 [00:11<00:04, 13.40it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.702e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 74%|███████▍  | 148/200 [00:11<00:03, 13.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.392e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 160/200 [00:12<00:03, 12.89it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 81%|████████  | 162/200 [00:12<00:03, 11.84it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [00:13<00:02, 12.29it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 87%|████████▋ | 174/200 [00:13<00:02, 12.55it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.450e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.805e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 190/200 [00:14<00:00, 12.40it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 97%|█████████▋| 194/200 [00:15<00:00, 12.82it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.187e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.81it/s]\n",
      "  3%|▎         | 6/200 [00:00<00:14, 13.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  5%|▌         | 10/200 [00:00<00:14, 13.00it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:15, 11.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.774e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:13, 12.76it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.132e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:02<00:13, 12.81it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.375e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 18%|█▊        | 36/200 [00:02<00:12, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 38/200 [00:02<00:12, 12.97it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.239e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 54/200 [00:04<00:11, 12.95it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 30%|███       | 60/200 [00:04<00:10, 13.53it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 34%|███▍      | 68/200 [00:05<00:10, 12.34it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 78/200 [00:06<00:09, 13.22it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.073e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 90/200 [00:06<00:08, 13.30it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.186e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 46%|████▌     | 92/200 [00:07<00:08, 13.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.577e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 96/200 [00:07<00:08, 12.59it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=9.398e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 57%|█████▋    | 114/200 [00:08<00:06, 13.32it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.647e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 58%|█████▊    | 116/200 [00:08<00:06, 13.13it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.124e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 62%|██████▏   | 124/200 [00:09<00:05, 12.90it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.984e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.472e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 65%|██████▌   | 130/200 [00:09<00:05, 13.37it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.864e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.864e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 67%|██████▋   | 134/200 [00:10<00:05, 13.17it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.893e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 76%|███████▌  | 151/200 [00:10<00:01, 38.63it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.804e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 155/200 [00:10<00:01, 28.33it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 90%|████████▉ | 179/200 [00:12<00:01, 13.09it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.94it/s]\n",
      "  5%|▌         | 10/200 [00:00<00:16, 11.46it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.078e-01, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 14/200 [00:01<00:15, 12.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 10%|█         | 20/200 [00:01<00:14, 12.30it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 11%|█         | 22/200 [00:01<00:14, 12.47it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 26/200 [00:02<00:13, 12.58it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.880e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 38/200 [00:03<00:12, 12.89it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 44/200 [00:03<00:11, 13.35it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 23%|██▎       | 46/200 [00:03<00:11, 13.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.838e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 26%|██▌       | 52/200 [00:04<00:11, 13.08it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=8.054e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 54/200 [00:04<00:11, 12.49it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 31%|███       | 62/200 [00:04<00:10, 12.86it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 64/200 [00:05<00:10, 12.99it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 42%|████▏     | 84/200 [00:06<00:09, 12.12it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.556e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 44%|████▍     | 88/200 [00:06<00:08, 13.06it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.751e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 52%|█████▏    | 104/200 [00:08<00:07, 12.66it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.554e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 106/200 [00:08<00:08, 11.71it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.258e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 110/200 [00:08<00:07, 12.05it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.128e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 112/200 [00:08<00:07, 12.09it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.906e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 122/200 [00:09<00:05, 13.26it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.468e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 128/200 [00:10<00:05, 12.93it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.999e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 69%|██████▉   | 138/200 [00:10<00:04, 12.50it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 140/200 [00:11<00:04, 12.46it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 78%|███████▊  | 156/200 [00:12<00:03, 12.87it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.451e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      " 79%|███████▉  | 158/200 [00:12<00:03, 11.88it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.856e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/sklearn/linear_model/_least_angle.py:723: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.663e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn(\n",
      "/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 85%|████████▌ | 170/200 [00:13<00:02, 12.94it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      " 88%|████████▊ | 176/200 [00:13<00:01, 13.02it/s]/home/kubog/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_kernel.py:708: UserWarning: Linear regression equation is singular, a least squares solutions is used instead.\n",
      "To avoid this situation and get a regular matrix do one of the following:\n",
      "1) turn up the number of samples,\n",
      "2) turn up the L1 regularization with num_features(N) where N is less than the number of samples,\n",
      "3) group features together to reduce the number of inputs that need to be explained.\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bootstrap selection frequency – Top 10:\n",
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "8      1.0\n",
      "4      1.0\n",
      "5      1.0\n",
      "6      1.0\n",
      "9      1.0\n",
      "462    1.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:30:50.191779Z",
     "start_time": "2025-06-02T10:30:50.043981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_subset_nb(feats, var_smoothing):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, y_tr = x_train_final.iloc[tr_idx][feats], y_train.iloc[tr_idx]\n",
    "        X_val, y_val = x_train_final.iloc[val_idx][feats], y_train.iloc[val_idx]\n",
    "        model = GaussianNB(var_smoothing=var_smoothing).fit(X_tr, y_tr)\n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:, 1])\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "top_shap_features = [274, 214, 3, 2, 6, 5, 414, 4, 9, 462]\n",
    "candidates = [\n",
    "    [274],\n",
    "    [274, 214, 3],\n",
    "    [274, 214, 3, 2, 6]\n",
    "]\n",
    "\n",
    "best_net, winning_subset, winning_thr = -np.inf, None, None\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset_nb(subset, best_vs)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f} | thr = {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\n Winning subset: {winning_subset} | net-score = {best_net:.1f} | thr = {winning_thr:.2f}\")\n",
    "\n",
    "final_nb = GaussianNB(var_smoothing=best_vs).fit(x_train_final[winning_subset], y_train)\n",
    "y_test_pred = (final_nb.predict_proba(x_test_final[winning_subset])[:, 1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"Final GaussianNB model trained on features {winning_subset}; test predictions ready.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset [274]: CV net-score = 25580.0 | thr = 0.50\n",
      "Subset [274, 214, 3]: CV net-score = 32630.0 | thr = 0.46\n",
      "Subset [274, 214, 3, 2, 6]: CV net-score = 33640.0 | thr = 0.40\n",
      "\n",
      "🏆 Winning subset: [274, 214, 3, 2, 6] | net-score = 33640.0 | thr = 0.40\n",
      "Final GaussianNB model trained on features [274, 214, 3, 2, 6]; test predictions ready.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:33:08.810352Z",
     "start_time": "2025-06-02T10:33:08.575470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_subset_nb(feats, var_smoothing):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, y_tr = x_train_final.iloc[tr_idx][feats], y_train.iloc[tr_idx]\n",
    "        X_val, y_val = x_train_final.iloc[val_idx][feats], y_train.iloc[val_idx]\n",
    "        model = GaussianNB(var_smoothing=var_smoothing).fit(X_tr, y_tr)\n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:, 1])\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "candidates = [\n",
    "    [2],\n",
    "    [2, 3],\n",
    "    [2, 3, 6],\n",
    "    [2, 3, 6, 5],\n",
    "    [2, 3, 6, 5, 462]\n",
    "]\n",
    "\n",
    "best_net, winning_subset, winning_thr = -np.inf, None, None\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset_nb(subset, best_vs)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f} | thr = {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\nWinning subset: {winning_subset} | net-score = {best_net:.1f} | thr = {winning_thr:.2f}\")\n",
    "\n",
    "final_nb = GaussianNB(var_smoothing=best_vs).fit(x_train_final[winning_subset], y_train)\n",
    "y_test_pred = (final_nb.predict_proba(x_test_final[winning_subset])[:, 1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"Final GaussianNB model trained on features {winning_subset}; test predictions ready.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset [2]: CV net-score = 35160.0 | thr = 0.45\n",
      "Subset [2, 3]: CV net-score = 34380.0 | thr = 0.41\n",
      "Subset [2, 3, 6]: CV net-score = 34190.0 | thr = 0.41\n",
      "Subset [2, 3, 6, 5]: CV net-score = 33710.0 | thr = 0.36\n",
      "Subset [2, 3, 6, 5, 462]: CV net-score = 33830.0 | thr = 0.37\n",
      "\n",
      "Winning subset: [2] | net-score = 35160.0 | thr = 0.45\n",
      "Final GaussianNB model trained on features [2]; test predictions ready.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def net_score(y_true, y_pred, n_feats):\n",
    "    return 10 * accuracy_score(y_true, y_pred) * len(y_true) - 200 * n_feats\n",
    "\n",
    "def count_rf_features(model):\n",
    "    return np.count_nonzero(model.feature_importances_)\n",
    "\n",
    "def objective_rf(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 800)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "    max_features = trial.suggest_float(\"max_features\", 0.1, 0.8)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.3, 0.7)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = []\n",
    "    for tr_idx, val_idx in cv.split(x_train_final, y_train):\n",
    "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features,\n",
    "            n_jobs=-1,\n",
    "            random_state=0\n",
    "        ).fit(X_tr, y_tr)\n",
    "        preds = (clf.predict_proba(X_val)[:, 1] >= threshold).astype(int)\n",
    "        scores.append(net_score(y_val, preds, count_rf_features(clf)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_rf, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_thr = best_params.pop(\"threshold\")\n",
    "print(f\"\\nBest RF params: {best_params}, threshold={best_thr:.2f}\")\n",
    "\n",
    "rf = RandomForestClassifier(**best_params, n_jobs=-1, random_state=0).fit(x_train_final, y_train)\n",
    "print(f\"\\nFinal model: #features used = {count_rf_features(rf)}\")\n",
    "\n",
    "print(\"\\nRandom Forest SHAP analysis\")\n",
    "explainer = shap.TreeExplainer(\n",
    "    rf,\n",
    "    model_output=\"probability\",\n",
    "    feature_perturbation=\"interventional\"\n",
    ")\n",
    "\n",
    "sample = shap.sample(x_train_final, 500, random_state=1)\n",
    "phi = explainer.shap_values(sample)[1]           # class-1 (N, F)\n",
    "mean_abs = np.abs(phi).mean(axis=0)\n",
    "shap_imp = pd.Series(mean_abs, index=x_train_final.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop-10 features by mean |SHAP|:\")\n",
    "print(shap_imp.head(10))\n",
    "\n",
    "B, rng = 20, np.random.RandomState(0)\n",
    "feat_count = pd.Series(0, index=x_train_final.columns)\n",
    "\n",
    "for b in range(B):\n",
    "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
    "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
    "    m = RandomForestClassifier(**best_params, n_jobs=-1, random_state=b).fit(Xb, yb)\n",
    "    expl_b = shap.TreeExplainer(\n",
    "        m,\n",
    "        model_output=\"probability\",\n",
    "        feature_perturbation=\"interventional\"\n",
    "    )\n",
    "    phi_b = expl_b.shap_values(Xb.iloc[:300])[1]\n",
    "    mean_abs_b = np.abs(phi_b).mean(axis=0)\n",
    "    sel = Xb.columns[mean_abs_b > np.median(mean_abs_b)]\n",
    "    feat_count[sel] += 1\n",
    "\n",
    "stability = (feat_count / B).sort_values(ascending=False)\n",
    "print(\"\\nBootstrap selection frequency – Top-10:\")\n",
    "print(stability.head(10))\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_subset_rf(feats, rf_params):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, y_tr = x_train_final.iloc[tr_idx][feats], y_train.iloc[tr_idx]\n",
    "        X_val, y_val = x_train_final.iloc[val_idx][feats], y_train.iloc[val_idx]\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators = rf_params[\"n_estimators\"],\n",
    "            max_depth    = rf_params[\"max_depth\"],\n",
    "            max_features = rf_params[\"max_features\"],\n",
    "            n_jobs       = -1,\n",
    "            random_state = 0\n",
    "        ).fit(X_tr, y_tr)\n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:, 1])\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "candidates = [[2], [2, 462, 6]]\n",
    "\n",
    "best_net, winning_subset, winning_thr = -np.inf, None, None\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset_rf(subset, best_params)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f} | thr = {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\n🏆 Winning subset: {winning_subset} | net-score = {best_net:.1f} | thr = {winning_thr:.2f}\")\n",
    "\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators = best_params[\"n_estimators\"],\n",
    "    max_depth    = best_params[\"max_depth\"],\n",
    "    max_features = best_params[\"max_features\"],\n",
    "    n_jobs       = -1,\n",
    "    random_state = 0\n",
    ").fit(x_train_final[winning_subset], y_train)\n",
    "\n",
    "y_test_pred = (final_rf.predict_proba(x_test_final[winning_subset])[:, 1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"✅ Final RandomForest model trained on features {winning_subset}; test predictions ready.\")"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:46:09,492] A new study created in memory with name: no-name-571ff3a9-4693-4c20-b9a0-48150a1990d8\n",
      "Best trial: 0. Best value: -91894:   3%|▎         | 1/30 [00:31<15:16, 31.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:46:41,098] Trial 0 finished with value: -91894.0 and parameters: {'n_estimators': 344, 'max_depth': 9, 'max_features': 0.18847739044982803, 'threshold': 0.4271921442229651}. Best is trial 0 with value: -91894.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -91894:   7%|▋         | 2/30 [00:53<12:09, 26.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:47:03,289] Trial 1 finished with value: -92010.0 and parameters: {'n_estimators': 415, 'max_depth': 4, 'max_features': 0.22766819060550955, 'threshold': 0.6585208840082575}. Best is trial 0 with value: -91894.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -91894:  10%|█         | 3/30 [04:04<45:28, 101.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:50:13,574] Trial 2 finished with value: -91998.0 and parameters: {'n_estimators': 736, 'max_depth': 7, 'max_features': 0.6973167370829153, 'threshold': 0.6660985785106917}. Best is trial 0 with value: -91894.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -91894:  13%|█▎        | 4/30 [06:10<48:08, 111.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:52:20,099] Trial 3 finished with value: -91942.0 and parameters: {'n_estimators': 720, 'max_depth': 4, 'max_features': 0.7972841041016869, 'threshold': 0.3223928643244902}. Best is trial 0 with value: -91894.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -91894:  17%|█▋        | 5/30 [07:22<40:20, 96.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:53:31,550] Trial 4 finished with value: -91970.0 and parameters: {'n_estimators': 444, 'max_depth': 6, 'max_features': 0.48927598985324205, 'threshold': 0.3388431777743617}. Best is trial 0 with value: -91894.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: -91894:  20%|██        | 6/30 [09:08<39:58, 99.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:55:17,594] Trial 5 finished with value: -91952.0 and parameters: {'n_estimators': 684, 'max_depth': 10, 'max_features': 0.3149363223607321, 'threshold': 0.6341543894814001}. Best is trial 0 with value: -91894.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: -91870:  23%|██▎       | 7/30 [09:51<31:15, 81.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:56:01,300] Trial 6 finished with value: -91870.0 and parameters: {'n_estimators': 562, 'max_depth': 8, 'max_features': 0.18608115136541897, 'threshold': 0.4473380843538748}. Best is trial 6 with value: -91870.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: -91870:  27%|██▋       | 8/30 [11:07<29:15, 79.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:57:17,271] Trial 7 finished with value: -92338.0 and parameters: {'n_estimators': 230, 'max_depth': 10, 'max_features': 0.6624210213447221, 'threshold': 0.6983390677193417}. Best is trial 6 with value: -91870.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: -91870:  30%|███       | 9/30 [11:48<23:39, 67.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:57:58,119] Trial 8 finished with value: -92098.0 and parameters: {'n_estimators': 602, 'max_depth': 6, 'max_features': 0.20907072690849132, 'threshold': 0.6511721885870821}. Best is trial 6 with value: -91870.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: -91870:  33%|███▎      | 10/30 [13:07<23:41, 71.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:59:17,017] Trial 9 finished with value: -92022.0 and parameters: {'n_estimators': 520, 'max_depth': 4, 'max_features': 0.6837140856190007, 'threshold': 0.6975245268381826}. Best is trial 6 with value: -91870.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: -91870:  37%|███▋      | 11/30 [13:34<18:16, 57.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 11:59:44,353] Trial 10 finished with value: -91896.0 and parameters: {'n_estimators': 582, 'max_depth': 8, 'max_features': 0.10301538686723477, 'threshold': 0.5254710291224127}. Best is trial 6 with value: -91870.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  40%|████      | 12/30 [14:22<16:26, 54.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:00:32,491] Trial 11 finished with value: -91854.0 and parameters: {'n_estimators': 284, 'max_depth': 8, 'max_features': 0.3970582860392865, 'threshold': 0.4273549330379627}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  43%|████▎     | 13/30 [15:05<14:25, 50.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:01:14,534] Trial 12 finished with value: -91856.0 and parameters: {'n_estimators': 235, 'max_depth': 8, 'max_features': 0.41790978445798616, 'threshold': 0.44583121283668553}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  47%|████▋     | 14/30 [15:49<13:02, 48.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:01:58,668] Trial 13 finished with value: -91858.0 and parameters: {'n_estimators': 228, 'max_depth': 8, 'max_features': 0.4500657545988904, 'threshold': 0.5206571195567813}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  50%|█████     | 15/30 [16:33<11:54, 47.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:02:43,416] Trial 14 finished with value: -91862.0 and parameters: {'n_estimators': 328, 'max_depth': 7, 'max_features': 0.36331943289713486, 'threshold': 0.4051277374310737}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  53%|█████▎    | 16/30 [17:52<13:18, 57.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:04:02,285] Trial 15 finished with value: -91900.0 and parameters: {'n_estimators': 310, 'max_depth': 9, 'max_features': 0.5435189077857945, 'threshold': 0.5759840470464763}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  57%|█████▋    | 17/30 [18:14<10:04, 46.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:04:24,296] Trial 16 finished with value: -91874.0 and parameters: {'n_estimators': 205, 'max_depth': 5, 'max_features': 0.37622501578405426, 'threshold': 0.3827422158959402}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  60%|██████    | 18/30 [19:22<10:35, 52.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:05:32,224] Trial 17 finished with value: -91866.0 and parameters: {'n_estimators': 284, 'max_depth': 9, 'max_features': 0.5105892049904671, 'threshold': 0.47868346148394325}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  63%|██████▎   | 19/30 [20:48<11:32, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:06:58,482] Trial 18 finished with value: -91864.0 and parameters: {'n_estimators': 387, 'max_depth': 7, 'max_features': 0.5766385906782249, 'threshold': 0.37154165089173363}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -91854:  67%|██████▋   | 20/30 [22:07<11:14, 67.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:08:16,493] Trial 19 finished with value: -91864.0 and parameters: {'n_estimators': 451, 'max_depth': 8, 'max_features': 0.39637035210870475, 'threshold': 0.464506552414341}. Best is trial 11 with value: -91854.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  70%|███████   | 21/30 [22:22<07:47, 51.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:08:32,271] Trial 20 finished with value: -64960.0 and parameters: {'n_estimators': 278, 'max_depth': 3, 'max_features': 0.30709732119463856, 'threshold': 0.5423754961473267}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  73%|███████▎  | 22/30 [22:39<05:30, 41.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:08:48,743] Trial 21 finished with value: -65514.0 and parameters: {'n_estimators': 286, 'max_depth': 3, 'max_features': 0.29036784168786073, 'threshold': 0.5464966217923791}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  77%|███████▋  | 23/30 [22:54<03:54, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:09:03,862] Trial 22 finished with value: -65978.0 and parameters: {'n_estimators': 283, 'max_depth': 3, 'max_features': 0.293529365141568, 'threshold': 0.571269023031321}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  80%|████████  | 24/30 [23:14<02:57, 29.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:09:24,432] Trial 23 finished with value: -73576.0 and parameters: {'n_estimators': 375, 'max_depth': 3, 'max_features': 0.2957493675620744, 'threshold': 0.5733267862686489}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  83%|████████▎ | 25/30 [23:31<02:07, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:09:40,671] Trial 24 finished with value: -65012.0 and parameters: {'n_estimators': 278, 'max_depth': 3, 'max_features': 0.2890482773080426, 'threshold': 0.573106654955077}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  87%|████████▋ | 26/30 [23:43<01:26, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:09:53,329] Trial 25 finished with value: -85708.0 and parameters: {'n_estimators': 495, 'max_depth': 3, 'max_features': 0.11293707775546855, 'threshold': 0.6069574248823179}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  90%|█████████ | 27/30 [24:13<01:12, 24.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:10:22,693] Trial 26 finished with value: -91878.0 and parameters: {'n_estimators': 361, 'max_depth': 5, 'max_features': 0.26869923863980477, 'threshold': 0.5368649444400233}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: -64960:  93%|█████████▎| 28/30 [24:33<00:46, 23.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:10:43,434] Trial 27 finished with value: -89894.0 and parameters: {'n_estimators': 260, 'max_depth': 4, 'max_features': 0.32973319029320675, 'threshold': 0.49045738656113513}. Best is trial 20 with value: -64960.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: -55042:  97%|█████████▋| 29/30 [24:44<00:19, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:10:53,509] Trial 28 finished with value: -55042.0 and parameters: {'n_estimators': 200, 'max_depth': 3, 'max_features': 0.24582761191885866, 'threshold': 0.6055500455796807}. Best is trial 28 with value: -55042.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: -55042: 100%|██████████| 30/30 [25:00<00:00, 50.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 12:11:10,189] Trial 29 finished with value: -92006.0 and parameters: {'n_estimators': 336, 'max_depth': 5, 'max_features': 0.15917912600070916, 'threshold': 0.6047295366923146}. Best is trial 28 with value: -55042.0.\n",
      "\n",
      "Best RF params: {'n_estimators': 200, 'max_depth': 3, 'max_features': 0.24582761191885866}, threshold=0.61\n",
      "\n",
      "Final model: #feat = 296\n",
      "\n",
      "Random Forest • SHAP analysis\n"
     ]
    },
    {
     "ename": "ExplainerError",
     "evalue": "The background dataset you provided does not cover all the leaves in the model, so TreeExplainer cannot run with the feature_perturbation=\"tree_path_dependent\" option! Try providing a larger background dataset, no background dataset, or using feature_perturbation=\"interventional\".",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mExplainerError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 64\u001B[39m\n\u001B[32m     61\u001B[39m explainer = shap.TreeExplainer(rf, x_train_final, feature_perturbation=\u001B[33m\"\u001B[39m\u001B[33mtree_path_dependent\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     63\u001B[39m sample = shap.sample(x_train_final, \u001B[32m500\u001B[39m, random_state=\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m phi    = \u001B[43mexplainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshap_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m1\u001B[39m]           \u001B[38;5;66;03m# klasa 1 ⇒ (N, F)\u001B[39;00m\n\u001B[32m     65\u001B[39m mean_abs = np.abs(phi).mean(axis=\u001B[32m0\u001B[39m)\n\u001B[32m     67\u001B[39m shap_imp = pd.Series(mean_abs, index=x_train_final.columns) \\\n\u001B[32m     68\u001B[39m               .sort_values(ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_tree.py:620\u001B[39m, in \u001B[36mTreeExplainer.shap_values\u001B[39m\u001B[34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001B[39m\n\u001B[32m    617\u001B[39m             out = np.stack(out, axis=-\u001B[32m1\u001B[39m)\n\u001B[32m    618\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m X, y, X_missing, flat_output, tree_limit, check_additivity = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_validate_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    621\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtree_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_additivity\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    623\u001B[39m transform = \u001B[38;5;28mself\u001B[39m.model.get_transform()\n\u001B[32m    624\u001B[39m _xgboost_cat_unsupported(\u001B[38;5;28mself\u001B[39m.model)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/AMLProject2/lib/python3.13/site-packages/shap/explainers/_tree.py:478\u001B[39m, in \u001B[36mTreeExplainer._validate_inputs\u001B[39m\u001B[34m(self, X, y, tree_limit, check_additivity)\u001B[39m\n\u001B[32m    468\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.fully_defined_weighting:\n\u001B[32m    469\u001B[39m         emsg = (\n\u001B[32m    470\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mThe background dataset you provided does \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    471\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mnot cover all the leaves in the model, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    476\u001B[39m             \u001B[33m'\u001B[39m\u001B[33mfeature_perturbation=\u001B[39m\u001B[33m\"\u001B[39m\u001B[33minterventional\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    477\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m478\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ExplainerError(emsg)\n\u001B[32m    480\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_additivity \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.model.model_type == \u001B[33m\"\u001B[39m\u001B[33mpyspark\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    481\u001B[39m     warnings.warn(\n\u001B[32m    482\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mcheck_additivity requires us to run predictions which is not supported with \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    483\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mspark, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    484\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mignoring.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    485\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m Set check_additivity=False to remove this warning\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    486\u001B[39m     )\n",
      "\u001B[31mExplainerError\u001B[39m: The background dataset you provided does not cover all the leaves in the model, so TreeExplainer cannot run with the feature_perturbation=\"tree_path_dependent\" option! Try providing a larger background dataset, no background dataset, or using feature_perturbation=\"interventional\"."
     ]
    }
   ],
   "execution_count": 26,
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_subset_rf(feats, rf_params):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, y_tr = x_train_final.iloc[tr_idx][feats], y_train.iloc[tr_idx]\n",
    "        X_val, y_val = x_train_final.iloc[val_idx][feats], y_train.iloc[val_idx]\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators = rf_params[\"n_estimators\"],\n",
    "            max_depth    = rf_params[\"max_depth\"],\n",
    "            max_features = rf_params[\"max_features\"],\n",
    "            n_jobs       = -1,\n",
    "            random_state = 0\n",
    "        ).fit(X_tr, y_tr)\n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:, 1])\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "candidates = [[2], [2, 462]]\n",
    "\n",
    "best_net, winning_subset, winning_thr = -np.inf, None, None\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset_rf(subset, best_params)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f}  |  thr = {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\n🏆 Winning subset: {winning_subset} | net-score = {best_net:.1f} | thr = {winning_thr:.2f}\")\n",
    "\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators = best_params[\"n_estimators\"],\n",
    "    max_depth    = best_params[\"max_depth\"],\n",
    "    max_features = best_params[\"max_features\"],\n",
    "    n_jobs       = -1,\n",
    "    random_state = 0\n",
    ").fit(x_train_final[winning_subset], y_train)\n",
    "\n",
    "y_test_pred = (final_rf.predict_proba(x_test_final[winning_subset])[:, 1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"✅ Final RandomForest model trained on features {winning_subset}; test predictions ready.\")"
   ]
  }
 ]
}
