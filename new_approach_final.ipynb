{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2 - best feature selection methods\n",
        "### Hubert Jaczyński, Aleksandra Kłos, Jakub Oganowski"
      ],
      "metadata": {
        "id": "TrrsTcJ2Dxc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our primary goal is to focus on preprocessing in order to remove as many columns as possible at the beginning. We will calculate statistics, compare training and test datasets. Therefore, let us remove some columns that do not match. Additionally, we will also calculate correlation and check for outliers so that the model is not affected by them as much.\n",
        "\n",
        "Most of the things are outlined in this comprehensive blog: https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions#:~:text=%2A%20Stratified%20KFold%20cross,Time%20Series%20split%20validation"
      ],
      "metadata": {
        "id": "lCvDd1XsEBYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, let us check some features' statistics in both train and test data. Let us then compare top 10 of them:"
      ],
      "metadata": {
        "id": "4IOIPgIyEcRG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipiFsA6YCtyi",
        "outputId": "bd246eb9-2668-493f-dc5f-0c806b63dc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         train_mean  test_mean  mean_diff  train_variance  test_variance  \\\n",
            "feature                                                                    \n",
            "0         15.560411  15.507078  -0.053333       18.730704      17.260227   \n",
            "1         12.650449  12.655507   0.005058       14.317654      13.704232   \n",
            "2         27.750084  27.736016  -0.014067       48.258792      44.019127   \n",
            "3         18.796808  18.825133   0.028325       24.323544      22.632589   \n",
            "4         19.071302  18.995343  -0.075959       27.471500      25.135808   \n",
            "5         11.820110  11.769083  -0.051027       13.312292      12.482966   \n",
            "6         19.365360  19.355964  -0.009395       28.260874      25.857634   \n",
            "7         15.602632  15.517396  -0.085235       19.669401      18.001512   \n",
            "8         14.163618  14.233636   0.070018       19.810112      19.051235   \n",
            "9         15.989661  16.041487   0.051826       22.978933      22.298445   \n",
            "\n",
            "         variance_ratio  corr_with_target  \n",
            "feature                                    \n",
            "0              0.921494          0.300607  \n",
            "1              0.957156          0.290802  \n",
            "2              0.912147          0.374769  \n",
            "3              0.930481          0.339719  \n",
            "4              0.914978          0.316651  \n",
            "5              0.937702          0.301198  \n",
            "6              0.914962          0.338679  \n",
            "7              0.915204          0.320353  \n",
            "8              0.961692          0.307160  \n",
            "9              0.970386          0.283087  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "x_train = pd.read_csv('/content/x_train.txt', sep=r'\\s+', header=None)\n",
        "y_train = pd.read_csv('/content/y_train.txt', sep=r'\\s+', header=None)[0]\n",
        "x_test  = pd.read_csv('/content/x_test.txt',  sep=r'\\s+', header=None)\n",
        "\n",
        "stats = []\n",
        "for i in x_train.columns:\n",
        "    t_mean = x_train[i].mean()\n",
        "    s_mean = x_test[i].mean()\n",
        "    t_var  = x_train[i].var()\n",
        "    s_var  = x_test[i].var()\n",
        "    stats.append({\n",
        "        'feature':             i,\n",
        "        'train_mean':          t_mean,\n",
        "        'test_mean':           s_mean,\n",
        "        'mean_diff':           s_mean - t_mean,\n",
        "        'train_variance':      t_var,\n",
        "        'test_variance':       s_var,\n",
        "        'variance_ratio':      (s_var / t_var) if t_var>0 else np.nan,\n",
        "        'corr_with_target':    x_train[i].corr(y_train)\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(stats).set_index('feature')\n",
        "\n",
        "print(summary_df.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will look at distribution mismatches and test using Komogorov-Smirnov test. Moreover, we will look at outliers and cap them to 1st and 99th percentiles and display mismatches:"
      ],
      "metadata": {
        "id": "pMZFe27bGim5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "q_low  = x_train.quantile(0.01)\n",
        "q_high = x_train.quantile(0.99)\n",
        "\n",
        "stats = []\n",
        "for col in x_train.columns:\n",
        "    ks_stat, ks_p = ks_2samp(x_train[col], x_test[col])\n",
        "    train_outliers = ((x_train[col] < q_low[col]) | (x_train[col] > q_high[col])).sum()\n",
        "    test_outliers  = ((x_test[col]  < q_low[col]) | (x_test[col]  > q_high[col])).sum()\n",
        "    stats.append({\n",
        "        'feature':        col,\n",
        "        'ks_stat':        ks_stat,\n",
        "        'ks_pvalue':      ks_p,\n",
        "        'train_outliers': train_outliers,\n",
        "        'test_outliers':  test_outliers,\n",
        "        'train_q01':      q_low[col],\n",
        "        'train_q99':      q_high[col],\n",
        "        'train_min':      x_train[col].min(),\n",
        "        'train_max':      x_train[col].max(),\n",
        "        'test_min':       x_test[col].min(),\n",
        "        'test_max':       x_test[col].max(),\n",
        "    })\n",
        "\n",
        "stats_df = pd.DataFrame(stats).set_index('feature')\n",
        "\n",
        "print(\"KS statistics for top 10 ks_stat features:\")\n",
        "print(stats_df.sort_values('ks_stat', ascending=False).head(10))\n",
        "\n",
        "x_train_capped = x_train.clip(lower=q_low, upper=q_high, axis=1)\n",
        "x_test_capped  = x_test.clip( lower=q_low, upper=q_high, axis=1)\n",
        "\n",
        "ks_after = []\n",
        "for col in x_train.columns:\n",
        "    ks2, p2 = ks_2samp(x_train_capped[col], x_test_capped[col])\n",
        "    ks_after.append(ks2)\n",
        "stats_df['ks_after_capping'] = ks_after\n",
        "\n",
        "print(\"\\nKS statistics for top 10 ks_stat features: after capping:\")\n",
        "top10 = stats_df.sort_values('ks_stat', ascending=False).head(10)\n",
        "print(top10[['ks_stat','ks_after_capping']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueQOMfhPMI0h",
        "outputId": "1ec35993-664c-4af2-c527-bbc7d84024e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KS statistics for top 10 ks_stat features:\n",
            "         ks_stat  ks_pvalue  train_outliers  test_outliers  train_q01  \\\n",
            "feature                                                                 \n",
            "428       0.0426   0.000229             100            112   2.665452   \n",
            "107       0.0368   0.002291             100             97  -2.315544   \n",
            "130       0.0368   0.002291             100            105  -2.249431   \n",
            "135       0.0354   0.003798             100            110  -2.354010   \n",
            "394       0.0340   0.006174             100            156   0.012440   \n",
            "303       0.0338   0.006607             100             76   0.008643   \n",
            "296       0.0336   0.007068             100             92   0.022230   \n",
            "252       0.0324   0.010504             100            108   0.025570   \n",
            "34        0.0314   0.014451             100             99  -2.257608   \n",
            "266       0.0308   0.017417             100            144   0.030595   \n",
            "\n",
            "         train_q99  train_min  train_max  test_min   test_max  \n",
            "feature                                                        \n",
            "428      23.415190   0.830056  34.928336  1.114300  32.063120  \n",
            "107       2.328245  -3.661105   3.565555 -3.407935   3.174591  \n",
            "130       2.302304  -3.641161   3.584047 -3.387836   3.381168  \n",
            "135       2.281959  -3.492702   3.617008 -3.427228   3.703639  \n",
            "394       0.988695   0.000535   0.999467  0.000059   0.999838  \n",
            "303       0.990880   0.000035   0.999969  0.000126   0.999650  \n",
            "296       9.410959   0.000598  18.144028  0.000620  17.965835  \n",
            "252       9.690338   0.000460  15.506521  0.001080  17.864720  \n",
            "34        2.389097  -3.176476   4.311408 -3.753966   3.770609  \n",
            "266       8.854601   0.000885  17.840303  0.000150  17.373207  \n",
            "\n",
            "KS statistics for top 10 ks_stat features: after capping:\n",
            "         ks_stat  ks_after_capping\n",
            "feature                           \n",
            "428       0.0426            0.0426\n",
            "107       0.0368            0.0368\n",
            "130       0.0368            0.0368\n",
            "135       0.0354            0.0354\n",
            "394       0.0340            0.0340\n",
            "303       0.0338            0.0338\n",
            "296       0.0336            0.0336\n",
            "252       0.0324            0.0324\n",
            "34        0.0314            0.0314\n",
            "266       0.0308            0.0308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will recalculate Komogorov-Smirnov test, use QuantileTransformer to cap the data. Moreover, we will utilize Adversarial Validation using Random Forest to distinguish between test and train, as well as we will drop some of the features that mismatch:"
      ],
      "metadata": {
        "id": "93CHqrrSj5oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "\n",
        "q_low, q_high = x_train.quantile(0.01), x_train.quantile(0.99)\n",
        "\n",
        "stats = []\n",
        "for col in x_train.columns:\n",
        "    ks_stat, _ = ks_2samp(x_train[col], x_test[col])\n",
        "    test_out = ((x_test[col] < q_low[col]) | (x_test[col] > q_high[col])).sum()\n",
        "    stats.append({'feature': col, 'ks_stat': ks_stat, 'test_outliers': test_out})\n",
        "stats_df = pd.DataFrame(stats).set_index('feature')\n",
        "\n",
        "ks_thresh = 0.035\n",
        "outlier_thresh = 120\n",
        "flagged = stats_df[\n",
        "    (stats_df['ks_stat'] >= ks_thresh) |\n",
        "    (stats_df['test_outliers'] > outlier_thresh)\n",
        "].index.tolist()\n",
        "print(f\"Flagged features (KS >= {ks_thresh} or test_outliers > {outlier_thresh}):\\n{flagged}\\n\")\n",
        "\n",
        "qt = QuantileTransformer(output_distribution='uniform', random_state=0)\n",
        "combined = pd.concat([x_train[flagged], x_test[flagged]], axis=0)\n",
        "qt.fit(combined)\n",
        "\n",
        "x_train_qt = x_train.copy()\n",
        "x_test_qt  = x_test.copy()\n",
        "x_train_qt[flagged] = qt.transform(x_train[flagged])\n",
        "x_test_qt[flagged]  = qt.transform(x_test[flagged])\n",
        "\n",
        "X_adv = pd.concat([x_train_qt[flagged], x_test_qt[flagged]], axis=0)\n",
        "y_adv = np.concatenate([np.zeros(len(x_train_qt)), np.ones(len(x_test_qt))])\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_adv, y_adv, test_size=0.3, random_state=0, stratify=y_adv\n",
        ")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf.fit(X_tr, y_tr)\n",
        "\n",
        "y_prob = rf.predict_proba(X_val)[:,1]\n",
        "print(f\"Adversarial AUC: {roc_auc_score(y_val, y_prob):.3f}, \"\n",
        "      f\"Accuracy: {accuracy_score(y_val, rf.predict(X_val)):.3f}\\n\")\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=flagged).sort_values(ascending=False)\n",
        "print(\"Top 10 adversarial-important features:\")\n",
        "print(importances.head(10), \"\\n\")\n",
        "\n",
        "new_stats = []\n",
        "for col in flagged:\n",
        "    ks2, _ = ks_2samp(x_train_qt[col], x_test_qt[col])\n",
        "    new_stats.append({'feature': col, 'ks_after': ks2})\n",
        "new_df = pd.DataFrame(new_stats).set_index('feature')\n",
        "print(\"KS after QuantileTransform:\")\n",
        "print(new_df, \"\\n\")\n",
        "\n",
        "to_drop = new_df[new_df['ks_after'] > ks_thresh].index.tolist()\n",
        "keep_after = [f for f in flagged if f not in to_drop]\n",
        "print(f\"Features to drop (ks_after > {ks_thresh}): {to_drop}\")\n",
        "print(f\"Features to keep: {keep_after}\\n\")\n",
        "\n",
        "x_train_processed = x_train_qt.drop(columns=to_drop)\n",
        "x_test_processed  = x_test_qt.drop(columns=to_drop)\n",
        "\n",
        "print(f\"Final shapes --> x_train: {x_train_processed.shape}, x_test: {x_test_processed.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXreImYIMLsP",
        "outputId": "e5896627-14d3-4220-bab5-71d487623bff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flagged features (KS >= 0.035 or test_outliers > 120):\n",
            "[11, 20, 24, 44, 47, 50, 55, 107, 114, 117, 121, 122, 123, 127, 130, 133, 135, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 428, 430, 434, 439, 464]\n",
            "\n",
            "Adversarial AUC: 0.501, Accuracy: 0.490\n",
            "\n",
            "Top 10 adversarial-important features:\n",
            "130    0.022881\n",
            "107    0.022392\n",
            "428    0.022245\n",
            "388    0.022131\n",
            "135    0.021990\n",
            "184    0.021944\n",
            "117    0.021707\n",
            "394    0.021654\n",
            "204    0.021605\n",
            "232    0.021597\n",
            "dtype: float64 \n",
            "\n",
            "KS after QuantileTransform:\n",
            "         ks_after\n",
            "feature          \n",
            "11         0.0078\n",
            "20         0.0186\n",
            "24         0.0134\n",
            "44         0.0206\n",
            "47         0.0164\n",
            "50         0.0206\n",
            "55         0.0242\n",
            "107        0.0368\n",
            "114        0.0084\n",
            "117        0.0218\n",
            "121        0.0184\n",
            "122        0.0178\n",
            "123        0.0180\n",
            "127        0.0196\n",
            "130        0.0368\n",
            "133        0.0172\n",
            "135        0.0354\n",
            "163        0.0120\n",
            "175        0.0176\n",
            "177        0.0106\n",
            "184        0.0180\n",
            "188        0.0114\n",
            "193        0.0166\n",
            "198        0.0084\n",
            "204        0.0204\n",
            "214        0.0230\n",
            "225        0.0190\n",
            "232        0.0102\n",
            "244        0.0118\n",
            "253        0.0188\n",
            "264        0.0166\n",
            "266        0.0308\n",
            "279        0.0226\n",
            "342        0.0134\n",
            "343        0.0100\n",
            "354        0.0198\n",
            "388        0.0166\n",
            "390        0.0172\n",
            "394        0.0340\n",
            "399        0.0280\n",
            "404        0.0156\n",
            "413        0.0142\n",
            "426        0.0138\n",
            "428        0.0426\n",
            "430        0.0240\n",
            "434        0.0096\n",
            "439        0.0184\n",
            "464        0.0218 \n",
            "\n",
            "Features to drop (ks_after > 0.035): [107, 130, 135, 428]\n",
            "Features to keep: [11, 20, 24, 44, 47, 50, 55, 114, 117, 121, 122, 123, 127, 133, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 430, 434, 439, 464]\n",
            "\n",
            "Final shapes --> x_train: (5000, 496), x_test: (5000, 496)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, let us check Variance Threshold that are near-zero. Furthermore, we will remove highly correlated features if there exist such. We will also scale the remaining features using both train and test data:"
      ],
      "metadata": {
        "id": "gvI3ZxVGkS_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "sel = VarianceThreshold(threshold=1e-5)\n",
        "sel.fit(x_train_processed)\n",
        "keep_var = x_train_processed.columns[sel.get_support()]\n",
        "drop_var = [c for c in x_train_processed.columns if c not in keep_var]\n",
        "print(\"Dropped for near-zero variance:\", drop_var)\n",
        "x_train_var = x_train_processed[keep_var].copy()\n",
        "x_test_var  = x_test_processed[keep_var].copy()\n",
        "\n",
        "corr = x_train_var.corr().abs()\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "drop_corr = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
        "print(\"Dropped for high correlation:\", drop_corr)\n",
        "x_train_corr = x_train_var.drop(columns=drop_corr).copy()\n",
        "x_test_corr  = x_test_var.drop(columns=drop_corr).copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "combined = pd.concat([x_train_corr, x_test_corr], axis=0)\n",
        "scaler.fit(combined)\n",
        "\n",
        "x_train_final = pd.DataFrame(\n",
        "    scaler.transform(x_train_corr),\n",
        "    columns=x_train_corr.columns,\n",
        "    index=x_train_corr.index\n",
        ")\n",
        "x_test_final = pd.DataFrame(\n",
        "    scaler.transform(x_test_corr),\n",
        "    columns=x_test_corr.columns,\n",
        "    index=x_test_corr.index\n",
        ")\n",
        "\n",
        "print(\"Final shapes -->\", x_train_final.shape, x_test_final.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI0DrgtVMQeh",
        "outputId": "e36fab7c-0a5a-4d0f-ac3b-482d5204cdd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped for near-zero variance: []\n",
            "Dropped for high correlation: [7]\n",
            "Final shapes --> (5000, 495) (5000, 495)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will install hyperparameter optimization framework Optuna:"
      ],
      "metadata": {
        "id": "KVbDmcAYknZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpf68SLNvLio",
        "outputId": "1f6012e9-89d7-4e08-e505-d564baa1d864"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, for all classifiers of choice, we will create our net_score that we want to minimize, do an optuna search to find best parameters of C and threshold. After that, we will use SHAP to rank features by importance. What is more, we will utilize the Bootstrap stability check:"
      ],
      "metadata": {
        "id": "wZOiaFx6kzXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "import shap\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = pd.DataFrame(scaler.fit_transform(x_train_final), columns=x_train_final.columns)\n",
        "x_test_scaled = pd.DataFrame(scaler.transform(x_test_final), columns=x_test_final.columns)\n",
        "\n",
        "\n",
        "def net_score(y_true, y_pred, n_features):\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return 10 * acc * len(y_true) - 200 * n_features\n",
        "\n",
        "def count_tree_features(model):\n",
        "    imp = model.feature_importances_\n",
        "    return np.count_nonzero(imp)\n",
        "\n",
        "\n",
        "def objective_logistic(trial):\n",
        "    C = trial.suggest_float('C', 0.001, 0.01, log=True)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = LogisticRegression(\n",
        "            penalty='l1', solver='saga', C=C,\n",
        "            max_iter=5000, random_state=0)\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = np.count_nonzero(model.coef_)\n",
        "\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "print(\"\\n--- Tuning logistic regression (L1) ---\")\n",
        "study_log = optuna.create_study(direction='maximize')\n",
        "study_log.optimize(objective_logistic, n_trials=20)\n",
        "\n",
        "best_log = study_log.best_params\n",
        "best_C_log = best_log['C']\n",
        "best_thr_log = best_log['threshold']\n",
        "print(\"Logistic L1 best parameters:\", best_log)\n",
        "\n",
        "print(\"Fitting final logistic L1 model...\")\n",
        "model_log = LogisticRegression(\n",
        "    penalty='l1', solver='saga', C=best_C_log,\n",
        "    max_iter=5000, random_state=0)\n",
        "model_log.fit(x_train_final, y_train)\n",
        "\n",
        "feat_log = list(np.where(model_log.coef_[0] != 0)[0])\n",
        "\n",
        "\n",
        "def objective_svm(trial):\n",
        "    C = trial.suggest_float('C', 0.01, 10.0, log=True)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_scaled, y_train):\n",
        "        X_tr, X_val = x_train_scaled.iloc[tr_idx, feat_log], x_train_scaled.iloc[val_idx, feat_log]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = SVC(C=C, kernel='linear', probability=True, random_state=0)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = len(feat_log)\n",
        "\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "print(\"\\n--- Tuning SVM ---\")\n",
        "study_svm = optuna.create_study(direction='maximize')\n",
        "study_svm.optimize(objective_svm, n_trials=15)\n",
        "\n",
        "best_svm = study_svm.best_params\n",
        "best_C_svm = best_svm['C']\n",
        "best_thr_svm = best_svm['threshold']\n",
        "print(\"SVM best parameters:\", best_svm)\n",
        "\n",
        "print(\"Fitting final SVM model...\")\n",
        "model_svm = SVC(C=best_C_svm, kernel='linear', probability=True, random_state=0)\n",
        "model_svm.fit(x_train_scaled.iloc[:, feat_log], y_train)\n",
        "\n",
        "\n",
        "\n",
        "def objective_knn(trial):\n",
        "    n_neighbors = trial.suggest_int('n_neighbors', 3, 15)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_scaled, y_train):\n",
        "        X_tr, X_val = x_train_scaled.iloc[tr_idx, feat_log], x_train_scaled.iloc[val_idx, feat_log]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = len(feat_log)\n",
        "\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "print(\"\\n--- Tuning KNN ---\")\n",
        "study_knn = optuna.create_study(direction='maximize')\n",
        "study_knn.optimize(objective_knn, n_trials=15)\n",
        "\n",
        "best_knn = study_knn.best_params\n",
        "best_n_neighbors = best_knn['n_neighbors']\n",
        "best_thr_knn = best_knn['threshold']\n",
        "print(\"KNN best parameters:\", best_knn)\n",
        "\n",
        "print(\"Fitting final KNN model...\")\n",
        "model_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
        "model_knn.fit(x_train_scaled.iloc[:, feat_log], y_train)\n",
        "\n",
        "\n",
        "def objective_elastic(trial):\n",
        "    C = trial.suggest_float('C', 0.001, 0.1, log=True)\n",
        "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 1.0)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = LogisticRegression(\n",
        "            penalty='elasticnet', solver='saga', C=C,\n",
        "            l1_ratio=l1_ratio, max_iter=5000, random_state=0)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = np.count_nonzero(model.coef_)\n",
        "\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "print(\"\\n--- Tuning logistic regression (ElasticNet) ---\")\n",
        "study_enet = optuna.create_study(direction='maximize')\n",
        "study_enet.optimize(objective_elastic, n_trials=20)\n",
        "\n",
        "best_enet = study_enet.best_params\n",
        "best_C_enet = best_enet['C']\n",
        "best_l1_ratio = best_enet['l1_ratio']\n",
        "best_thr_enet = best_enet['threshold']\n",
        "print(\"ElasticNet best parameters:\", best_enet)\n",
        "\n",
        "print(\"Fitting final ElasticNet model...\")\n",
        "model_enet = LogisticRegression(\n",
        "    penalty='elasticnet', solver='saga',\n",
        "    C=best_C_enet, l1_ratio=best_l1_ratio,\n",
        "    max_iter=5000, random_state=0)\n",
        "model_enet.fit(x_train_final, y_train)\n",
        "\n",
        "\n",
        "def objective_rf(trial):\n",
        "    estimators = trial.suggest_int('n_estimators', 100, 500)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "    max_features = trial.suggest_float('max_features', 0.05, 0.5)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = RandomForestClassifier()\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = count_tree_features(model)\n",
        "\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "print(\"\\n--- Tuning random forest ---\")\n",
        "study_rf = optuna.create_study(direction='maximize')\n",
        "study_rf.optimize(objective_rf, n_trials=20)\n",
        "\n",
        "best_rf = study_rf.best_params\n",
        "best_mf_rf = best_rf['max_features']\n",
        "best_thr_rf = best_rf['threshold']\n",
        "best_estimators_rf = best_rf['n_estimators']\n",
        "print(\"RandomForest best parameters:\", best_rf)\n",
        "\n",
        "print(\"Fitting final random forest model...\")\n",
        "model_rf = RandomForestClassifier(\n",
        "    n_estimators=best_estimators_rf,\n",
        "    max_features=best_mf_rf,\n",
        "    random_state=0, n_jobs=-1)\n",
        "model_rf.fit(x_train_final, y_train)\n",
        "\n",
        "\n",
        "def objective_xgb(trial):\n",
        "    lambda_l1 = trial.suggest_float('lambda_l1', 50.0, 200.0, log=True)\n",
        "    max_depth = trial.suggest_int('max_depth', 2, 4)\n",
        "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 0.3)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "    estimators = trial.suggest_int('n_estimators', 100, 500)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=estimators,\n",
        "            eval_metric='logloss',\n",
        "            reg_alpha=lambda_l1,\n",
        "            reg_lambda=0.0,\n",
        "            max_depth=max_depth,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            random_state=0, n_jobs=-1)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = count_tree_features(model)\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "print(\"\\n--- Tuning XGBoost ---\")\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=20)\n",
        "\n",
        "best_xgb = study_xgb.best_params\n",
        "best_l1_xgb = best_xgb['lambda_l1']\n",
        "best_depth_xgb = best_xgb['max_depth']\n",
        "best_col_xgb = best_xgb['colsample_bytree']\n",
        "best_thr_xgb = best_xgb['threshold']\n",
        "best_estimators_xgb = best_xgb['n_estimators']\n",
        "print(\"XGBoost best parameters:\", best_xgb)\n",
        "\n",
        "print(\"Fitting final XGBoost model...\")\n",
        "model_xgb = XGBClassifier(\n",
        "    n_estimators=best_estimators_xgb,\n",
        "    eval_metric='logloss',\n",
        "    reg_alpha=best_l1_xgb,\n",
        "    reg_lambda=0.0,\n",
        "    max_depth=best_depth_xgb,\n",
        "    colsample_bytree=best_col_xgb,\n",
        "    random_state=0, n_jobs=-1)\n",
        "model_xgb.fit(x_train_final, y_train)\n",
        "\n",
        "\n",
        "def objective_lgb(trial):\n",
        "    lambda_l1 = trial.suggest_float('lambda_l1', 1.0, 100.0, log=True)\n",
        "    max_depth = trial.suggest_int('max_depth', 2, 4)\n",
        "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 0.3)\n",
        "    threshold = trial.suggest_float('threshold', 0.3, 0.7)\n",
        "    estimators = trial.suggest_int('n_estimators', 100, 500)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    fold_scores = []\n",
        "\n",
        "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
        "        X_tr, X_val = x_train_final.iloc[tr_idx], x_train_final.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = LGBMClassifier(\n",
        "            n_estimators=estimators,\n",
        "            reg_lambda=lambda_l1,\n",
        "            reg_alpha=0.0,\n",
        "            max_depth=max_depth,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            random_state=0, n_jobs=-1)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        probs = model.predict_proba(X_val)[:, 1]\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        n_feats = count_tree_features(model)\n",
        "        fold_scores.append(net_score(y_val, preds, n_feats))\n",
        "\n",
        "    return np.mean(fold_scores)\n",
        "\n",
        "\n",
        "print(\"\\n--- Tuning LightGBM ---\")\n",
        "study_lgb = optuna.create_study(direction='maximize')\n",
        "study_lgb.optimize(objective_lgb, n_trials=20)\n",
        "\n",
        "best_lgb = study_lgb.best_params\n",
        "best_l1_lgb = best_lgb['lambda_l1']\n",
        "best_depth_lgb = best_lgb['max_depth']\n",
        "best_col_lgb = best_lgb['colsample_bytree']\n",
        "best_thr_lgb = best_lgb['threshold']\n",
        "best_estimators_lgb = best_lgb['n_estimators']\n",
        "print(\"LightGBM best parameters:\", best_lgb)\n",
        "\n",
        "print(\"Fitting final LightGBM model...\")\n",
        "model_lgb = LGBMClassifier(\n",
        "    n_estimators=best_estimators_lgb,\n",
        "    reg_lambda=best_l1_lgb,\n",
        "    reg_alpha=0.0,\n",
        "    max_depth=best_depth_lgb,\n",
        "    colsample_bytree=best_col_lgb,\n",
        "    random_state=0, n_jobs=-1)\n",
        "model_lgb.fit(x_train_final, y_train)\n",
        "\n",
        "\n",
        "print(\"\\nFinal model tuning results:\")\n",
        "print(\"Logistic L1: C =\", best_C_log, \", threshold =\", best_thr_log,\n",
        "      \", # features =\", np.count_nonzero(model_log.coef_))\n",
        "print(\"SVM: C =\", best_C_svm, \", threshold =\", best_thr_svm,\n",
        "      \", # features =\", len(feat_log))\n",
        "print(\"KNN: n_neighbors =\", best_n_neighbors, \", threshold =\", best_thr_knn,\n",
        "      \", # features =\", len(feat_log))\n",
        "print(\"ElasticNet: C =\", best_C_enet, \", l1_ratio =\", best_l1_ratio,\n",
        "      \", thresh =\", best_thr_enet, \", #feat =\", np.count_nonzero(model_enet.coef_))\n",
        "print(\"RandomForest: max_features =\", best_mf_rf,\n",
        "      \", thresh =\", best_thr_rf, \", # features =\", count_tree_features(model_rf))\n",
        "print(\"XGBoost: lambda_l1 =\", best_l1_xgb, \", max_depth =\", best_depth_xgb,\n",
        "      \", colsample_bytree =\", best_col_xgb,\n",
        "      \", threshold =\", best_thr_xgb, \", #feat =\", count_tree_features(model_xgb))\n",
        "print(\"LightGBM: lambda_l1 =\", best_l1_lgb, \", max_depth =\", best_depth_lgb,\n",
        "      \", colsample_bytree =\", best_col_lgb,\n",
        "      \", threshold =\", best_thr_lgb, \", # features =\", count_tree_features(model_lgb))\n",
        "\n",
        "\n",
        "models_info = [\n",
        "    (\"Logistic L1\", model_log, best_thr_log, \"linear\"),\n",
        "    (\"SVM\", model_svm, best_thr_svm, \"linear\"),\n",
        "    (\"KNN\", model_knn, best_thr_knn, \"linear\"),\n",
        "    (\"ElasticNet\", model_enet, best_thr_enet, \"linear\"),\n",
        "    (\"RandomForest\", model_rf, best_thr_rf, \"tree\"),\n",
        "    (\"XGBoost\", model_xgb, best_thr_xgb, \"tree\"),\n",
        "    (\"LightGBM\", model_lgb, best_thr_lgb, \"tree\")]\n",
        "\n",
        "for name, model, thr, model_type in models_info:\n",
        "    print(f\"\\n--- {name} Analysis ---\")\n",
        "    test_probs = model.predict_proba(x_test_final)[:, 1]\n",
        "    y_test_pred = (test_probs >= thr).astype(int)\n",
        "    frac_pos = np.mean(y_test_pred)\n",
        "    print(f\"Threshold = {thr:.2f}, # positives = {int(frac_pos * len(test_probs))} / {len(test_probs)}\")\n",
        "\n",
        "    if model_type == \"linear\":\n",
        "        explainer = shap.LinearExplainer(model, x_train_final, feature_perturbation=\"interventional\")\n",
        "        shap_vals = explainer.shap_values(x_train_final)\n",
        "\n",
        "    else:\n",
        "        explainer = shap.TreeExplainer(model, x_train_final)\n",
        "        shap_vals = explainer.shap_values(x_train_final)\n",
        "\n",
        "        if isinstance(shap_vals, list):\n",
        "            shap_vals = shap_vals[1]\n",
        "\n",
        "    mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
        "    shap_imp = pd.Series(mean_abs_shap, index=x_train_final.columns).sort_values(ascending=False)\n",
        "    print(\"Top 10 features by mean |SHAP|:\")\n",
        "    print(shap_imp.head(10))\n",
        "\n",
        "    B = 100\n",
        "    rng = np.random.RandomState(0)\n",
        "    feat_count = pd.Series(0, index=x_train_final.columns)\n",
        "\n",
        "    for i in range(B):\n",
        "        idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
        "        Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
        "\n",
        "        if model_type == \"linear\":\n",
        "            m = LogisticRegression(\n",
        "                penalty='l1' if name==\"Logistic L1\" else 'elasticnet',\n",
        "                solver='saga',\n",
        "                C=(best_C_log if name==\"Logistic L1\" else best_C_enet),\n",
        "                l1_ratio=(best_l1_ratio if name==\"ElasticNet\" else None),\n",
        "                max_iter=5000, random_state=i)\n",
        "\n",
        "        elif name == \"RandomForest\":\n",
        "            m = RandomForestClassifier(\n",
        "                n_estimators=100, max_features=best_mf_rf,\n",
        "                random_state=i, n_jobs=-1)\n",
        "\n",
        "        elif name == \"XGBoost\":\n",
        "            m = XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                eval_metric='logloss',\n",
        "                reg_lambda=best_l1_xgb, max_depth=best_depth_xgb,\n",
        "                colsample_bytree=best_col_xgb,\n",
        "                random_state=i, n_jobs=-1)\n",
        "\n",
        "        else:\n",
        "            m = LGBMClassifier(\n",
        "                n_estimators=100,\n",
        "                reg_lambda=best_l1_lgb,\n",
        "                max_depth=best_l1_lgb,\n",
        "                colsample_bytree=best_col_lgb,\n",
        "                random_state=i, n_jobs=-1)\n",
        "\n",
        "        m.fit(Xb, yb)\n",
        "\n",
        "        if model_type == \"linear\":\n",
        "            sel = x_train_final.columns[m.coef_[0] != 0]\n",
        "\n",
        "        else:\n",
        "            sel = x_train_final.columns[m.feature_importances_ > 0]\n",
        "        feat_count[sel] += 1\n",
        "\n",
        "    stability = (feat_count / B).sort_values(ascending=False)\n",
        "    print(\"Bootstrap selection frequency (top 10):\")\n",
        "    print(stability.head(10))\n",
        "\n",
        "\n",
        "feat_nb = list(range(x_train_final.shape[1]))\n",
        "feat_enet = list(np.where(model_enet.coef_[0] != 0)[0])\n",
        "feat_rf = list(np.where(model_rf.feature_importances_ > 0)[0])\n",
        "feat_xgb = list(np.where(model_xgb.feature_importances_ > 0)[0])\n",
        "feat_lgb = list(np.where(model_lgb.feature_importances_ > 0)[0])\n",
        "\n",
        "\n",
        "model_dicts = [\n",
        "    {'name': 'Logistic L1', 'model': LogisticRegression, 'params': {'penalty': 'l1', 'solver': 'saga', 'C': best_C_log, 'max_iter': 10000, 'random_state': 0}},\n",
        "    {'name': 'SVM', 'model': SVC, 'params': {'C': best_C_svm, 'kernel': 'linear', 'probability': True, 'random_state': 0}},\n",
        "    {'name': 'KNN', 'model': KNeighborsClassifier, 'params': {'n_neighbors': best_n_neighbors}},\n",
        "    {'name': 'ElasticNet', 'model': LogisticRegression, 'params': {'penalty': 'elasticnet', 'solver': 'saga', 'C': best_C_enet, 'l1_ratio': best_l1_ratio, 'max_iter': 10000, 'random_state': 0}},\n",
        "    {'name': 'RandomForest', 'model': RandomForestClassifier, 'params': {'n_estimators': best_estimators_rf, 'max_features': best_mf_rf, 'random_state': 0, 'n_jobs': -1}},\n",
        "    {'name': 'XGBoost', 'model': XGBClassifier, 'params': {'n_estimators': best_estimators_xgb, 'reg_alpha': best_l1_xgb, 'reg_lambda': 0.0, 'max_depth': best_depth_xgb, 'colsample_bytree': best_col_xgb, 'eval_metric': 'logloss', 'random_state': 0, 'n_jobs': -1}},\n",
        "    {'name': 'LightGBM', 'model': LGBMClassifier, 'params': {'n_estimators': best_estimators_lgb, 'reg_lambda': best_l1_lgb, 'reg_alpha': 0.0, 'max_depth': best_depth_lgb, 'colsample_bytree': best_col_lgb, 'random_state': 0, 'n_jobs': -1}},]\n",
        "\n",
        "feature_dicts = {\n",
        "    'Logistic L1': feat_log,\n",
        "    'SVM': feat_log,\n",
        "    'KNN': feat_log,\n",
        "    'ElasticNet': feat_enet,\n",
        "    'RandomForest': feat_rf,\n",
        "    'XGBoost': feat_xgb,\n",
        "    'LightGBM': feat_lgb,}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFfoT3q4Fvh9",
        "outputId": "93005b32-66d9-4359-8e37-3160fbab0a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:17:43,249] A new study created in memory with name: no-name-fde4bbf8-8c84-4780-8ca0-56e5748cc90c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuning logistic regression (L1) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:17:46,152] Trial 0 finished with value: 5202.0 and parameters: {'C': 0.001854123988286725, 'threshold': 0.5754726744893557}. Best is trial 0 with value: 5202.0.\n",
            "[I 2025-06-02 12:17:48,975] Trial 1 finished with value: 5420.0 and parameters: {'C': 0.0029568484555345595, 'threshold': 0.3766128268257427}. Best is trial 1 with value: 5420.0.\n",
            "[I 2025-06-02 12:17:51,934] Trial 2 finished with value: 6862.0 and parameters: {'C': 0.0025830842130550517, 'threshold': 0.4786808089002411}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:17:56,080] Trial 3 finished with value: 5470.0 and parameters: {'C': 0.006107622334077325, 'threshold': 0.3390098029677393}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:17:59,467] Trial 4 finished with value: 6766.0 and parameters: {'C': 0.005081173469315885, 'threshold': 0.4978502177671862}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:05,172] Trial 5 finished with value: 5352.0 and parameters: {'C': 0.009127768061167345, 'threshold': 0.690114488533865}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:09,865] Trial 6 finished with value: 6184.0 and parameters: {'C': 0.007043011023470311, 'threshold': 0.40334814509828165}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:11,136] Trial 7 finished with value: 4886.0 and parameters: {'C': 0.0010251760154446848, 'threshold': 0.4811478594414925}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:17,069] Trial 8 finished with value: 5412.0 and parameters: {'C': 0.009857099550783004, 'threshold': 0.6533782234913746}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:19,921] Trial 9 finished with value: 5538.0 and parameters: {'C': 0.0020074362046052082, 'threshold': 0.42439307418665384}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:22,699] Trial 10 finished with value: 5794.0 and parameters: {'C': 0.0031516798838796527, 'threshold': 0.5846833442729125}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:25,835] Trial 11 finished with value: 6810.0 and parameters: {'C': 0.004625661573414017, 'threshold': 0.4977462361902306}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:29,005] Trial 12 finished with value: 6400.0 and parameters: {'C': 0.0042490082772629915, 'threshold': 0.5407037414865296}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:31,969] Trial 13 finished with value: 6250.0 and parameters: {'C': 0.0020946723249450363, 'threshold': 0.44983255759577545}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:35,056] Trial 14 finished with value: 6452.0 and parameters: {'C': 0.00419395836831959, 'threshold': 0.5363006732851459}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:38,295] Trial 15 finished with value: 4914.0 and parameters: {'C': 0.0014906511884494554, 'threshold': 0.6139830036065612}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:41,290] Trial 16 finished with value: 4762.0 and parameters: {'C': 0.0027766162240343605, 'threshold': 0.30501494877737045}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:44,418] Trial 17 finished with value: 6746.0 and parameters: {'C': 0.0039516032417559336, 'threshold': 0.4639230024125576}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:47,398] Trial 18 finished with value: 6288.0 and parameters: {'C': 0.0025846202111565603, 'threshold': 0.5325514930173167}. Best is trial 2 with value: 6862.0.\n",
            "[I 2025-06-02 12:18:50,707] Trial 19 finished with value: 4686.0 and parameters: {'C': 0.001375702031179252, 'threshold': 0.40686071458930395}. Best is trial 2 with value: 6862.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic L1 best parameters: {'C': 0.0025830842130550517, 'threshold': 0.4786808089002411}\n",
            "Fitting final logistic L1 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:18:51,391] A new study created in memory with name: no-name-86e5ec3e-1552-4607-8182-a9ee62bf69f3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuning SVM ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:18:59,722] Trial 0 finished with value: 5744.0 and parameters: {'C': 1.7806057055030342, 'threshold': 0.6798704900678589}. Best is trial 0 with value: 5744.0.\n",
            "[I 2025-06-02 12:19:06,541] Trial 1 finished with value: 6858.0 and parameters: {'C': 0.07493009992846221, 'threshold': 0.49621676721188673}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:13,889] Trial 2 finished with value: 5898.0 and parameters: {'C': 0.6103985660417276, 'threshold': 0.33382811718035743}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:20,832] Trial 3 finished with value: 5972.0 and parameters: {'C': 0.45825482307621745, 'threshold': 0.6457699578470654}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:27,651] Trial 4 finished with value: 6718.0 and parameters: {'C': 0.06787659551481384, 'threshold': 0.5325867279108601}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:34,577] Trial 5 finished with value: 6234.0 and parameters: {'C': 0.44301128405193446, 'threshold': 0.37294299639276024}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:43,580] Trial 6 finished with value: 6388.0 and parameters: {'C': 2.533489266903811, 'threshold': 0.5821079007569099}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:50,385] Trial 7 finished with value: 6320.0 and parameters: {'C': 0.018098028947347123, 'threshold': 0.5896562665849757}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:19:57,188] Trial 8 finished with value: 6090.0 and parameters: {'C': 0.014387462144415313, 'threshold': 0.6288667412046638}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:20:04,051] Trial 9 finished with value: 5828.0 and parameters: {'C': 0.2098512021977672, 'threshold': 0.3242921535644511}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:20:10,911] Trial 10 finished with value: 6702.0 and parameters: {'C': 0.0771752576703565, 'threshold': 0.4305668012764028}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:20:17,798] Trial 11 finished with value: 6858.0 and parameters: {'C': 0.06475743748200054, 'threshold': 0.5005209688640122}. Best is trial 1 with value: 6858.0.\n",
            "[I 2025-06-02 12:20:24,632] Trial 12 finished with value: 6880.0 and parameters: {'C': 0.061302609149575304, 'threshold': 0.4746153029712166}. Best is trial 12 with value: 6880.0.\n",
            "[I 2025-06-02 12:20:31,450] Trial 13 finished with value: 6758.0 and parameters: {'C': 0.0354315269402416, 'threshold': 0.4460307136324683}. Best is trial 12 with value: 6880.0.\n",
            "[I 2025-06-02 12:20:38,301] Trial 14 finished with value: 6738.0 and parameters: {'C': 0.14051019586992655, 'threshold': 0.4417608509905363}. Best is trial 12 with value: 6880.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM best parameters: {'C': 0.061302609149575304, 'threshold': 0.4746153029712166}\n",
            "Fitting final SVM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:20:40,495] A new study created in memory with name: no-name-0eccf1f5-5846-40e2-8bf9-909dd2ad28cc\n",
            "[I 2025-06-02 12:20:40,535] Trial 0 finished with value: 6350.0 and parameters: {'n_neighbors': 15, 'threshold': 0.39824739147629806}. Best is trial 0 with value: 6350.0.\n",
            "[I 2025-06-02 12:20:40,571] Trial 1 finished with value: 5680.0 and parameters: {'n_neighbors': 6, 'threshold': 0.30281981340497977}. Best is trial 0 with value: 6350.0.\n",
            "[I 2025-06-02 12:20:40,609] Trial 2 finished with value: 6168.0 and parameters: {'n_neighbors': 14, 'threshold': 0.32278009134287083}. Best is trial 0 with value: 6350.0.\n",
            "[I 2025-06-02 12:20:40,648] Trial 3 finished with value: 6640.0 and parameters: {'n_neighbors': 14, 'threshold': 0.4357914798234114}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,684] Trial 4 finished with value: 6388.0 and parameters: {'n_neighbors': 7, 'threshold': 0.5382458853215801}. Best is trial 3 with value: 6640.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuning KNN ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:20:40,722] Trial 5 finished with value: 6268.0 and parameters: {'n_neighbors': 11, 'threshold': 0.6377995751375667}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,758] Trial 6 finished with value: 6466.0 and parameters: {'n_neighbors': 10, 'threshold': 0.46847035896428224}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,794] Trial 7 finished with value: 6404.0 and parameters: {'n_neighbors': 8, 'threshold': 0.4292677886604585}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,831] Trial 8 finished with value: 6144.0 and parameters: {'n_neighbors': 4, 'threshold': 0.5104522019894124}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,867] Trial 9 finished with value: 6402.0 and parameters: {'n_neighbors': 9, 'threshold': 0.6048911108814712}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,913] Trial 10 finished with value: 6358.0 and parameters: {'n_neighbors': 12, 'threshold': 0.3865960966258747}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:40,959] Trial 11 finished with value: 6548.0 and parameters: {'n_neighbors': 12, 'threshold': 0.4559232900121679}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:41,006] Trial 12 finished with value: 6624.0 and parameters: {'n_neighbors': 13, 'threshold': 0.5668039421664933}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:41,053] Trial 13 finished with value: 6580.0 and parameters: {'n_neighbors': 14, 'threshold': 0.5719394437840681}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:41,100] Trial 14 finished with value: 6374.0 and parameters: {'n_neighbors': 13, 'threshold': 0.6806795430924022}. Best is trial 3 with value: 6640.0.\n",
            "[I 2025-06-02 12:20:41,105] A new study created in memory with name: no-name-032eb77e-a567-4222-aa64-c67c0dbb0c42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN best parameters: {'n_neighbors': 14, 'threshold': 0.4357914798234114}\n",
            "Fitting final KNN model...\n",
            "\n",
            "--- Tuning logistic regression (ElasticNet) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:22:13,360] Trial 0 finished with value: -67830.0 and parameters: {'C': 0.09675828747147912, 'l1_ratio': 0.7937960359603429, 'threshold': 0.6758000155546047}. Best is trial 0 with value: -67830.0.\n",
            "[I 2025-06-02 12:22:16,368] Trial 1 finished with value: 4914.0 and parameters: {'C': 0.0010007096409628984, 'l1_ratio': 0.6860914744718605, 'threshold': 0.6454157477511124}. Best is trial 1 with value: 4914.0.\n",
            "[I 2025-06-02 12:22:34,046] Trial 2 finished with value: -61894.0 and parameters: {'C': 0.025834433591715254, 'l1_ratio': 0.2845030527179061, 'threshold': 0.6553203438225808}. Best is trial 1 with value: 4914.0.\n",
            "[I 2025-06-02 12:22:40,238] Trial 3 finished with value: -31808.0 and parameters: {'C': 0.006158281263123593, 'l1_ratio': 0.15974649441278216, 'threshold': 0.6212389189420671}. Best is trial 1 with value: 4914.0.\n",
            "[I 2025-06-02 12:23:44,587] Trial 4 finished with value: -75194.0 and parameters: {'C': 0.09621264523036395, 'l1_ratio': 0.5593722349571608, 'threshold': 0.6021881434284473}. Best is trial 1 with value: 4914.0.\n",
            "[I 2025-06-02 12:23:48,471] Trial 5 finished with value: 5244.0 and parameters: {'C': 0.0019490387550862183, 'l1_ratio': 0.7625802803082565, 'threshold': 0.5908180737579845}. Best is trial 5 with value: 5244.0.\n",
            "[I 2025-06-02 12:23:51,329] Trial 6 finished with value: 4696.0 and parameters: {'C': 0.001389705972268633, 'l1_ratio': 0.7408947152201819, 'threshold': 0.386233638469827}. Best is trial 5 with value: 5244.0.\n",
            "[I 2025-06-02 12:23:55,193] Trial 7 finished with value: 4090.0 and parameters: {'C': 0.0027543493850139896, 'l1_ratio': 0.32847559593158876, 'threshold': 0.6737441085958056}. Best is trial 5 with value: 5244.0.\n",
            "[I 2025-06-02 12:24:35,383] Trial 8 finished with value: -42514.0 and parameters: {'C': 0.052410486168208884, 'l1_ratio': 0.9728172104411987, 'threshold': 0.6515317809120087}. Best is trial 5 with value: 5244.0.\n",
            "[I 2025-06-02 12:24:38,035] Trial 9 finished with value: 6468.000000000001 and parameters: {'C': 0.00258976797964946, 'l1_ratio': 0.9558504642872871, 'threshold': 0.5232410255936546}. Best is trial 9 with value: 6468.000000000001.\n",
            "[I 2025-06-02 12:24:43,565] Trial 10 finished with value: 6646.0 and parameters: {'C': 0.006704206256840547, 'l1_ratio': 0.9417567165670416, 'threshold': 0.46500394488278407}. Best is trial 10 with value: 6646.0.\n",
            "[I 2025-06-02 12:24:48,369] Trial 11 finished with value: 6766.0 and parameters: {'C': 0.006710491299252708, 'l1_ratio': 0.980956554686088, 'threshold': 0.47609595295077023}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:24:56,423] Trial 12 finished with value: 5966.0 and parameters: {'C': 0.00955606148164409, 'l1_ratio': 0.9036873650355267, 'threshold': 0.41191584034113937}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:25:05,242] Trial 13 finished with value: 5808.0 and parameters: {'C': 0.006320973170880787, 'l1_ratio': 0.5873862334691733, 'threshold': 0.464908535124789}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:25:27,694] Trial 14 finished with value: -10366.0 and parameters: {'C': 0.021334914097815426, 'l1_ratio': 0.8657092057443441, 'threshold': 0.3315606060292118}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:25:33,951] Trial 15 finished with value: 5588.0 and parameters: {'C': 0.004547452574505892, 'l1_ratio': 0.4386137432710431, 'threshold': 0.5186312145438704}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:25:42,658] Trial 16 finished with value: 3514.0 and parameters: {'C': 0.015499019846845881, 'l1_ratio': 0.9808313329607156, 'threshold': 0.46755129990565875}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:25:57,223] Trial 17 finished with value: 228.00000000000017 and parameters: {'C': 0.012078017036443034, 'l1_ratio': 0.6609276776343689, 'threshold': 0.413937497050672}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:26:02,311] Trial 18 finished with value: 6152.0 and parameters: {'C': 0.004242519328062332, 'l1_ratio': 0.8769873303641031, 'threshold': 0.5514575896520496}. Best is trial 11 with value: 6766.0.\n",
            "[I 2025-06-02 12:26:09,280] Trial 19 finished with value: 6593.999999999998 and parameters: {'C': 0.008180524412672058, 'l1_ratio': 0.8364660839364376, 'threshold': 0.4712796110144151}. Best is trial 11 with value: 6766.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet best parameters: {'C': 0.006710491299252708, 'l1_ratio': 0.980956554686088, 'threshold': 0.47609595295077023}\n",
            "Fitting final ElasticNet model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:26:10,533] A new study created in memory with name: no-name-76b07fd5-df86-4b24-94df-f536133abf8a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuning random forest ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-02 12:27:10,390] Trial 0 finished with value: -92350.0 and parameters: {'n_estimators': 383, 'threshold': 0.38438911230672346, 'max_features': 0.33334267205391954}. Best is trial 0 with value: -92350.0.\n",
            "[I 2025-06-02 12:28:10,664] Trial 1 finished with value: -92098.0 and parameters: {'n_estimators': 382, 'threshold': 0.5661748181956261, 'max_features': 0.46947865745297435}. Best is trial 1 with value: -92098.0.\n",
            "[I 2025-06-02 12:29:10,669] Trial 2 finished with value: -93220.0 and parameters: {'n_estimators': 391, 'threshold': 0.3008161125968057, 'max_features': 0.486591286945903}. Best is trial 1 with value: -92098.0.\n",
            "[I 2025-06-02 12:30:11,620] Trial 3 finished with value: -91974.0 and parameters: {'n_estimators': 251, 'threshold': 0.5177025277810962, 'max_features': 0.16683356880330458}. Best is trial 3 with value: -91974.0.\n",
            "[I 2025-06-02 12:31:12,350] Trial 4 finished with value: -92698.0 and parameters: {'n_estimators': 425, 'threshold': 0.6448104633241611, 'max_features': 0.3086036292086772}. Best is trial 3 with value: -91974.0.\n",
            "[I 2025-06-02 12:32:12,674] Trial 5 finished with value: -92138.0 and parameters: {'n_estimators': 162, 'threshold': 0.57758779465884, 'max_features': 0.09255510527200463}. Best is trial 3 with value: -91974.0.\n",
            "[I 2025-06-02 12:33:12,294] Trial 6 finished with value: -93350.0 and parameters: {'n_estimators': 170, 'threshold': 0.6985543483456298, 'max_features': 0.2714826440189222}. Best is trial 3 with value: -91974.0.\n",
            "[I 2025-06-02 12:34:12,548] Trial 7 finished with value: -92050.0 and parameters: {'n_estimators': 344, 'threshold': 0.5554785285912873, 'max_features': 0.37603257824193326}. Best is trial 3 with value: -91974.0.\n",
            "[I 2025-06-02 12:35:12,278] Trial 8 finished with value: -92918.0 and parameters: {'n_estimators': 376, 'threshold': 0.6641209719613982, 'max_features': 0.4302307211900305}. Best is trial 3 with value: -91974.0.\n",
            "[I 2025-06-02 12:36:12,577] Trial 9 finished with value: -91964.0 and parameters: {'n_estimators': 100, 'threshold': 0.5090022607378303, 'max_features': 0.39735221934815096}. Best is trial 9 with value: -91964.0.\n",
            "[I 2025-06-02 12:37:12,267] Trial 10 finished with value: -92062.0 and parameters: {'n_estimators': 250, 'threshold': 0.46383676921740186, 'max_features': 0.2352520562194096}. Best is trial 9 with value: -91964.0.\n",
            "[I 2025-06-02 12:38:12,248] Trial 11 finished with value: -92048.0 and parameters: {'n_estimators': 103, 'threshold': 0.4452946615159055, 'max_features': 0.15262641623058262}. Best is trial 9 with value: -91964.0.\n",
            "[I 2025-06-02 12:39:12,747] Trial 12 finished with value: -91958.0 and parameters: {'n_estimators': 490, 'threshold': 0.5061956462440097, 'max_features': 0.1887771989664354}. Best is trial 12 with value: -91958.0.\n",
            "[I 2025-06-02 12:40:13,703] Trial 13 finished with value: -92186.0 and parameters: {'n_estimators': 497, 'threshold': 0.4108673834529297, 'max_features': 0.2067150877080658}. Best is trial 12 with value: -91958.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def net_score(y_true, y_pred, n_features):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return 10 * acc * len(y_true) - 200 * n_features\n",
        "\n",
        "def evaluate_model_on_features(model_class, model_params, feats, X, y):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    all_true, all_prob = [], []\n",
        "    for tr_idx, val_idx in skf.split(X, y):\n",
        "        X_tr = X.iloc[tr_idx, feats]\n",
        "        y_tr = y.iloc[tr_idx]\n",
        "        X_val = X.iloc[val_idx, feats]\n",
        "        y_val = y.iloc[val_idx]\n",
        "        model = model_class(**model_params)\n",
        "        model.fit(X_tr, y_tr)\n",
        "        all_true.append(y_val.values)\n",
        "        all_prob.append(model.predict_proba(X_val)[:, 1])\n",
        "    all_true = np.concatenate(all_true)\n",
        "    all_prob = np.concatenate(all_prob)\n",
        "    best_net, best_thr = -np.inf, 0.5\n",
        "    for thr in np.linspace(0.1, 0.9, 81):\n",
        "        preds = (all_prob >= thr).astype(int)\n",
        "        net = net_score(all_true, preds, len(feats))\n",
        "        if net > best_net:\n",
        "            best_net, best_thr = net, thr\n",
        "    return best_net, best_thr\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_info in model_dicts:\n",
        "    name = model_info['name']\n",
        "    feats = feature_dicts[name]\n",
        "    if len(feats) == 0:\n",
        "        print(f\"{name}: No selected features, skipping.\")\n",
        "        continue\n",
        "    net, thr = evaluate_model_on_features(model_info['model'], model_info['params'], feats, x_train_final, y_train)\n",
        "    print(f\"{name}: net-score = {net:.1f} @ threshold {thr:.2f} with {len(feats)} features.\")\n",
        "    results.append({'name': name, 'feats': feats, 'net_score': net, 'threshold': thr})\n",
        "\n",
        "best_result = max(results, key=lambda d: d['net_score'])\n",
        "print(f\"\\nBest model: {best_result['name']} with net-score {best_result['net_score']:.1f}, {len(best_result['feats'])} features, and threshold {best_result['threshold']:.2f}\")\n",
        "\n",
        "# final model trained on ALL data with best subset and threshold\n",
        "final_model_info = next(item for item in model_dicts if item['name'] == best_result['name'])\n",
        "final_model = final_model_info['model'](**final_model_info['params'])\n",
        "final_model.fit(x_train_final.iloc[:, best_result['feats']], y_train)\n",
        "\n",
        "# predict on test set\n",
        "test_probs = final_model.predict_proba(x_test_final.iloc[:, best_result['feats']])[:, 1]\n",
        "\n",
        "top_1000_idx = np.argsort(-test_probs)[:1000]\n",
        "\n",
        "student_id = '323597'\n",
        "np.savetxt(f\"{student_id}_obs.txt\", top_1000_idx, fmt='%d')\n",
        "np.savetxt(f\"{student_id}_vars.txt\", best_result['feats'], fmt='%d')\n"
      ],
      "metadata": {
        "id": "rnMCsEiwGjYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}