{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562592dc",
   "metadata": {},
   "source": [
    "## In this notebook we will focus on preprocessing to drop as many columns at the begining. We will compute statistics, compare the train and test dataset. Drop some columns that don't match. Moreover we will calculate correlation and check outliers so that the model is not affected that much by them. \n",
    "\n",
    "Most of the things are outlined in this blog https://neptune.ai/blog/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions#:~:text=%2A%20Stratified%20KFold%20cross,Time%20Series%20split%20validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d897c",
   "metadata": {},
   "source": [
    "# First of all let's check some features statistics in both train and test data. Let's compare top 10 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1947163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         train_mean  test_mean  mean_diff  train_variance  test_variance  \\\n",
      "feature                                                                    \n",
      "0         15.560411  15.507078  -0.053333       18.730704      17.260227   \n",
      "1         12.650449  12.655507   0.005058       14.317654      13.704232   \n",
      "2         27.750084  27.736016  -0.014067       48.258792      44.019127   \n",
      "3         18.796808  18.825133   0.028325       24.323544      22.632589   \n",
      "4         19.071302  18.995343  -0.075959       27.471500      25.135808   \n",
      "5         11.820110  11.769083  -0.051027       13.312292      12.482966   \n",
      "6         19.365360  19.355964  -0.009395       28.260874      25.857634   \n",
      "7         15.602632  15.517396  -0.085235       19.669401      18.001512   \n",
      "8         14.163618  14.233636   0.070018       19.810112      19.051235   \n",
      "9         15.989661  16.041487   0.051826       22.978933      22.298445   \n",
      "\n",
      "         variance_ratio  corr_with_target  \n",
      "feature                                    \n",
      "0              0.921494          0.300607  \n",
      "1              0.957156          0.290802  \n",
      "2              0.912147          0.374769  \n",
      "3              0.930481          0.339719  \n",
      "4              0.914978          0.316651  \n",
      "5              0.937702          0.301198  \n",
      "6              0.914962          0.338679  \n",
      "7              0.915204          0.320353  \n",
      "8              0.961692          0.307160  \n",
      "9              0.970386          0.283087  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_train = pd.read_csv('data/x_train.txt', sep=r'\\s+', header=None)\n",
    "y_train = pd.read_csv('data/y_train.txt', sep=r'\\s+', header=None)[0]\n",
    "x_test  = pd.read_csv('data/x_test.txt',  sep=r'\\s+', header=None)\n",
    "\n",
    "stats = []\n",
    "for i in x_train.columns:\n",
    "    t_mean = x_train[i].mean()\n",
    "    s_mean = x_test[i].mean()\n",
    "    t_var  = x_train[i].var()\n",
    "    s_var  = x_test[i].var()\n",
    "    stats.append({\n",
    "        'feature':             i,\n",
    "        'train_mean':          t_mean,\n",
    "        'test_mean':           s_mean,\n",
    "        'mean_diff':           s_mean - t_mean,\n",
    "        'train_variance':      t_var,\n",
    "        'test_variance':       s_var,\n",
    "        'variance_ratio':      (s_var / t_var) if t_var>0 else np.nan,\n",
    "        'corr_with_target':    x_train[i].corr(y_train)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(stats).set_index('feature')\n",
    "\n",
    "print(summary_df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e1a52",
   "metadata": {},
   "source": [
    "# In this part we will look at distribution mismatches and test using Komogorov-Smirnov test. Moreover, we will look at outliers and cap them to 1st and 99th percentiles and display mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b86480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistics for top 10 ks_stat features:\n",
      "         ks_stat  ks_pvalue  train_outliers  test_outliers  train_q01  \\\n",
      "feature                                                                 \n",
      "428       0.0426   0.000229             100            112   2.665452   \n",
      "130       0.0368   0.002291             100            105  -2.249431   \n",
      "107       0.0368   0.002291             100             97  -2.315544   \n",
      "135       0.0354   0.003798             100            110  -2.354010   \n",
      "394       0.0340   0.006174             100            156   0.012440   \n",
      "303       0.0338   0.006607             100             76   0.008643   \n",
      "296       0.0336   0.007068             100             92   0.022230   \n",
      "252       0.0324   0.010504             100            108   0.025570   \n",
      "34        0.0314   0.014451             100             99  -2.257608   \n",
      "266       0.0308   0.017417             100            144   0.030595   \n",
      "\n",
      "         train_q99  train_min  train_max  test_min   test_max  \n",
      "feature                                                        \n",
      "428      23.415190   0.830056  34.928336  1.114300  32.063120  \n",
      "130       2.302304  -3.641161   3.584047 -3.387836   3.381168  \n",
      "107       2.328245  -3.661105   3.565555 -3.407935   3.174591  \n",
      "135       2.281959  -3.492702   3.617008 -3.427228   3.703639  \n",
      "394       0.988695   0.000535   0.999467  0.000059   0.999838  \n",
      "303       0.990880   0.000035   0.999969  0.000126   0.999650  \n",
      "296       9.410959   0.000598  18.144028  0.000620  17.965835  \n",
      "252       9.690338   0.000460  15.506521  0.001080  17.864720  \n",
      "34        2.389097  -3.176476   4.311408 -3.753966   3.770609  \n",
      "266       8.854601   0.000885  17.840303  0.000150  17.373207  \n",
      "\n",
      "KS statistics for top 10 ks_stat features: after capping:\n",
      "         ks_stat  ks_after_capping\n",
      "feature                           \n",
      "428       0.0426            0.0426\n",
      "130       0.0368            0.0368\n",
      "107       0.0368            0.0368\n",
      "135       0.0354            0.0354\n",
      "394       0.0340            0.0340\n",
      "303       0.0338            0.0338\n",
      "296       0.0336            0.0336\n",
      "252       0.0324            0.0324\n",
      "34        0.0314            0.0314\n",
      "266       0.0308            0.0308\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "\n",
    "q_low  = x_train.quantile(0.01)\n",
    "q_high = x_train.quantile(0.99)\n",
    "\n",
    "stats = []\n",
    "for col in x_train.columns:\n",
    "    ks_stat, ks_p = ks_2samp(x_train[col], x_test[col])\n",
    "    train_outliers = ((x_train[col] < q_low[col]) | (x_train[col] > q_high[col])).sum()\n",
    "    test_outliers  = ((x_test[col]  < q_low[col]) | (x_test[col]  > q_high[col])).sum()\n",
    "    stats.append({\n",
    "        'feature':        col,\n",
    "        'ks_stat':        ks_stat,\n",
    "        'ks_pvalue':      ks_p,\n",
    "        'train_outliers': train_outliers,\n",
    "        'test_outliers':  test_outliers,\n",
    "        'train_q01':      q_low[col],\n",
    "        'train_q99':      q_high[col],\n",
    "        'train_min':      x_train[col].min(),\n",
    "        'train_max':      x_train[col].max(),\n",
    "        'test_min':       x_test[col].min(),\n",
    "        'test_max':       x_test[col].max(),\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats).set_index('feature')\n",
    "\n",
    "print(\"KS statistics for top 10 ks_stat features:\")\n",
    "print(stats_df.sort_values('ks_stat', ascending=False).head(10))\n",
    "\n",
    "x_train_capped = x_train.clip(lower=q_low, upper=q_high, axis=1)\n",
    "x_test_capped  = x_test.clip( lower=q_low, upper=q_high, axis=1)\n",
    "\n",
    "ks_after = []\n",
    "for col in x_train.columns:\n",
    "    ks2, p2 = ks_2samp(x_train_capped[col], x_test_capped[col])\n",
    "    ks_after.append(ks2)\n",
    "stats_df['ks_after_capping'] = ks_after\n",
    "\n",
    "print(\"\\nKS statistics for top 10 ks_stat features: after capping:\")\n",
    "top10 = stats_df.sort_values('ks_stat', ascending=False).head(10)\n",
    "print(top10[['ks_stat','ks_after_capping']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef048f4",
   "metadata": {},
   "source": [
    "# Now in this part recalculate ks test, use QuantileTransformer to cap the data. Moreover Do Adversarial Validation using Random Forest to distinguish between test and train and drop some of the features that mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef33b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged features (KS ≥ 0.035 or test_outliers > 120):\n",
      "[11, 20, 24, 44, 47, 50, 55, 107, 114, 117, 121, 122, 123, 127, 130, 133, 135, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 428, 430, 434, 439, 464]\n",
      "\n",
      "Adversarial AUC: 0.501, Accuracy: 0.490\n",
      "\n",
      "Top 10 adversarial-important features:\n",
      "130    0.022881\n",
      "107    0.022392\n",
      "428    0.022245\n",
      "388    0.022131\n",
      "135    0.021990\n",
      "184    0.021944\n",
      "117    0.021707\n",
      "394    0.021654\n",
      "204    0.021605\n",
      "232    0.021597\n",
      "dtype: float64 \n",
      "\n",
      "KS after QuantileTransform:\n",
      "         ks_after\n",
      "feature          \n",
      "11         0.0078\n",
      "20         0.0186\n",
      "24         0.0134\n",
      "44         0.0206\n",
      "47         0.0164\n",
      "50         0.0206\n",
      "55         0.0242\n",
      "107        0.0368\n",
      "114        0.0084\n",
      "117        0.0218\n",
      "121        0.0184\n",
      "122        0.0178\n",
      "123        0.0180\n",
      "127        0.0196\n",
      "130        0.0368\n",
      "133        0.0172\n",
      "135        0.0354\n",
      "163        0.0120\n",
      "175        0.0176\n",
      "177        0.0106\n",
      "184        0.0180\n",
      "188        0.0114\n",
      "193        0.0166\n",
      "198        0.0084\n",
      "204        0.0204\n",
      "214        0.0230\n",
      "225        0.0190\n",
      "232        0.0102\n",
      "244        0.0118\n",
      "253        0.0188\n",
      "264        0.0166\n",
      "266        0.0308\n",
      "279        0.0226\n",
      "342        0.0134\n",
      "343        0.0100\n",
      "354        0.0198\n",
      "388        0.0166\n",
      "390        0.0172\n",
      "394        0.0340\n",
      "399        0.0280\n",
      "404        0.0156\n",
      "413        0.0142\n",
      "426        0.0138\n",
      "428        0.0426\n",
      "430        0.0240\n",
      "434        0.0096\n",
      "439        0.0184\n",
      "464        0.0218 \n",
      "\n",
      "Features to drop (ks_after > 0.035): [107, 130, 135, 428]\n",
      "Features to keep: [11, 20, 24, 44, 47, 50, 55, 114, 117, 121, 122, 123, 127, 133, 163, 175, 177, 184, 188, 193, 198, 204, 214, 225, 232, 244, 253, 264, 266, 279, 342, 343, 354, 388, 390, 394, 399, 404, 413, 426, 430, 434, 439, 464]\n",
      "\n",
      "Final shapes → x_train: (5000, 496), x_test: (5000, 496)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "q_low, q_high = x_train.quantile(0.01), x_train.quantile(0.99)\n",
    "\n",
    "stats = []\n",
    "for col in x_train.columns:\n",
    "    ks_stat, _ = ks_2samp(x_train[col], x_test[col])\n",
    "    test_out = ((x_test[col] < q_low[col]) | (x_test[col] > q_high[col])).sum()\n",
    "    stats.append({'feature': col, 'ks_stat': ks_stat, 'test_outliers': test_out})\n",
    "stats_df = pd.DataFrame(stats).set_index('feature')\n",
    "\n",
    "ks_thresh = 0.035\n",
    "outlier_thresh = 120\n",
    "flagged = stats_df[\n",
    "    (stats_df['ks_stat'] >= ks_thresh) |\n",
    "    (stats_df['test_outliers'] > outlier_thresh)\n",
    "].index.tolist()\n",
    "print(f\"Flagged features (KS ≥ {ks_thresh} or test_outliers > {outlier_thresh}):\\n{flagged}\\n\")\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='uniform', random_state=0)\n",
    "combined = pd.concat([x_train[flagged], x_test[flagged]], axis=0)\n",
    "qt.fit(combined)\n",
    "\n",
    "x_train_qt = x_train.copy()\n",
    "x_test_qt  = x_test.copy()\n",
    "x_train_qt[flagged] = qt.transform(x_train[flagged])\n",
    "x_test_qt[flagged]  = qt.transform(x_test[flagged])\n",
    "\n",
    "X_adv = pd.concat([x_train_qt[flagged], x_test_qt[flagged]], axis=0)\n",
    "y_adv = np.concatenate([np.zeros(len(x_train_qt)), np.ones(len(x_test_qt))])\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_adv, y_adv, test_size=0.3, random_state=0, stratify=y_adv\n",
    ")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_tr, y_tr)\n",
    "\n",
    "y_prob = rf.predict_proba(X_val)[:,1]\n",
    "print(f\"Adversarial AUC: {roc_auc_score(y_val, y_prob):.3f}, \"\n",
    "      f\"Accuracy: {accuracy_score(y_val, rf.predict(X_val)):.3f}\\n\")\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=flagged).sort_values(ascending=False)\n",
    "print(\"Top 10 adversarial-important features:\")\n",
    "print(importances.head(10), \"\\n\")\n",
    "\n",
    "new_stats = []\n",
    "for col in flagged:\n",
    "    ks2, _ = ks_2samp(x_train_qt[col], x_test_qt[col])\n",
    "    new_stats.append({'feature': col, 'ks_after': ks2})\n",
    "new_df = pd.DataFrame(new_stats).set_index('feature')\n",
    "print(\"KS after QuantileTransform:\")\n",
    "print(new_df, \"\\n\")\n",
    "\n",
    "to_drop = new_df[new_df['ks_after'] > ks_thresh].index.tolist()\n",
    "keep_after = [f for f in flagged if f not in to_drop]\n",
    "print(f\"Features to drop (ks_after > {ks_thresh}): {to_drop}\")\n",
    "print(f\"Features to keep: {keep_after}\\n\")\n",
    "\n",
    "x_train_processed = x_train_qt.drop(columns=to_drop)\n",
    "x_test_processed  = x_test_qt.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Final shapes → x_train: {x_train_processed.shape}, x_test: {x_test_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5f447",
   "metadata": {},
   "source": [
    "# Now let's chech Variance Threshold that are near-zero. Moreover, remove highly correlated features if there exist such. Scale the remaining features using both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped for near-zero variance: []\n",
      "Dropped for high correlation: [7]\n",
      "Final shapes → (5000, 495) (5000, 495)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "sel = VarianceThreshold(threshold=1e-5)\n",
    "sel.fit(x_train_processed)\n",
    "keep_var = x_train_processed.columns[sel.get_support()]\n",
    "drop_var = [c for c in x_train_processed.columns if c not in keep_var]\n",
    "print(\"Dropped for near-zero variance:\", drop_var)\n",
    "x_train_var = x_train_processed[keep_var].copy()\n",
    "x_test_var  = x_test_processed[keep_var].copy()\n",
    "\n",
    "corr = x_train_var.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "drop_corr = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "print(\"Dropped for high correlation:\", drop_corr)\n",
    "x_train_corr = x_train_var.drop(columns=drop_corr).copy()\n",
    "x_test_corr  = x_test_var.drop(columns=drop_corr).copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "combined = pd.concat([x_train_corr, x_test_corr], axis=0)\n",
    "scaler.fit(combined)\n",
    "\n",
    "x_train_final = pd.DataFrame(\n",
    "    scaler.transform(x_train_corr),\n",
    "    columns=x_train_corr.columns,\n",
    "    index=x_train_corr.index\n",
    ")\n",
    "x_test_final = pd.DataFrame(\n",
    "    scaler.transform(x_test_corr),\n",
    "    columns=x_test_corr.columns,\n",
    "    index=x_test_corr.index\n",
    ")\n",
    "\n",
    "print(\"Final shapes →\", x_train_final.shape, x_test_final.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc341f79",
   "metadata": {},
   "source": [
    "# Now we will create our net_score that we want to minimize, do an optuna search to find best parameters of C and threshold. After that use SHAP to rank features by importance. Also do the Bootstrap stability check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 00:52:28,121] A new study created in memory with name: no-name-5a0a16ac-4988-4df8-ba5c-47e5ee007f28\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:29,339] Trial 0 finished with value: 6830.0 and parameters: {'C': 0.004522437918151069, 'threshold': 0.48978592450321723}. Best is trial 0 with value: 6830.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:30,453] Trial 1 finished with value: 5922.0 and parameters: {'C': 0.0018839712621663148, 'threshold': 0.5313700607227497}. Best is trial 0 with value: 6830.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:31,602] Trial 2 finished with value: 6836.0 and parameters: {'C': 0.004590763009886912, 'threshold': 0.4760647627503256}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:33,076] Trial 3 finished with value: 6582.0 and parameters: {'C': 0.005985592080546905, 'threshold': 0.4395536154442109}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:34,194] Trial 4 finished with value: 6094.0 and parameters: {'C': 0.0037538126116888936, 'threshold': 0.4059692963218539}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:34,713] Trial 5 finished with value: 4886.0 and parameters: {'C': 0.0010737789912946979, 'threshold': 0.4182320189259382}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:35,911] Trial 6 finished with value: 4934.0 and parameters: {'C': 0.001381620162429894, 'threshold': 0.5218950766311958}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:37,142] Trial 7 finished with value: 5096.0 and parameters: {'C': 0.0016093955460968238, 'threshold': 0.5573434372604524}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:39,277] Trial 8 finished with value: 6240.000000000001 and parameters: {'C': 0.0095127723618945, 'threshold': 0.4116053564724521}. Best is trial 2 with value: 6836.0.\n",
      "C:\\Users\\hucu\\AppData\\Local\\Temp\\ipykernel_23856\\2709083352.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
      "[I 2025-05-29 00:52:40,290] Trial 9 finished with value: 6414.0 and parameters: {'C': 0.002468588657099085, 'threshold': 0.5245533212096122}. Best is trial 2 with value: 6836.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params → C: 0.00459, threshold: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hucu\\Desktop\\AMLProject2\\venv\\Lib\\site-packages\\shap\\explainers\\_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features by mean |SHAP|:\n",
      "2     0.501693\n",
      "6     0.000000\n",
      "8     0.000000\n",
      "9     0.000000\n",
      "10    0.000000\n",
      "11    0.000000\n",
      "12    0.000000\n",
      "13    0.000000\n",
      "14    0.000000\n",
      "15    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Bootstrap selection frequency (top 10):\n",
      "2      1.00\n",
      "6      0.53\n",
      "414    0.04\n",
      "5      0.03\n",
      "8      0.01\n",
      "11     0.00\n",
      "12     0.00\n",
      "13     0.00\n",
      "14     0.00\n",
      "15     0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "\n",
    "\n",
    "def net_score(y_true, y_pred, n_features):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return 10 * acc * len(y_true) - 200 * n_features\n",
    "\n",
    "def objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 0.001, 0.01)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr, X_val = x_train_final.iloc[train_idx], x_train_final.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = LogisticRegression(\n",
    "            penalty='l1', solver='saga', C=C,\n",
    "            max_iter=10000, random_state=0\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        probs = model.predict_proba(X_val)[:, 1]\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        n_feats = np.count_nonzero(model.coef_)\n",
    "        \n",
    "        scores.append(net_score(y_val, preds, n_feats))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_C = best_params['C']\n",
    "best_threshold = best_params['threshold']\n",
    "print(f\"Best params C: {best_C:.5f}, threshold: {best_threshold:.2f}\")\n",
    "\n",
    "final_model = LogisticRegression(\n",
    "    penalty='l1', solver='saga', C=best_C,\n",
    "    max_iter=10000, random_state=0\n",
    ")\n",
    "final_model.fit(x_train_final, y_train)\n",
    "\n",
    "test_probs = final_model.predict_proba(x_test_final)[:, 1]\n",
    "y_test_pred = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "explainer = shap.LinearExplainer(\n",
    "    final_model,\n",
    "    x_train_final,\n",
    "    feature_perturbation=\"interventional\"\n",
    ")\n",
    "shap_values = explainer.shap_values(x_train_final)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "shap_imp = pd.Series(mean_abs_shap, index=x_train_final.columns)\n",
    "shap_imp = shap_imp.sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by mean |SHAP|:\")\n",
    "print(shap_imp.head(10))\n",
    "\n",
    "B = 100\n",
    "rng = np.random.RandomState(0)\n",
    "feat_count = pd.Series(0, index=x_train_final.columns)\n",
    "\n",
    "for i in range(B):\n",
    "    idx = rng.choice(len(x_train_final), len(x_train_final), replace=True)\n",
    "    Xb, yb = x_train_final.iloc[idx], y_train.iloc[idx]\n",
    "    m = LogisticRegression(\n",
    "        penalty='l1', solver='saga', C=best_C,\n",
    "        max_iter=10000, random_state=i\n",
    "    )\n",
    "    m.fit(Xb, yb)\n",
    "    sel = x_train_final.columns[m.coef_[0] != 0]\n",
    "    feat_count[sel] += 1\n",
    "\n",
    "stability = (feat_count / B).sort_values(ascending=False)\n",
    "print(\"\\nBootstrap selection frequency (top 10):\")\n",
    "print(stability.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30baf5ab",
   "metadata": {},
   "source": [
    "# Now with the SHAP and Bootstrap we have distinguished two features that stand out among the rest hence we will check only two subsets 2 and [2, 6] to chech the net score of those with Kfold validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3287d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset [2]: CV net‐score = 35130.0 @ threshold 0.49\n",
      "Subset [2, 6]: CV net‐score = 34940.0 @ threshold 0.48\n",
      "\n",
      "Winning subset: [2] with net‐score 35130.0 @ threshold 0.49\n",
      "Final model trained with features [2]; test predictions ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_subset(feats):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    all_true, all_prob = [], []\n",
    "    for tr_idx, val_idx in skf.split(x_train_final, y_train):\n",
    "        X_tr = x_train_final.iloc[tr_idx][feats]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        X_val = x_train_final.iloc[val_idx][feats]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model = LogisticRegression(penalty='l1', solver='saga',\n",
    "                                   C=best_C, max_iter=10000, random_state=0)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        all_true.append(y_val.values)\n",
    "        all_prob.append(model.predict_proba(X_val)[:,1])\n",
    "        \n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    \n",
    "    best_net, best_thr = -np.inf, 0.5\n",
    "    for thr in np.linspace(0.1, 0.9, 81):\n",
    "        preds = (all_prob >= thr).astype(int)\n",
    "        net = net_score(all_true, preds, len(feats))\n",
    "        if net > best_net:\n",
    "            best_net, best_thr = net, thr\n",
    "    return best_net, best_thr\n",
    "\n",
    "candidates = [[2], [2, 6]]\n",
    "\n",
    "best_net = -np.inf\n",
    "winning_subset = None\n",
    "winning_thr = None\n",
    "\n",
    "for subset in candidates:\n",
    "    net, thr = evaluate_subset(subset)\n",
    "    print(f\"Subset {subset}: CV net-score = {net:.1f} @ threshold {thr:.2f}\")\n",
    "    if net > best_net:\n",
    "        best_net, winning_subset, winning_thr = net, subset, thr\n",
    "\n",
    "print(f\"\\nWinning subset: {winning_subset} with net-score {best_net:.1f} @ threshold {winning_thr:.2f}\")\n",
    "\n",
    "final = LogisticRegression(penalty='l1', solver='saga', C=best_C,\n",
    "                           max_iter=10000, random_state=0)\n",
    "final.fit(x_train_final[winning_subset], y_train)\n",
    "\n",
    "y_test_pred = (final.predict_proba(x_test_final[winning_subset])[:,1] >= winning_thr).astype(int)\n",
    "\n",
    "print(f\"Final model trained with features {winning_subset}; test predictions ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcf230",
   "metadata": {},
   "source": [
    "# Create histogram to check distributions of these two to compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336b85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 2 – Training Set Statistics:\n",
      "{'mean': np.float64(27.75008354959396), 'median': np.float64(27.1105612298134), 'std': np.float64(6.946854798411643), 'min': np.float64(9.36101575097885), 'max': np.float64(58.1441483368565)}\n",
      "\n",
      "Feature 2 – Test Set Statistics:\n",
      "{'mean': np.float64(27.736016441053316), 'median': np.float64(27.27966727062615), 'std': np.float64(6.634691184525816), 'min': np.float64(10.1794240572433), 'max': np.float64(59.9774966977393)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "feat = 2\n",
    "\n",
    "train_stats = {\n",
    "    'mean':   x_train[feat].mean(),\n",
    "    'median': x_train[feat].median(),\n",
    "    'std':    x_train[feat].std(),\n",
    "    'min':    x_train[feat].min(),\n",
    "    'max':    x_train[feat].max()\n",
    "}\n",
    "test_stats = {\n",
    "    'mean':   x_test[feat].mean(),\n",
    "    'median': x_test[feat].median(),\n",
    "    'std':    x_test[feat].std(),\n",
    "    'min':    x_test[feat].min(),\n",
    "    'max':    x_test[feat].max()\n",
    "}\n",
    "\n",
    "print(\"Feature 2 – Training Set Statistics:\")\n",
    "print(train_stats)\n",
    "print(\"\\nFeature 2 – Test Set Statistics:\")\n",
    "print(test_stats)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(x_train[feat], bins=30)\n",
    "plt.title('Feature 2 Distribution – Training Set')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(x_test[feat], bins=30)\n",
    "plt.title('Feature 2 Distribution – Test Set')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
